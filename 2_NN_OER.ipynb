{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62d48018-0264-483b-bf96-6eca4158516a",
   "metadata": {},
   "source": [
    "Previous training with the initial 7 features gives high importance to the features mu, RA and t. So next step we can create new features using operations * and / with these features as component, that is, we create new features $RA\\cdot t, mu/RA, mu/t$, add them into the training set and retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57250860-59a1-45b4-9dc8-9bc597d01505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55b28ac-483e-43dc-b425-867de44640fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>mu</th>\n",
       "      <th>RA</th>\n",
       "      <th>XA</th>\n",
       "      <th>XB</th>\n",
       "      <th>QA</th>\n",
       "      <th>Nd</th>\n",
       "      <th>mu/t</th>\n",
       "      <th>VRHE</th>\n",
       "      <th>RA*t</th>\n",
       "      <th>mu/RA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.993000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>1.791583</td>\n",
       "      <td>1.350480</td>\n",
       "      <td>0.316176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.422000</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.7300</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.50</td>\n",
       "      <td>0.423000</td>\n",
       "      <td>1.722750</td>\n",
       "      <td>1.357280</td>\n",
       "      <td>0.310294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.003000</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.9100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>1.707833</td>\n",
       "      <td>1.364080</td>\n",
       "      <td>0.305147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.988000</td>\n",
       "      <td>0.437000</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.7250</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.442000</td>\n",
       "      <td>1.774417</td>\n",
       "      <td>1.343680</td>\n",
       "      <td>0.321324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.004000</td>\n",
       "      <td>0.414000</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.9020</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.80</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>1.790833</td>\n",
       "      <td>1.365440</td>\n",
       "      <td>0.304412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.004000</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.8940</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.60</td>\n",
       "      <td>0.412000</td>\n",
       "      <td>1.753917</td>\n",
       "      <td>1.365440</td>\n",
       "      <td>0.303676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.009000</td>\n",
       "      <td>0.407000</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.8300</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>1.759083</td>\n",
       "      <td>1.372240</td>\n",
       "      <td>0.299265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.407000</td>\n",
       "      <td>1.3650</td>\n",
       "      <td>1.1150</td>\n",
       "      <td>1.8300</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.403000</td>\n",
       "      <td>1.724667</td>\n",
       "      <td>1.378650</td>\n",
       "      <td>0.298168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.012000</td>\n",
       "      <td>0.407000</td>\n",
       "      <td>1.3700</td>\n",
       "      <td>1.1300</td>\n",
       "      <td>1.8300</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.402000</td>\n",
       "      <td>1.755583</td>\n",
       "      <td>1.386440</td>\n",
       "      <td>0.297080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.011000</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.8800</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.399000</td>\n",
       "      <td>1.720583</td>\n",
       "      <td>1.374960</td>\n",
       "      <td>0.297059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.011000</td>\n",
       "      <td>0.398000</td>\n",
       "      <td>1.3500</td>\n",
       "      <td>1.0500</td>\n",
       "      <td>1.8800</td>\n",
       "      <td>2.50</td>\n",
       "      <td>5.50</td>\n",
       "      <td>0.394000</td>\n",
       "      <td>1.681583</td>\n",
       "      <td>1.364850</td>\n",
       "      <td>0.294815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.019000</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>1.3760</td>\n",
       "      <td>1.0700</td>\n",
       "      <td>1.8800</td>\n",
       "      <td>2.80</td>\n",
       "      <td>5.80</td>\n",
       "      <td>0.394000</td>\n",
       "      <td>1.687500</td>\n",
       "      <td>1.402144</td>\n",
       "      <td>0.291424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.020000</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>1.3800</td>\n",
       "      <td>1.0630</td>\n",
       "      <td>1.8550</td>\n",
       "      <td>2.75</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.393000</td>\n",
       "      <td>1.718583</td>\n",
       "      <td>1.407600</td>\n",
       "      <td>0.290580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.034000</td>\n",
       "      <td>0.397000</td>\n",
       "      <td>1.4080</td>\n",
       "      <td>1.0100</td>\n",
       "      <td>1.8800</td>\n",
       "      <td>2.40</td>\n",
       "      <td>5.40</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>1.699333</td>\n",
       "      <td>1.455872</td>\n",
       "      <td>0.281960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.042000</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>1.4240</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>1.8800</td>\n",
       "      <td>2.20</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.379000</td>\n",
       "      <td>1.680250</td>\n",
       "      <td>1.483808</td>\n",
       "      <td>0.277388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.049000</td>\n",
       "      <td>0.393000</td>\n",
       "      <td>1.4400</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>1.8800</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>1.670000</td>\n",
       "      <td>1.510560</td>\n",
       "      <td>0.272917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.082000</td>\n",
       "      <td>0.391000</td>\n",
       "      <td>1.5250</td>\n",
       "      <td>0.9200</td>\n",
       "      <td>1.8760</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>1.638250</td>\n",
       "      <td>1.650050</td>\n",
       "      <td>0.256393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.119000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>1.6100</td>\n",
       "      <td>0.8900</td>\n",
       "      <td>1.8300</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>1.709167</td>\n",
       "      <td>1.801590</td>\n",
       "      <td>0.239130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.063938</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>1.4900</td>\n",
       "      <td>1.0225</td>\n",
       "      <td>1.7300</td>\n",
       "      <td>2.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.374221</td>\n",
       "      <td>1.672500</td>\n",
       "      <td>1.585268</td>\n",
       "      <td>0.267214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.093738</td>\n",
       "      <td>0.397407</td>\n",
       "      <td>1.5680</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>1.7975</td>\n",
       "      <td>2.20</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.363348</td>\n",
       "      <td>1.586167</td>\n",
       "      <td>1.714982</td>\n",
       "      <td>0.253449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.088382</td>\n",
       "      <td>0.379259</td>\n",
       "      <td>1.5160</td>\n",
       "      <td>1.0070</td>\n",
       "      <td>1.9100</td>\n",
       "      <td>2.40</td>\n",
       "      <td>6.40</td>\n",
       "      <td>0.348461</td>\n",
       "      <td>1.622583</td>\n",
       "      <td>1.649988</td>\n",
       "      <td>0.250171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.070734</td>\n",
       "      <td>0.364815</td>\n",
       "      <td>1.4400</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>1.9025</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.340715</td>\n",
       "      <td>1.619417</td>\n",
       "      <td>1.541857</td>\n",
       "      <td>0.253344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.127314</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>1.5675</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>1.9100</td>\n",
       "      <td>2.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.315401</td>\n",
       "      <td>1.607417</td>\n",
       "      <td>1.767064</td>\n",
       "      <td>0.226830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           t        mu      RA      XA      XB    QA    Nd      mu/t  \\\n",
       "0   0.993000  0.430000  1.3600  1.1000  1.5500  3.00  4.00  0.433000   \n",
       "1   0.998000  0.422000  1.3600  1.1000  1.7300  3.00  5.50  0.423000   \n",
       "2   1.003000  0.415000  1.3600  1.1000  1.9100  3.00  7.00  0.413000   \n",
       "3   0.988000  0.437000  1.3600  1.1000  1.7250  3.00  6.00  0.442000   \n",
       "4   1.004000  0.414000  1.3600  1.1000  1.9020  3.00  6.80  0.413000   \n",
       "5   1.004000  0.413000  1.3600  1.1000  1.8940  3.00  6.60  0.412000   \n",
       "6   1.009000  0.407000  1.3600  1.1000  1.8300  3.00  5.00  0.404000   \n",
       "7   1.010000  0.407000  1.3650  1.1150  1.8300  3.00  5.00  0.403000   \n",
       "8   1.012000  0.407000  1.3700  1.1300  1.8300  3.00  5.00  0.402000   \n",
       "9   1.011000  0.404000  1.3600  1.1000  1.8800  3.00  6.00  0.399000   \n",
       "10  1.011000  0.398000  1.3500  1.0500  1.8800  2.50  5.50  0.394000   \n",
       "11  1.019000  0.401000  1.3760  1.0700  1.8800  2.80  5.80  0.394000   \n",
       "12  1.020000  0.401000  1.3800  1.0630  1.8550  2.75  5.25  0.393000   \n",
       "13  1.034000  0.397000  1.4080  1.0100  1.8800  2.40  5.40  0.384000   \n",
       "14  1.042000  0.395000  1.4240  0.9800  1.8800  2.20  5.20  0.379000   \n",
       "15  1.049000  0.393000  1.4400  0.9500  1.8800  2.00  5.00  0.374000   \n",
       "16  1.082000  0.391000  1.5250  0.9200  1.8760  2.00  4.80  0.361000   \n",
       "17  1.119000  0.385000  1.6100  0.8900  1.8300  2.00  4.00  0.344000   \n",
       "18  1.063938  0.398148  1.4900  1.0225  1.7300  2.50  5.00  0.374221   \n",
       "19  1.093738  0.397407  1.5680  0.9760  1.7975  2.20  4.70  0.363348   \n",
       "20  1.088382  0.379259  1.5160  1.0070  1.9100  2.40  6.40  0.348461   \n",
       "21  1.070734  0.364815  1.4400  0.9500  1.9025  2.00  5.75  0.340715   \n",
       "22  1.127314  0.355556  1.5675  0.9050  1.9100  2.00  6.00  0.315401   \n",
       "\n",
       "        VRHE      RA*t     mu/RA  \n",
       "0   1.791583  1.350480  0.316176  \n",
       "1   1.722750  1.357280  0.310294  \n",
       "2   1.707833  1.364080  0.305147  \n",
       "3   1.774417  1.343680  0.321324  \n",
       "4   1.790833  1.365440  0.304412  \n",
       "5   1.753917  1.365440  0.303676  \n",
       "6   1.759083  1.372240  0.299265  \n",
       "7   1.724667  1.378650  0.298168  \n",
       "8   1.755583  1.386440  0.297080  \n",
       "9   1.720583  1.374960  0.297059  \n",
       "10  1.681583  1.364850  0.294815  \n",
       "11  1.687500  1.402144  0.291424  \n",
       "12  1.718583  1.407600  0.290580  \n",
       "13  1.699333  1.455872  0.281960  \n",
       "14  1.680250  1.483808  0.277388  \n",
       "15  1.670000  1.510560  0.272917  \n",
       "16  1.638250  1.650050  0.256393  \n",
       "17  1.709167  1.801590  0.239130  \n",
       "18  1.672500  1.585268  0.267214  \n",
       "19  1.586167  1.714982  0.253449  \n",
       "20  1.622583  1.649988  0.250171  \n",
       "21  1.619417  1.541857  0.253344  \n",
       "22  1.607417  1.767064  0.226830  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_excel(\"data_gp.xlsx\")\n",
    "df = df.drop(['mu*RA', 'mu*t', 'RA/t'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21748eb2-4a06-464c-a352-860a9f669532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.08223085,  1.62676953, -0.77345319,  0.86166497, -3.46038823,\n",
       "          0.97887708, -1.88632043,  1.48595843, -0.89069789,  1.30696623],\n",
       "        [-0.95938445,  1.18516573, -0.77345319,  0.86166497, -1.30350668,\n",
       "          0.97887708,  0.04477912,  1.16041762, -0.84320669,  1.07547261],\n",
       "        [-0.83653805,  0.79876242, -0.77345319,  0.86166497,  0.85337488,\n",
       "          0.97887708,  1.97587866,  0.83487681, -0.79571548,  0.8729157 ],\n",
       "        [-1.20507726,  2.01317285, -0.77345319,  0.86166497, -1.36342005,\n",
       "          0.97887708,  0.68847897,  1.77894515, -0.9381891 ,  1.50952314],\n",
       "        [-0.81196877,  0.74356194, -0.77345319,  0.86166497,  0.75751347,\n",
       "          0.97887708,  1.71839873,  0.83487681, -0.78621724,  0.84397899],\n",
       "        [-0.81196877,  0.68836147, -0.77345319,  0.86166497,  0.66165207,\n",
       "          0.97887708,  1.46091879,  0.80232273, -0.78621724,  0.81504229],\n",
       "        [-0.68912236,  0.35715862, -0.77345319,  0.86166497, -0.10523915,\n",
       "          0.97887708, -0.59892073,  0.54189008, -0.73872603,  0.64142208],\n",
       "        [-0.66455308,  0.35715862, -0.71123065,  1.0650686 , -0.10523915,\n",
       "          0.97887708, -0.59892073,  0.509336  , -0.69395859,  0.59828201],\n",
       "        [-0.61541452,  0.35715862, -0.64900811,  1.26847224, -0.10523915,\n",
       "          0.97887708, -0.59892073,  0.47678192, -0.63955322,  0.55545684],\n",
       "        [-0.6399838 ,  0.1915572 , -0.77345319,  0.86166497,  0.49389462,\n",
       "          0.97887708,  0.68847897,  0.37911968, -0.71972955,  0.55461197],\n",
       "        [-0.6399838 , -0.13964564, -0.89789826,  0.18365285,  0.49389462,\n",
       "         -0.23810524,  0.04477912,  0.21634928, -0.7903378 ,  0.46630145],\n",
       "        [-0.44342955,  0.02595578, -0.57434107,  0.4548577 ,  0.49389462,\n",
       "          0.49208416,  0.43099903,  0.21634928, -0.52987647,  0.33287608],\n",
       "        [-0.41886027,  0.02595578, -0.52456304,  0.359936  ,  0.19432773,\n",
       "          0.37038592, -0.2770708 ,  0.1837952 , -0.49177176,  0.2996335 ],\n",
       "        [-0.07489034, -0.19484612, -0.17611683, -0.35875685,  0.49389462,\n",
       "         -0.4815017 , -0.08396085, -0.10919153, -0.15464006, -0.0395769 ],\n",
       "        [ 0.12166391, -0.30524707,  0.02299529, -0.76556412,  0.49389462,\n",
       "         -0.96829463, -0.34144079, -0.27196193,  0.04046499, -0.21952609],\n",
       "        [ 0.29364887, -0.41564801,  0.2221074 , -1.17237139,  0.49389462,\n",
       "         -1.45508756, -0.59892073, -0.43473234,  0.22730099, -0.39547641],\n",
       "        [ 1.10443514, -0.52604896,  1.27989053, -1.57917866,  0.44596392,\n",
       "         -1.45508756, -0.85640067, -0.85793539,  1.20149929, -1.04572997],\n",
       "        [ 2.01349853, -0.85725181,  2.33767366, -1.98598594, -0.10523915,\n",
       "         -1.45508756, -1.88632043, -1.41135476,  2.2598548 , -1.72509691]]),\n",
       " array([1.79158333, 1.72275   , 1.70783333, 1.77441667, 1.79083333,\n",
       "        1.75391667, 1.75908333, 1.72466667, 1.75558333, 1.72058333,\n",
       "        1.68158333, 1.6875    , 1.71858333, 1.69933333, 1.68025   ,\n",
       "        1.67      , 1.63825   , 1.70916667]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select input features and target\n",
    "features = ['t', 'mu', 'RA', 'XA', 'XB', 'QA', 'Nd', 'mu/t', 'RA*t', 'mu/RA']\n",
    "target = 'VRHE'\n",
    "X = df[features].values\n",
    "y = df[target].values\n",
    "\n",
    "# Normalize input features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train = X_scaled[:18, :]\n",
    "y_train = y[:18]\n",
    "X_val = X_scaled[18:, :]\n",
    "y_val = y[18:]\n",
    "\n",
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffd6762-455d-4db2-b3be-af52e309ca85",
   "metadata": {},
   "source": [
    "Use the same 4-layer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d06fed8-2309-43a1-a13c-288348f5f935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "Epoch 1: Warmup phase (1/150) - val_mae: 1.3952[0m \u001b[1m0s\u001b[0m 1s/step - loss: 10.0750 - mae: 1.5069\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 10.0750 - mae: 1.5069 - val_loss: 9.6997 - val_mae: 1.3952\n",
      "Epoch 2/600\n",
      "Epoch 2: Warmup phase (2/150) - val_mae: 1.3109[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 9.7650 - mae: 1.4212\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 9.7650 - mae: 1.4212 - val_loss: 9.4105 - val_mae: 1.3109\n",
      "Epoch 3/600\n",
      "Epoch 3: Warmup phase (3/150) - val_mae: 1.2263[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.4981 - mae: 1.3458\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 9.4981 - mae: 1.3458 - val_loss: 9.1331 - val_mae: 1.2263\n",
      "Epoch 4/600\n",
      "Epoch 4: Warmup phase (4/150) - val_mae: 1.1471[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 9.2452 - mae: 1.2709\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 9.2452 - mae: 1.2709 - val_loss: 8.8840 - val_mae: 1.1471\n",
      "Epoch 5/600\n",
      "Epoch 5: Warmup phase (5/150) - val_mae: 1.0761[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.0076 - mae: 1.1968\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 9.0076 - mae: 1.1968 - val_loss: 8.6663 - val_mae: 1.0761\n",
      "Epoch 6/600\n",
      "Epoch 6: Warmup phase (6/150) - val_mae: 1.0031[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.7849 - mae: 1.1239\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 8.7849 - mae: 1.1239 - val_loss: 8.4568 - val_mae: 1.0031\n",
      "Epoch 7/600\n",
      "Epoch 7: Warmup phase (7/150) - val_mae: 0.9306[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.5713 - mae: 1.0491\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 8.5713 - mae: 1.0491 - val_loss: 8.2622 - val_mae: 0.9306\n",
      "Epoch 8/600\n",
      "Epoch 8: Warmup phase (8/150) - val_mae: 0.8591[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.3675 - mae: 0.9725\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 8.3675 - mae: 0.9725 - val_loss: 8.0811 - val_mae: 0.8591\n",
      "Epoch 9/600\n",
      "Epoch 9: Warmup phase (9/150) - val_mae: 0.7885[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.1750 - mae: 0.8947\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 8.1750 - mae: 0.8947 - val_loss: 7.9132 - val_mae: 0.7885\n",
      "Epoch 10/600\n",
      "Epoch 10: Warmup phase (10/150) - val_mae: 0.7189m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.9946 - mae: 0.8159\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 7.9946 - mae: 0.8159 - val_loss: 7.7580 - val_mae: 0.7189\n",
      "Epoch 11/600\n",
      "Epoch 11: Warmup phase (11/150) - val_mae: 0.6507m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.8268 - mae: 0.7405\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 7.8268 - mae: 0.7405 - val_loss: 7.6154 - val_mae: 0.6507\n",
      "Epoch 12/600\n",
      "Epoch 12: Warmup phase (12/150) - val_mae: 0.6074m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.6707 - mae: 0.6826\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 7.6707 - mae: 0.6826 - val_loss: 7.4839 - val_mae: 0.6074\n",
      "Epoch 13/600\n",
      "Epoch 13: Warmup phase (13/150) - val_mae: 0.5859m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.5287 - mae: 0.6433\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 7.5287 - mae: 0.6433 - val_loss: 7.3636 - val_mae: 0.5859\n",
      "Epoch 14/600\n",
      "Epoch 14: Warmup phase (14/150) - val_mae: 0.5664m \u001b[1m0s\u001b[0m 25ms/step - loss: 7.3998 - mae: 0.6039\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 7.3998 - mae: 0.6039 - val_loss: 7.2554 - val_mae: 0.5664\n",
      "Epoch 15/600\n",
      "Epoch 15: Warmup phase (15/150) - val_mae: 0.5487m \u001b[1m0s\u001b[0m 26ms/step - loss: 7.2829 - mae: 0.5653\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 7.2829 - mae: 0.5653 - val_loss: 7.1573 - val_mae: 0.5487\n",
      "Epoch 16/600\n",
      "Epoch 16: Warmup phase (16/150) - val_mae: 0.5332m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.1770 - mae: 0.5357\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 7.1770 - mae: 0.5357 - val_loss: 7.0671 - val_mae: 0.5332\n",
      "Epoch 17/600\n",
      "Epoch 17: Warmup phase (17/150) - val_mae: 0.5198m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.0801 - mae: 0.5129\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 7.0801 - mae: 0.5129 - val_loss: 6.9831 - val_mae: 0.5198\n",
      "Epoch 18/600\n",
      "Epoch 18: Warmup phase (18/150) - val_mae: 0.5089m \u001b[1m0s\u001b[0m 27ms/step - loss: 6.9901 - mae: 0.4937\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 6.9901 - mae: 0.4937 - val_loss: 6.9037 - val_mae: 0.5089\n",
      "Epoch 19/600\n",
      "Epoch 19: Warmup phase (19/150) - val_mae: 0.5003m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.9047 - mae: 0.4883\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 6.9047 - mae: 0.4883 - val_loss: 6.8270 - val_mae: 0.5003\n",
      "Epoch 20/600\n",
      "Epoch 20: Warmup phase (20/150) - val_mae: 0.4937m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.8218 - mae: 0.4860\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 6.8218 - mae: 0.4860 - val_loss: 6.7519 - val_mae: 0.4937\n",
      "Epoch 21/600\n",
      "Epoch 21: Warmup phase (21/150) - val_mae: 0.4894m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.7393 - mae: 0.4831\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 6.7393 - mae: 0.4831 - val_loss: 6.6778 - val_mae: 0.4894\n",
      "Epoch 22/600\n",
      "Epoch 22: Warmup phase (22/150) - val_mae: 0.4872m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.6559 - mae: 0.4780\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 6.6559 - mae: 0.4780 - val_loss: 6.6041 - val_mae: 0.4872\n",
      "Epoch 23/600\n",
      "Epoch 23: Warmup phase (23/150) - val_mae: 0.4869m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.5703 - mae: 0.4707\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 6.5703 - mae: 0.4707 - val_loss: 6.5308 - val_mae: 0.4869\n",
      "Epoch 24/600\n",
      "Epoch 24: Warmup phase (24/150) - val_mae: 0.4885m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.4825 - mae: 0.4616\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 6.4825 - mae: 0.4616 - val_loss: 6.4580 - val_mae: 0.4885\n",
      "Epoch 25/600\n",
      "Epoch 25: Warmup phase (25/150) - val_mae: 0.4934m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.3929 - mae: 0.4510\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 6.3929 - mae: 0.4510 - val_loss: 6.3855 - val_mae: 0.4934\n",
      "Epoch 26/600\n",
      "Epoch 26: Warmup phase (26/150) - val_mae: 0.4994m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.3018 - mae: 0.4391\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 6.3018 - mae: 0.4391 - val_loss: 6.3137 - val_mae: 0.4994\n",
      "Epoch 27/600\n",
      "Epoch 27: Warmup phase (27/150) - val_mae: 0.5050m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.2102 - mae: 0.4263\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 6.2102 - mae: 0.4263 - val_loss: 6.2427 - val_mae: 0.5050\n",
      "Epoch 28/600\n",
      "Epoch 28: Warmup phase (28/150) - val_mae: 0.5103m \u001b[1m0s\u001b[0m 25ms/step - loss: 6.1192 - mae: 0.4133\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 6.1192 - mae: 0.4133 - val_loss: 6.1730 - val_mae: 0.5103\n",
      "Epoch 29/600\n",
      "Epoch 29: Warmup phase (29/150) - val_mae: 0.5154m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.0293 - mae: 0.4004\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 6.0293 - mae: 0.4004 - val_loss: 6.1046 - val_mae: 0.5154\n",
      "Epoch 30/600\n",
      "Epoch 30: Warmup phase (30/150) - val_mae: 0.5202m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.9414 - mae: 0.3875\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 5.9414 - mae: 0.3875 - val_loss: 6.0374 - val_mae: 0.5202\n",
      "Epoch 31/600\n",
      "Epoch 31: Warmup phase (31/150) - val_mae: 0.5246m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.8557 - mae: 0.3761\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 5.8557 - mae: 0.3761 - val_loss: 5.9716 - val_mae: 0.5246\n",
      "Epoch 32/600\n",
      "Epoch 32: Warmup phase (32/150) - val_mae: 0.5290m \u001b[1m0s\u001b[0m 23ms/step - loss: 5.7725 - mae: 0.3654\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 5.7725 - mae: 0.3654 - val_loss: 5.9071 - val_mae: 0.5290\n",
      "Epoch 33/600\n",
      "Epoch 33: Warmup phase (33/150) - val_mae: 0.5331m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.6916 - mae: 0.3549\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 5.6916 - mae: 0.3549 - val_loss: 5.8435 - val_mae: 0.5331\n",
      "Epoch 34/600\n",
      "Epoch 34: Warmup phase (34/150) - val_mae: 0.5369m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.6131 - mae: 0.3445\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 5.6131 - mae: 0.3445 - val_loss: 5.7809 - val_mae: 0.5369\n",
      "Epoch 35/600\n",
      "Epoch 35: Warmup phase (35/150) - val_mae: 0.5405m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.5365 - mae: 0.3342\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 5.5365 - mae: 0.3342 - val_loss: 5.7188 - val_mae: 0.5405\n",
      "Epoch 36/600\n",
      "Epoch 36: Warmup phase (36/150) - val_mae: 0.5444m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.4616 - mae: 0.3263\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 5.4616 - mae: 0.3263 - val_loss: 5.6576 - val_mae: 0.5444\n",
      "Epoch 37/600\n",
      "Epoch 37: Warmup phase (37/150) - val_mae: 0.5480m \u001b[1m0s\u001b[0m 25ms/step - loss: 5.3881 - mae: 0.3200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 5.3881 - mae: 0.3200 - val_loss: 5.5965 - val_mae: 0.5480\n",
      "Epoch 38/600\n",
      "Epoch 38: Warmup phase (38/150) - val_mae: 0.5516m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.3157 - mae: 0.3162\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 5.3157 - mae: 0.3162 - val_loss: 5.5356 - val_mae: 0.5516\n",
      "Epoch 39/600\n",
      "Epoch 39: Warmup phase (39/150) - val_mae: 0.5543m \u001b[1m0s\u001b[0m 25ms/step - loss: 5.2443 - mae: 0.3117\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 5.2443 - mae: 0.3117 - val_loss: 5.4737 - val_mae: 0.5543\n",
      "Epoch 40/600\n",
      "Epoch 40: Warmup phase (40/150) - val_mae: 0.5574m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.1737 - mae: 0.3063\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 5.1737 - mae: 0.3063 - val_loss: 5.4120 - val_mae: 0.5574\n",
      "Epoch 41/600\n",
      "Epoch 41: Warmup phase (41/150) - val_mae: 0.5603m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.1037 - mae: 0.3005\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 5.1037 - mae: 0.3005 - val_loss: 5.3499 - val_mae: 0.5603\n",
      "Epoch 42/600\n",
      "Epoch 42: Warmup phase (42/150) - val_mae: 0.5631m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.0341 - mae: 0.2939\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 5.0341 - mae: 0.2939 - val_loss: 5.2878 - val_mae: 0.5631\n",
      "Epoch 43/600\n",
      "Epoch 43: Warmup phase (43/150) - val_mae: 0.5659m \u001b[1m0s\u001b[0m 24ms/step - loss: 4.9649 - mae: 0.2864\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 4.9649 - mae: 0.2864 - val_loss: 5.2256 - val_mae: 0.5659\n",
      "Epoch 44/600\n",
      "Epoch 44: Warmup phase (44/150) - val_mae: 0.5689m \u001b[1m0s\u001b[0m 27ms/step - loss: 4.8963 - mae: 0.2783\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 4.8963 - mae: 0.2783 - val_loss: 5.1636 - val_mae: 0.5689\n",
      "Epoch 45/600\n",
      "Epoch 45: Warmup phase (45/150) - val_mae: 0.5717m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.8281 - mae: 0.2694\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 4.8281 - mae: 0.2694 - val_loss: 5.1022 - val_mae: 0.5717\n",
      "Epoch 46/600\n",
      "Epoch 46: Warmup phase (46/150) - val_mae: 0.5762m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.7605 - mae: 0.2598\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 4.7605 - mae: 0.2598 - val_loss: 5.0416 - val_mae: 0.5762\n",
      "Epoch 47/600\n",
      "Epoch 47: Warmup phase (47/150) - val_mae: 0.5820m \u001b[1m0s\u001b[0m 27ms/step - loss: 4.6936 - mae: 0.2494\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 4.6936 - mae: 0.2494 - val_loss: 4.9825 - val_mae: 0.5820\n",
      "Epoch 48/600\n",
      "Epoch 48: Warmup phase (48/150) - val_mae: 0.5871m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.6278 - mae: 0.2390\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 4.6278 - mae: 0.2390 - val_loss: 4.9233 - val_mae: 0.5871\n",
      "Epoch 49/600\n",
      "Epoch 49: Warmup phase (49/150) - val_mae: 0.5907m \u001b[1m0s\u001b[0m 26ms/step - loss: 4.5630 - mae: 0.2282\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 4.5630 - mae: 0.2282 - val_loss: 4.8635 - val_mae: 0.5907\n",
      "Epoch 50/600\n",
      "Epoch 50: Warmup phase (50/150) - val_mae: 0.5965m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.4993 - mae: 0.2168\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 4.4993 - mae: 0.2168 - val_loss: 4.8066 - val_mae: 0.5965\n",
      "Epoch 51/600\n",
      "Epoch 51: Warmup phase (51/150) - val_mae: 0.6019m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.4368 - mae: 0.2055\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 4.4368 - mae: 0.2055 - val_loss: 4.7513 - val_mae: 0.6019\n",
      "Epoch 52/600\n",
      "Epoch 52: Warmup phase (52/150) - val_mae: 0.6068m \u001b[1m0s\u001b[0m 24ms/step - loss: 4.3756 - mae: 0.1962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 4.3756 - mae: 0.1962 - val_loss: 4.6969 - val_mae: 0.6068\n",
      "Epoch 53/600\n",
      "Epoch 53: Warmup phase (53/150) - val_mae: 0.6109m \u001b[1m0s\u001b[0m 24ms/step - loss: 4.3156 - mae: 0.1883\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 4.3156 - mae: 0.1883 - val_loss: 4.6432 - val_mae: 0.6109\n",
      "Epoch 54/600\n",
      "Epoch 54: Warmup phase (54/150) - val_mae: 0.6146m \u001b[1m0s\u001b[0m 27ms/step - loss: 4.2567 - mae: 0.1820\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 4.2567 - mae: 0.1820 - val_loss: 4.5905 - val_mae: 0.6146\n",
      "Epoch 55/600\n",
      "Epoch 55: Warmup phase (55/150) - val_mae: 0.6176m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.1988 - mae: 0.1759\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 4.1988 - mae: 0.1759 - val_loss: 4.5382 - val_mae: 0.6176\n",
      "Epoch 56/600\n",
      "Epoch 56: Warmup phase (56/150) - val_mae: 0.6197m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.1418 - mae: 0.1698\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 4.1418 - mae: 0.1698 - val_loss: 4.4861 - val_mae: 0.6197\n",
      "Epoch 57/600\n",
      "Epoch 57: Warmup phase (57/150) - val_mae: 0.6211m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.0854 - mae: 0.1636\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 4.0854 - mae: 0.1636 - val_loss: 4.4341 - val_mae: 0.6211\n",
      "Epoch 58/600\n",
      "Epoch 58: Warmup phase (58/150) - val_mae: 0.6220m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.0296 - mae: 0.1590\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 4.0296 - mae: 0.1590 - val_loss: 4.3826 - val_mae: 0.6220\n",
      "Epoch 59/600\n",
      "Epoch 59: Warmup phase (59/150) - val_mae: 0.6218m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.9744 - mae: 0.1541\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 3.9744 - mae: 0.1541 - val_loss: 4.3309 - val_mae: 0.6218\n",
      "Epoch 60/600\n",
      "Epoch 60: Warmup phase (60/150) - val_mae: 0.6215m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.9199 - mae: 0.1492\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 3.9199 - mae: 0.1492 - val_loss: 4.2798 - val_mae: 0.6215\n",
      "Epoch 61/600\n",
      "Epoch 61: Warmup phase (61/150) - val_mae: 0.6205m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.8660 - mae: 0.1452\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 3.8660 - mae: 0.1452 - val_loss: 4.2290 - val_mae: 0.6205\n",
      "Epoch 62/600\n",
      "Epoch 62: Warmup phase (62/150) - val_mae: 0.6192m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.8128 - mae: 0.1409\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 3.8128 - mae: 0.1409 - val_loss: 4.1787 - val_mae: 0.6192\n",
      "Epoch 63/600\n",
      "Epoch 63: Warmup phase (63/150) - val_mae: 0.6177m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.7604 - mae: 0.1364\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 3.7604 - mae: 0.1364 - val_loss: 4.1289 - val_mae: 0.6177\n",
      "Epoch 64/600\n",
      "Epoch 64: Warmup phase (64/150) - val_mae: 0.6160m \u001b[1m0s\u001b[0m 26ms/step - loss: 3.7087 - mae: 0.1319\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 3.7087 - mae: 0.1319 - val_loss: 4.0796 - val_mae: 0.6160\n",
      "Epoch 65/600\n",
      "Epoch 65: Warmup phase (65/150) - val_mae: 0.6142m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.6576 - mae: 0.1274\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 3.6576 - mae: 0.1274 - val_loss: 4.0305 - val_mae: 0.6142\n",
      "Epoch 66/600\n",
      "Epoch 66: Warmup phase (66/150) - val_mae: 0.6122m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.6072 - mae: 0.1253\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 3.6072 - mae: 0.1253 - val_loss: 3.9816 - val_mae: 0.6122\n",
      "Epoch 67/600\n",
      "Epoch 67: Warmup phase (67/150) - val_mae: 0.6102m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.5572 - mae: 0.1237\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 3.5572 - mae: 0.1237 - val_loss: 3.9327 - val_mae: 0.6102\n",
      "Epoch 68/600\n",
      "Epoch 68: Warmup phase (68/150) - val_mae: 0.6082m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.5080 - mae: 0.1218\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 3.5080 - mae: 0.1218 - val_loss: 3.8839 - val_mae: 0.6082\n",
      "Epoch 69/600\n",
      "Epoch 69: Warmup phase (69/150) - val_mae: 0.6064m \u001b[1m0s\u001b[0m 25ms/step - loss: 3.4594 - mae: 0.1202\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 3.4594 - mae: 0.1202 - val_loss: 3.8353 - val_mae: 0.6064\n",
      "Epoch 70/600\n",
      "Epoch 70: Warmup phase (70/150) - val_mae: 0.6044m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.4114 - mae: 0.1188\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 3.4114 - mae: 0.1188 - val_loss: 3.7865 - val_mae: 0.6044\n",
      "Epoch 71/600\n",
      "Epoch 71: Warmup phase (71/150) - val_mae: 0.6022m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.3641 - mae: 0.1170\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 3.3641 - mae: 0.1170 - val_loss: 3.7375 - val_mae: 0.6022\n",
      "Epoch 72/600\n",
      "Epoch 72: Warmup phase (72/150) - val_mae: 0.6000m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.3175 - mae: 0.1155\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 3.3175 - mae: 0.1155 - val_loss: 3.6888 - val_mae: 0.6000\n",
      "Epoch 73/600\n",
      "Epoch 73: Warmup phase (73/150) - val_mae: 0.5978m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.2715 - mae: 0.1140\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 3.2715 - mae: 0.1140 - val_loss: 3.6404 - val_mae: 0.5978\n",
      "Epoch 74/600\n",
      "Epoch 74: Warmup phase (74/150) - val_mae: 0.5955m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.2262 - mae: 0.1123\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 3.2262 - mae: 0.1123 - val_loss: 3.5921 - val_mae: 0.5955\n",
      "Epoch 75/600\n",
      "Epoch 75: Warmup phase (75/150) - val_mae: 0.5930m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.1814 - mae: 0.1105\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 3.1814 - mae: 0.1105 - val_loss: 3.5442 - val_mae: 0.5930\n",
      "Epoch 76/600\n",
      "Epoch 76: Warmup phase (76/150) - val_mae: 0.5902m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.1374 - mae: 0.1094\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 3.1374 - mae: 0.1094 - val_loss: 3.4965 - val_mae: 0.5902\n",
      "Epoch 77/600\n",
      "Epoch 77: Warmup phase (77/150) - val_mae: 0.5873m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.0940 - mae: 0.1083\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 3.0940 - mae: 0.1083 - val_loss: 3.4494 - val_mae: 0.5873\n",
      "Epoch 78/600\n",
      "Epoch 78: Warmup phase (78/150) - val_mae: 0.5842m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.0513 - mae: 0.1071\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 3.0513 - mae: 0.1071 - val_loss: 3.4030 - val_mae: 0.5842\n",
      "Epoch 79/600\n",
      "Epoch 79: Warmup phase (79/150) - val_mae: 0.5810m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.0091 - mae: 0.1061\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 3.0091 - mae: 0.1061 - val_loss: 3.3571 - val_mae: 0.5810\n",
      "Epoch 80/600\n",
      "Epoch 80: Warmup phase (80/150) - val_mae: 0.5777m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.9675 - mae: 0.1053\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 2.9675 - mae: 0.1053 - val_loss: 3.3119 - val_mae: 0.5777\n",
      "Epoch 81/600\n",
      "Epoch 81: Warmup phase (81/150) - val_mae: 0.5744m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.9265 - mae: 0.1043\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.9265 - mae: 0.1043 - val_loss: 3.2672 - val_mae: 0.5744\n",
      "Epoch 82/600\n",
      "Epoch 82: Warmup phase (82/150) - val_mae: 0.5709m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.8859 - mae: 0.1031\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.8859 - mae: 0.1031 - val_loss: 3.2232 - val_mae: 0.5709\n",
      "Epoch 83/600\n",
      "Epoch 83: Warmup phase (83/150) - val_mae: 0.5673m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.8459 - mae: 0.1015\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.8459 - mae: 0.1015 - val_loss: 3.1798 - val_mae: 0.5673\n",
      "Epoch 84/600\n",
      "Epoch 84: Warmup phase (84/150) - val_mae: 0.5637m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.8064 - mae: 0.0997\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 2.8064 - mae: 0.0997 - val_loss: 3.1371 - val_mae: 0.5637\n",
      "Epoch 85/600\n",
      "Epoch 85: Warmup phase (85/150) - val_mae: 0.5601m \u001b[1m0s\u001b[0m 28ms/step - loss: 2.7675 - mae: 0.0984\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 2.7675 - mae: 0.0984 - val_loss: 3.0949 - val_mae: 0.5601\n",
      "Epoch 86/600\n",
      "Epoch 86: Warmup phase (86/150) - val_mae: 0.5564m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.7291 - mae: 0.0974\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.7291 - mae: 0.0974 - val_loss: 3.0534 - val_mae: 0.5564\n",
      "Epoch 87/600\n",
      "Epoch 87: Warmup phase (87/150) - val_mae: 0.5528m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.6912 - mae: 0.0968\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 2.6912 - mae: 0.0968 - val_loss: 3.0125 - val_mae: 0.5528\n",
      "Epoch 88/600\n",
      "Epoch 88: Warmup phase (88/150) - val_mae: 0.5492m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.6540 - mae: 0.0968\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 2.6540 - mae: 0.0968 - val_loss: 2.9722 - val_mae: 0.5492\n",
      "Epoch 89/600\n",
      "Epoch 89: Warmup phase (89/150) - val_mae: 0.5454m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.6173 - mae: 0.0973\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 2.6173 - mae: 0.0973 - val_loss: 2.9323 - val_mae: 0.5454\n",
      "Epoch 90/600\n",
      "Epoch 90: Warmup phase (90/150) - val_mae: 0.5420m \u001b[1m0s\u001b[0m 25ms/step - loss: 2.5811 - mae: 0.0976\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 2.5811 - mae: 0.0976 - val_loss: 2.8929 - val_mae: 0.5420\n",
      "Epoch 91/600\n",
      "Epoch 91: Warmup phase (91/150) - val_mae: 0.5383m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.5454 - mae: 0.0976\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 2.5454 - mae: 0.0976 - val_loss: 2.8539 - val_mae: 0.5383\n",
      "Epoch 92/600\n",
      "Epoch 92: Warmup phase (92/150) - val_mae: 0.5349m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.5102 - mae: 0.0973\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 2.5102 - mae: 0.0973 - val_loss: 2.8153 - val_mae: 0.5349\n",
      "Epoch 93/600\n",
      "Epoch 93: Warmup phase (93/150) - val_mae: 0.5317m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.4754 - mae: 0.0966\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.4754 - mae: 0.0966 - val_loss: 2.7774 - val_mae: 0.5317\n",
      "Epoch 94/600\n",
      "Epoch 94: Warmup phase (94/150) - val_mae: 0.5284m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.4412 - mae: 0.0958\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 2.4412 - mae: 0.0958 - val_loss: 2.7399 - val_mae: 0.5284\n",
      "Epoch 95/600\n",
      "Epoch 95: Warmup phase (95/150) - val_mae: 0.5254m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.4074 - mae: 0.0948\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 2.4074 - mae: 0.0948 - val_loss: 2.7027 - val_mae: 0.5254\n",
      "Epoch 96/600\n",
      "Epoch 96: Warmup phase (96/150) - val_mae: 0.5221m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.3741 - mae: 0.0938\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 2.3741 - mae: 0.0938 - val_loss: 2.6660 - val_mae: 0.5221\n",
      "Epoch 97/600\n",
      "Epoch 97: Warmup phase (97/150) - val_mae: 0.5189m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.3413 - mae: 0.0933\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 2.3413 - mae: 0.0933 - val_loss: 2.6297 - val_mae: 0.5189\n",
      "Epoch 98/600\n",
      "Epoch 98: Warmup phase (98/150) - val_mae: 0.5157m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.3090 - mae: 0.0933\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 2.3090 - mae: 0.0933 - val_loss: 2.5940 - val_mae: 0.5157\n",
      "Epoch 99/600\n",
      "Epoch 99: Warmup phase (99/150) - val_mae: 0.5126m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.2771 - mae: 0.0931\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 2.2771 - mae: 0.0931 - val_loss: 2.5588 - val_mae: 0.5126\n",
      "Epoch 100/600\n",
      "Epoch 100: Warmup phase (100/150) - val_mae: 0.5094\u001b[1m0s\u001b[0m 26ms/step - loss: 2.2457 - mae: 0.0927\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 2.2457 - mae: 0.0927 - val_loss: 2.5241 - val_mae: 0.5094\n",
      "Epoch 101/600\n",
      "Epoch 101: Warmup phase (101/150) - val_mae: 0.5062\u001b[1m0s\u001b[0m 23ms/step - loss: 2.2147 - mae: 0.0923\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 2.2147 - mae: 0.0923 - val_loss: 2.4900 - val_mae: 0.5062\n",
      "Epoch 102/600\n",
      "Epoch 102: Warmup phase (102/150) - val_mae: 0.5031\u001b[1m0s\u001b[0m 23ms/step - loss: 2.1841 - mae: 0.0917\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.1841 - mae: 0.0917 - val_loss: 2.4565 - val_mae: 0.5031\n",
      "Epoch 103/600\n",
      "Epoch 103: Warmup phase (103/150) - val_mae: 0.5001\u001b[1m0s\u001b[0m 23ms/step - loss: 2.1540 - mae: 0.0909\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 2.1540 - mae: 0.0909 - val_loss: 2.4235 - val_mae: 0.5001\n",
      "Epoch 104/600\n",
      "Epoch 104: Warmup phase (104/150) - val_mae: 0.4970\u001b[1m0s\u001b[0m 23ms/step - loss: 2.1243 - mae: 0.0901\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 2.1243 - mae: 0.0901 - val_loss: 2.3910 - val_mae: 0.4970\n",
      "Epoch 105/600\n",
      "Epoch 105: Warmup phase (105/150) - val_mae: 0.4939\u001b[1m0s\u001b[0m 31ms/step - loss: 2.0950 - mae: 0.0893\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 2.0950 - mae: 0.0893 - val_loss: 2.3589 - val_mae: 0.4939\n",
      "Epoch 106/600\n",
      "Epoch 106: Warmup phase (106/150) - val_mae: 0.4909\u001b[1m0s\u001b[0m 22ms/step - loss: 2.0662 - mae: 0.0886\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 2.0662 - mae: 0.0886 - val_loss: 2.3274 - val_mae: 0.4909\n",
      "Epoch 107/600\n",
      "Epoch 107: Warmup phase (107/150) - val_mae: 0.4878\u001b[1m0s\u001b[0m 22ms/step - loss: 2.0377 - mae: 0.0880\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 2.0377 - mae: 0.0880 - val_loss: 2.2961 - val_mae: 0.4878\n",
      "Epoch 108/600\n",
      "Epoch 108: Warmup phase (108/150) - val_mae: 0.4848\u001b[1m0s\u001b[0m 22ms/step - loss: 2.0097 - mae: 0.0876\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 2.0097 - mae: 0.0876 - val_loss: 2.2653 - val_mae: 0.4848\n",
      "Epoch 109/600\n",
      "Epoch 109: Warmup phase (109/150) - val_mae: 0.4820\u001b[1m0s\u001b[0m 23ms/step - loss: 1.9821 - mae: 0.0872\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 1.9821 - mae: 0.0872 - val_loss: 2.2350 - val_mae: 0.4820\n",
      "Epoch 110/600\n",
      "Epoch 110: Warmup phase (110/150) - val_mae: 0.4792\u001b[1m0s\u001b[0m 27ms/step - loss: 1.9548 - mae: 0.0867\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.9548 - mae: 0.0867 - val_loss: 2.2050 - val_mae: 0.4792\n",
      "Epoch 111/600\n",
      "Epoch 111: Warmup phase (111/150) - val_mae: 0.4765\u001b[1m0s\u001b[0m 24ms/step - loss: 1.9279 - mae: 0.0862\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.9279 - mae: 0.0862 - val_loss: 2.1755 - val_mae: 0.4765\n",
      "Epoch 112/600\n",
      "Epoch 112: Warmup phase (112/150) - val_mae: 0.4737\u001b[1m0s\u001b[0m 25ms/step - loss: 1.9014 - mae: 0.0857\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 1.9014 - mae: 0.0857 - val_loss: 2.1463 - val_mae: 0.4737\n",
      "Epoch 113/600\n",
      "Epoch 113: Warmup phase (113/150) - val_mae: 0.4711\u001b[1m0s\u001b[0m 23ms/step - loss: 1.8753 - mae: 0.0851\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.8753 - mae: 0.0851 - val_loss: 2.1175 - val_mae: 0.4711\n",
      "Epoch 114/600\n",
      "Epoch 114: Warmup phase (114/150) - val_mae: 0.4685\u001b[1m0s\u001b[0m 22ms/step - loss: 1.8495 - mae: 0.0845\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 1.8495 - mae: 0.0845 - val_loss: 2.0892 - val_mae: 0.4685\n",
      "Epoch 115/600\n",
      "Epoch 115: Warmup phase (115/150) - val_mae: 0.4658\u001b[1m0s\u001b[0m 26ms/step - loss: 1.8242 - mae: 0.0841\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.8242 - mae: 0.0841 - val_loss: 2.0613 - val_mae: 0.4658\n",
      "Epoch 116/600\n",
      "Epoch 116: Warmup phase (116/150) - val_mae: 0.4633\u001b[1m0s\u001b[0m 22ms/step - loss: 1.7992 - mae: 0.0837\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.7992 - mae: 0.0837 - val_loss: 2.0338 - val_mae: 0.4633\n",
      "Epoch 117/600\n",
      "Epoch 117: Warmup phase (117/150) - val_mae: 0.4607\u001b[1m0s\u001b[0m 22ms/step - loss: 1.7745 - mae: 0.0833\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.7745 - mae: 0.0833 - val_loss: 2.0067 - val_mae: 0.4607\n",
      "Epoch 118/600\n",
      "Epoch 118: Warmup phase (118/150) - val_mae: 0.4582\u001b[1m0s\u001b[0m 22ms/step - loss: 1.7502 - mae: 0.0828\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.7502 - mae: 0.0828 - val_loss: 1.9800 - val_mae: 0.4582\n",
      "Epoch 119/600\n",
      "Epoch 119: Warmup phase (119/150) - val_mae: 0.4557\u001b[1m0s\u001b[0m 22ms/step - loss: 1.7262 - mae: 0.0823\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 1.7262 - mae: 0.0823 - val_loss: 1.9536 - val_mae: 0.4557\n",
      "Epoch 120/600\n",
      "Epoch 120: Warmup phase (120/150) - val_mae: 0.4531\u001b[1m0s\u001b[0m 24ms/step - loss: 1.7026 - mae: 0.0819\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 1.7026 - mae: 0.0819 - val_loss: 1.9276 - val_mae: 0.4531\n",
      "Epoch 121/600\n",
      "Epoch 121: Warmup phase (121/150) - val_mae: 0.4507\u001b[1m0s\u001b[0m 22ms/step - loss: 1.6793 - mae: 0.0815\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.6793 - mae: 0.0815 - val_loss: 1.9020 - val_mae: 0.4507\n",
      "Epoch 122/600\n",
      "Epoch 122: Warmup phase (122/150) - val_mae: 0.4482\u001b[1m0s\u001b[0m 23ms/step - loss: 1.6563 - mae: 0.0810\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.6563 - mae: 0.0810 - val_loss: 1.8767 - val_mae: 0.4482\n",
      "Epoch 123/600\n",
      "Epoch 123: Warmup phase (123/150) - val_mae: 0.4457\u001b[1m0s\u001b[0m 24ms/step - loss: 1.6337 - mae: 0.0805\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 1.6337 - mae: 0.0805 - val_loss: 1.8517 - val_mae: 0.4457\n",
      "Epoch 124/600\n",
      "Epoch 124: Warmup phase (124/150) - val_mae: 0.4430\u001b[1m0s\u001b[0m 22ms/step - loss: 1.6114 - mae: 0.0800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 1.6114 - mae: 0.0800 - val_loss: 1.8270 - val_mae: 0.4430\n",
      "Epoch 125/600\n",
      "Epoch 125: Warmup phase (125/150) - val_mae: 0.4404\u001b[1m0s\u001b[0m 25ms/step - loss: 1.5894 - mae: 0.0795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 1.5894 - mae: 0.0795 - val_loss: 1.8026 - val_mae: 0.4404\n",
      "Epoch 126/600\n",
      "Epoch 126: Warmup phase (126/150) - val_mae: 0.4377\u001b[1m0s\u001b[0m 22ms/step - loss: 1.5677 - mae: 0.0790\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.5677 - mae: 0.0790 - val_loss: 1.7784 - val_mae: 0.4377\n",
      "Epoch 127/600\n",
      "Epoch 127: Warmup phase (127/150) - val_mae: 0.4351\u001b[1m0s\u001b[0m 22ms/step - loss: 1.5463 - mae: 0.0785\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.5463 - mae: 0.0785 - val_loss: 1.7546 - val_mae: 0.4351\n",
      "Epoch 128/600\n",
      "Epoch 128: Warmup phase (128/150) - val_mae: 0.4323\u001b[1m0s\u001b[0m 22ms/step - loss: 1.5253 - mae: 0.0779\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.5253 - mae: 0.0779 - val_loss: 1.7310 - val_mae: 0.4323\n",
      "Epoch 129/600\n",
      "Epoch 129: Warmup phase (129/150) - val_mae: 0.4296\u001b[1m0s\u001b[0m 22ms/step - loss: 1.5045 - mae: 0.0775\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.5045 - mae: 0.0775 - val_loss: 1.7078 - val_mae: 0.4296\n",
      "Epoch 130/600\n",
      "Epoch 130: Warmup phase (130/150) - val_mae: 0.4268\u001b[1m0s\u001b[0m 23ms/step - loss: 1.4841 - mae: 0.0769\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 1.4841 - mae: 0.0769 - val_loss: 1.6849 - val_mae: 0.4268\n",
      "Epoch 131/600\n",
      "Epoch 131: Warmup phase (131/150) - val_mae: 0.4242\u001b[1m0s\u001b[0m 22ms/step - loss: 1.4639 - mae: 0.0763\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.4639 - mae: 0.0763 - val_loss: 1.6624 - val_mae: 0.4242\n",
      "Epoch 132/600\n",
      "Epoch 132: Warmup phase (132/150) - val_mae: 0.4214\u001b[1m0s\u001b[0m 24ms/step - loss: 1.4440 - mae: 0.0758\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.4440 - mae: 0.0758 - val_loss: 1.6401 - val_mae: 0.4214\n",
      "Epoch 133/600\n",
      "Epoch 133: Warmup phase (133/150) - val_mae: 0.4186\u001b[1m0s\u001b[0m 23ms/step - loss: 1.4244 - mae: 0.0753\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 1.4244 - mae: 0.0753 - val_loss: 1.6181 - val_mae: 0.4186\n",
      "Epoch 134/600\n",
      "Epoch 134: Warmup phase (134/150) - val_mae: 0.4158\u001b[1m0s\u001b[0m 29ms/step - loss: 1.4052 - mae: 0.0748\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.4052 - mae: 0.0748 - val_loss: 1.5963 - val_mae: 0.4158\n",
      "Epoch 135/600\n",
      "Epoch 135: Warmup phase (135/150) - val_mae: 0.4130\u001b[1m0s\u001b[0m 23ms/step - loss: 1.3861 - mae: 0.0743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.3861 - mae: 0.0743 - val_loss: 1.5749 - val_mae: 0.4130\n",
      "Epoch 136/600\n",
      "Epoch 136: Warmup phase (136/150) - val_mae: 0.4102\u001b[1m0s\u001b[0m 22ms/step - loss: 1.3674 - mae: 0.0738\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.3674 - mae: 0.0738 - val_loss: 1.5538 - val_mae: 0.4102\n",
      "Epoch 137/600\n",
      "Epoch 137: Warmup phase (137/150) - val_mae: 0.4073\u001b[1m0s\u001b[0m 21ms/step - loss: 1.3488 - mae: 0.0733\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.3488 - mae: 0.0733 - val_loss: 1.5329 - val_mae: 0.4073\n",
      "Epoch 138/600\n",
      "Epoch 138: Warmup phase (138/150) - val_mae: 0.4043\u001b[1m0s\u001b[0m 22ms/step - loss: 1.3306 - mae: 0.0727\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.3306 - mae: 0.0727 - val_loss: 1.5122 - val_mae: 0.4043\n",
      "Epoch 139/600\n",
      "Epoch 139: Warmup phase (139/150) - val_mae: 0.4014\u001b[1m0s\u001b[0m 27ms/step - loss: 1.3126 - mae: 0.0723\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.3126 - mae: 0.0723 - val_loss: 1.4918 - val_mae: 0.4014\n",
      "Epoch 140/600\n",
      "Epoch 140: Warmup phase (140/150) - val_mae: 0.3983\u001b[1m0s\u001b[0m 23ms/step - loss: 1.2949 - mae: 0.0718\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.2949 - mae: 0.0718 - val_loss: 1.4716 - val_mae: 0.3983\n",
      "Epoch 141/600\n",
      "Epoch 141: Warmup phase (141/150) - val_mae: 0.3953\u001b[1m0s\u001b[0m 24ms/step - loss: 1.2775 - mae: 0.0714\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.2775 - mae: 0.0714 - val_loss: 1.4518 - val_mae: 0.3953\n",
      "Epoch 142/600\n",
      "Epoch 142: Warmup phase (142/150) - val_mae: 0.3923\u001b[1m0s\u001b[0m 22ms/step - loss: 1.2602 - mae: 0.0710\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.2602 - mae: 0.0710 - val_loss: 1.4322 - val_mae: 0.3923\n",
      "Epoch 143/600\n",
      "Epoch 143: Warmup phase (143/150) - val_mae: 0.3891\u001b[1m0s\u001b[0m 23ms/step - loss: 1.2433 - mae: 0.0706\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.2433 - mae: 0.0706 - val_loss: 1.4128 - val_mae: 0.3891\n",
      "Epoch 144/600\n",
      "Epoch 144: Warmup phase (144/150) - val_mae: 0.3861\u001b[1m0s\u001b[0m 26ms/step - loss: 1.2266 - mae: 0.0702\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.2266 - mae: 0.0702 - val_loss: 1.3937 - val_mae: 0.3861\n",
      "Epoch 145/600\n",
      "Epoch 145: Warmup phase (145/150) - val_mae: 0.3832\u001b[1m0s\u001b[0m 22ms/step - loss: 1.2101 - mae: 0.0697\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.2101 - mae: 0.0697 - val_loss: 1.3749 - val_mae: 0.3832\n",
      "Epoch 146/600\n",
      "Epoch 146: Warmup phase (146/150) - val_mae: 0.3801\u001b[1m0s\u001b[0m 22ms/step - loss: 1.1938 - mae: 0.0693\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 1.1938 - mae: 0.0693 - val_loss: 1.3564 - val_mae: 0.3801\n",
      "Epoch 147/600\n",
      "Epoch 147: Warmup phase (147/150) - val_mae: 0.3774\u001b[1m0s\u001b[0m 30ms/step - loss: 1.1778 - mae: 0.0688\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 1.1778 - mae: 0.0688 - val_loss: 1.3381 - val_mae: 0.3774\n",
      "Epoch 148/600\n",
      "Epoch 148: Warmup phase (148/150) - val_mae: 0.3748\u001b[1m0s\u001b[0m 23ms/step - loss: 1.1620 - mae: 0.0683\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.1620 - mae: 0.0683 - val_loss: 1.3201 - val_mae: 0.3748\n",
      "Epoch 149/600\n",
      "Epoch 149: Warmup phase (149/150) - val_mae: 0.3721\u001b[1m0s\u001b[0m 26ms/step - loss: 1.1465 - mae: 0.0679\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 1.1465 - mae: 0.0679 - val_loss: 1.3023 - val_mae: 0.3721\n",
      "Epoch 150/600\n",
      "Epoch 150: Warmup phase (150/150) - val_mae: 0.3695\u001b[1m0s\u001b[0m 22ms/step - loss: 1.1312 - mae: 0.0675\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.1312 - mae: 0.0675 - val_loss: 1.2848 - val_mae: 0.3695\n",
      "Epoch 151/600\n",
      "Epoch 151: val_mae improved to 0.3668\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.1161 - mae: 0.0670\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.1161 - mae: 0.0670 - val_loss: 1.2676 - val_mae: 0.3668\n",
      "Epoch 152/600\n",
      "Epoch 152: val_mae improved to 0.3642\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.1012 - mae: 0.0666\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.1012 - mae: 0.0666 - val_loss: 1.2507 - val_mae: 0.3642\n",
      "Epoch 153/600\n",
      "Epoch 153: val_mae improved to 0.3615\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.0865 - mae: 0.0663\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 1.0865 - mae: 0.0663 - val_loss: 1.2340 - val_mae: 0.3615\n",
      "Epoch 154/600\n",
      "Epoch 154: val_mae improved to 0.3589\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.0720 - mae: 0.0659\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 1.0720 - mae: 0.0659 - val_loss: 1.2175 - val_mae: 0.3589\n",
      "Epoch 155/600\n",
      "Epoch 155: val_mae improved to 0.3562\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0578 - mae: 0.0655\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.0578 - mae: 0.0655 - val_loss: 1.2013 - val_mae: 0.3562\n",
      "Epoch 156/600\n",
      "Epoch 156: val_mae improved to 0.3535\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0437 - mae: 0.0652\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.0437 - mae: 0.0652 - val_loss: 1.1853 - val_mae: 0.3535\n",
      "Epoch 157/600\n",
      "Epoch 157: val_mae improved to 0.3508\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0299 - mae: 0.0648\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.0299 - mae: 0.0648 - val_loss: 1.1695 - val_mae: 0.3508\n",
      "Epoch 158/600\n",
      "Epoch 158: val_mae improved to 0.3481\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.0162 - mae: 0.0645\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 1.0162 - mae: 0.0645 - val_loss: 1.1539 - val_mae: 0.3481\n",
      "Epoch 159/600\n",
      "Epoch 159: val_mae improved to 0.3455\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.0027 - mae: 0.0642\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 1.0027 - mae: 0.0642 - val_loss: 1.1386 - val_mae: 0.3455\n",
      "Epoch 160/600\n",
      "Epoch 160: val_mae improved to 0.3427\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.9895 - mae: 0.0638\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.9895 - mae: 0.0638 - val_loss: 1.1234 - val_mae: 0.3427\n",
      "Epoch 161/600\n",
      "Epoch 161: val_mae improved to 0.3400\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.9764 - mae: 0.0636\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.9764 - mae: 0.0636 - val_loss: 1.1084 - val_mae: 0.3400\n",
      "Epoch 162/600\n",
      "Epoch 162: val_mae improved to 0.3374\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.9635 - mae: 0.0633\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.9635 - mae: 0.0633 - val_loss: 1.0937 - val_mae: 0.3374\n",
      "Epoch 163/600\n",
      "Epoch 163: val_mae improved to 0.3347\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.9509 - mae: 0.0630\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.9509 - mae: 0.0630 - val_loss: 1.0791 - val_mae: 0.3347\n",
      "Epoch 164/600\n",
      "Epoch 164: val_mae improved to 0.3320\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.9383 - mae: 0.0627\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.9383 - mae: 0.0627 - val_loss: 1.0647 - val_mae: 0.3320\n",
      "Epoch 165/600\n",
      "Epoch 165: val_mae improved to 0.3294\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.9260 - mae: 0.0624\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.9260 - mae: 0.0624 - val_loss: 1.0506 - val_mae: 0.3294\n",
      "Epoch 166/600\n",
      "Epoch 166: val_mae improved to 0.3267\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.9139 - mae: 0.0620\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.9139 - mae: 0.0620 - val_loss: 1.0366 - val_mae: 0.3267\n",
      "Epoch 167/600\n",
      "Epoch 167: val_mae improved to 0.3240\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.9019 - mae: 0.0617\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.9019 - mae: 0.0617 - val_loss: 1.0228 - val_mae: 0.3240\n",
      "Epoch 168/600\n",
      "Epoch 168: val_mae improved to 0.3214\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.8901 - mae: 0.0613\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8901 - mae: 0.0613 - val_loss: 1.0092 - val_mae: 0.3214\n",
      "Epoch 169/600\n",
      "Epoch 169: val_mae improved to 0.3188\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.8784 - mae: 0.0609\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.8784 - mae: 0.0609 - val_loss: 0.9958 - val_mae: 0.3188\n",
      "Epoch 170/600\n",
      "Epoch 170: val_mae improved to 0.3162\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.8670 - mae: 0.0606\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8670 - mae: 0.0606 - val_loss: 0.9825 - val_mae: 0.3162\n",
      "Epoch 171/600\n",
      "Epoch 171: val_mae improved to 0.3135\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.8557 - mae: 0.0602\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.8557 - mae: 0.0602 - val_loss: 0.9695 - val_mae: 0.3135\n",
      "Epoch 172/600\n",
      "Epoch 172: val_mae improved to 0.3110\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.8445 - mae: 0.0598\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8445 - mae: 0.0598 - val_loss: 0.9566 - val_mae: 0.3110\n",
      "Epoch 173/600\n",
      "Epoch 173: val_mae improved to 0.3084\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.8336 - mae: 0.0594\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.8336 - mae: 0.0594 - val_loss: 0.9439 - val_mae: 0.3084\n",
      "Epoch 174/600\n",
      "Epoch 174: val_mae improved to 0.3059\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.8227 - mae: 0.0590\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.8227 - mae: 0.0590 - val_loss: 0.9314 - val_mae: 0.3059\n",
      "Epoch 175/600\n",
      "Epoch 175: val_mae improved to 0.3033\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.8121 - mae: 0.0586\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8121 - mae: 0.0586 - val_loss: 0.9190 - val_mae: 0.3033\n",
      "Epoch 176/600\n",
      "Epoch 176: val_mae improved to 0.3007\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.8016 - mae: 0.0583\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8016 - mae: 0.0583 - val_loss: 0.9068 - val_mae: 0.3007\n",
      "Epoch 177/600\n",
      "Epoch 177: val_mae improved to 0.2982\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.7912 - mae: 0.0580\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7912 - mae: 0.0580 - val_loss: 0.8948 - val_mae: 0.2982\n",
      "Epoch 178/600\n",
      "Epoch 178: val_mae improved to 0.2958\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.7810 - mae: 0.0576\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7810 - mae: 0.0576 - val_loss: 0.8830 - val_mae: 0.2958\n",
      "Epoch 179/600\n",
      "Epoch 179: val_mae improved to 0.2933\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.7710 - mae: 0.0572\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.7710 - mae: 0.0572 - val_loss: 0.8713 - val_mae: 0.2933\n",
      "Epoch 180/600\n",
      "Epoch 180: val_mae improved to 0.2908\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.7610 - mae: 0.0569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7610 - mae: 0.0569 - val_loss: 0.8598 - val_mae: 0.2908\n",
      "Epoch 181/600\n",
      "Epoch 181: val_mae improved to 0.2883\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.7513 - mae: 0.0566\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.7513 - mae: 0.0566 - val_loss: 0.8485 - val_mae: 0.2883\n",
      "Epoch 182/600\n",
      "Epoch 182: val_mae improved to 0.2858\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.7416 - mae: 0.0563\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7416 - mae: 0.0563 - val_loss: 0.8372 - val_mae: 0.2858\n",
      "Epoch 183/600\n",
      "Epoch 183: val_mae improved to 0.2834\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.7321 - mae: 0.0560\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.7321 - mae: 0.0560 - val_loss: 0.8262 - val_mae: 0.2834\n",
      "Epoch 184/600\n",
      "Epoch 184: val_mae improved to 0.2809\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.7228 - mae: 0.0557\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.7228 - mae: 0.0557 - val_loss: 0.8153 - val_mae: 0.2809\n",
      "Epoch 185/600\n",
      "Epoch 185: val_mae improved to 0.2785\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.7136 - mae: 0.0554\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7136 - mae: 0.0554 - val_loss: 0.8045 - val_mae: 0.2785\n",
      "Epoch 186/600\n",
      "Epoch 186: val_mae improved to 0.2760\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.7045 - mae: 0.0551\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7045 - mae: 0.0551 - val_loss: 0.7939 - val_mae: 0.2760\n",
      "Epoch 187/600\n",
      "Epoch 187: val_mae improved to 0.2735\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.6955 - mae: 0.0549\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.6955 - mae: 0.0549 - val_loss: 0.7834 - val_mae: 0.2735\n",
      "Epoch 188/600\n",
      "Epoch 188: val_mae improved to 0.2710\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.6867 - mae: 0.0546\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.6867 - mae: 0.0546 - val_loss: 0.7731 - val_mae: 0.2710\n",
      "Epoch 189/600\n",
      "Epoch 189: val_mae improved to 0.2686\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.6780 - mae: 0.0543\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.6780 - mae: 0.0543 - val_loss: 0.7629 - val_mae: 0.2686\n",
      "Epoch 190/600\n",
      "Epoch 190: val_mae improved to 0.2662\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.6694 - mae: 0.0540\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.6694 - mae: 0.0540 - val_loss: 0.7529 - val_mae: 0.2662\n",
      "Epoch 191/600\n",
      "Epoch 191: val_mae improved to 0.2638\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.6609 - mae: 0.0537\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6609 - mae: 0.0537 - val_loss: 0.7430 - val_mae: 0.2638\n",
      "Epoch 192/600\n",
      "Epoch 192: val_mae improved to 0.2614\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.6526 - mae: 0.0536\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.6526 - mae: 0.0536 - val_loss: 0.7332 - val_mae: 0.2614\n",
      "Epoch 193/600\n",
      "Epoch 193: val_mae improved to 0.2590\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.6444 - mae: 0.0533\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.6444 - mae: 0.0533 - val_loss: 0.7235 - val_mae: 0.2590\n",
      "Epoch 194/600\n",
      "Epoch 194: val_mae improved to 0.2566\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.6363 - mae: 0.0530\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.6363 - mae: 0.0530 - val_loss: 0.7139 - val_mae: 0.2566\n",
      "Epoch 195/600\n",
      "Epoch 195: val_mae improved to 0.2543\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.6283 - mae: 0.0526\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.6283 - mae: 0.0526 - val_loss: 0.7046 - val_mae: 0.2543\n",
      "Epoch 196/600\n",
      "Epoch 196: val_mae improved to 0.2518\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.6205 - mae: 0.0523\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6205 - mae: 0.0523 - val_loss: 0.6952 - val_mae: 0.2518\n",
      "Epoch 197/600\n",
      "Epoch 197: val_mae improved to 0.2495\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.6127 - mae: 0.0519\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.6127 - mae: 0.0519 - val_loss: 0.6861 - val_mae: 0.2495\n",
      "Epoch 198/600\n",
      "Epoch 198: val_mae improved to 0.2471\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.6051 - mae: 0.0516\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.6051 - mae: 0.0516 - val_loss: 0.6770 - val_mae: 0.2471\n",
      "Epoch 199/600\n",
      "Epoch 199: val_mae improved to 0.2448\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.5975 - mae: 0.0514\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.5975 - mae: 0.0514 - val_loss: 0.6682 - val_mae: 0.2448\n",
      "Epoch 200/600\n",
      "Epoch 200: val_mae improved to 0.2425\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5901 - mae: 0.0510\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.5901 - mae: 0.0510 - val_loss: 0.6594 - val_mae: 0.2425\n",
      "Epoch 201/600\n",
      "Epoch 201: val_mae improved to 0.2401\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5828 - mae: 0.0507\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.5828 - mae: 0.0507 - val_loss: 0.6508 - val_mae: 0.2401\n",
      "Epoch 202/600\n",
      "Epoch 202: val_mae improved to 0.2379\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.5756 - mae: 0.0504\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.5756 - mae: 0.0504 - val_loss: 0.6422 - val_mae: 0.2379\n",
      "Epoch 203/600\n",
      "Epoch 203: val_mae improved to 0.2356\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5685 - mae: 0.0500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.5685 - mae: 0.0500 - val_loss: 0.6338 - val_mae: 0.2356\n",
      "Epoch 204/600\n",
      "Epoch 204: val_mae improved to 0.2333\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.5615 - mae: 0.0498\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.5615 - mae: 0.0498 - val_loss: 0.6256 - val_mae: 0.2333\n",
      "Epoch 205/600\n",
      "Epoch 205: val_mae improved to 0.2310\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5546 - mae: 0.0495\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.5546 - mae: 0.0495 - val_loss: 0.6174 - val_mae: 0.2310\n",
      "Epoch 206/600\n",
      "Epoch 206: val_mae improved to 0.2288\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.5478 - mae: 0.0493\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.5478 - mae: 0.0493 - val_loss: 0.6094 - val_mae: 0.2288\n",
      "Epoch 207/600\n",
      "Epoch 207: val_mae improved to 0.2265\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5411 - mae: 0.0490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.5411 - mae: 0.0490 - val_loss: 0.6014 - val_mae: 0.2265\n",
      "Epoch 208/600\n",
      "Epoch 208: val_mae improved to 0.2244\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5344 - mae: 0.0487\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.5344 - mae: 0.0487 - val_loss: 0.5937 - val_mae: 0.2244\n",
      "Epoch 209/600\n",
      "Epoch 209: val_mae improved to 0.2221\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.5279 - mae: 0.0485\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.5279 - mae: 0.0485 - val_loss: 0.5859 - val_mae: 0.2221\n",
      "Epoch 210/600\n",
      "Epoch 210: val_mae improved to 0.2199\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5215 - mae: 0.0482\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.5215 - mae: 0.0482 - val_loss: 0.5784 - val_mae: 0.2199\n",
      "Epoch 211/600\n",
      "Epoch 211: val_mae improved to 0.2178\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5151 - mae: 0.0479\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.5151 - mae: 0.0479 - val_loss: 0.5709 - val_mae: 0.2178\n",
      "Epoch 212/600\n",
      "Epoch 212: val_mae improved to 0.2155\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5089 - mae: 0.0477\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5089 - mae: 0.0477 - val_loss: 0.5635 - val_mae: 0.2155\n",
      "Epoch 213/600\n",
      "Epoch 213: val_mae improved to 0.2134\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5027 - mae: 0.0474\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.5027 - mae: 0.0474 - val_loss: 0.5563 - val_mae: 0.2134\n",
      "Epoch 214/600\n",
      "Epoch 214: val_mae improved to 0.2112\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.4967 - mae: 0.0472\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.4967 - mae: 0.0472 - val_loss: 0.5491 - val_mae: 0.2112\n",
      "Epoch 215/600\n",
      "Epoch 215: val_mae improved to 0.2090\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4907 - mae: 0.0469\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4907 - mae: 0.0469 - val_loss: 0.5421 - val_mae: 0.2090\n",
      "Epoch 216/600\n",
      "Epoch 216: val_mae improved to 0.2068\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4848 - mae: 0.0467\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4848 - mae: 0.0467 - val_loss: 0.5351 - val_mae: 0.2068\n",
      "Epoch 217/600\n",
      "Epoch 217: val_mae improved to 0.2047\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.4790 - mae: 0.0464\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.4790 - mae: 0.0464 - val_loss: 0.5282 - val_mae: 0.2047\n",
      "Epoch 218/600\n",
      "Epoch 218: val_mae improved to 0.2025\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4733 - mae: 0.0462\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.4733 - mae: 0.0462 - val_loss: 0.5215 - val_mae: 0.2025\n",
      "Epoch 219/600\n",
      "Epoch 219: val_mae improved to 0.2004\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.4676 - mae: 0.0459\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.4676 - mae: 0.0459 - val_loss: 0.5148 - val_mae: 0.2004\n",
      "Epoch 220/600\n",
      "Epoch 220: val_mae improved to 0.1984\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4620 - mae: 0.0456\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4620 - mae: 0.0456 - val_loss: 0.5082 - val_mae: 0.1984\n",
      "Epoch 221/600\n",
      "Epoch 221: val_mae improved to 0.1962\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4565 - mae: 0.0453\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.4565 - mae: 0.0453 - val_loss: 0.5017 - val_mae: 0.1962\n",
      "Epoch 222/600\n",
      "Epoch 222: val_mae improved to 0.1941\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4511 - mae: 0.0451\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4511 - mae: 0.0451 - val_loss: 0.4953 - val_mae: 0.1941\n",
      "Epoch 223/600\n",
      "Epoch 223: val_mae improved to 0.1920\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4458 - mae: 0.0448\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.4458 - mae: 0.0448 - val_loss: 0.4891 - val_mae: 0.1920\n",
      "Epoch 224/600\n",
      "Epoch 224: val_mae improved to 0.1900\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.4405 - mae: 0.0446\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.4405 - mae: 0.0446 - val_loss: 0.4828 - val_mae: 0.1900\n",
      "Epoch 225/600\n",
      "Epoch 225: val_mae improved to 0.1880\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.4353 - mae: 0.0444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4353 - mae: 0.0444 - val_loss: 0.4767 - val_mae: 0.1880\n",
      "Epoch 226/600\n",
      "Epoch 226: val_mae improved to 0.1859\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4302 - mae: 0.0441\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4302 - mae: 0.0441 - val_loss: 0.4707 - val_mae: 0.1859\n",
      "Epoch 227/600\n",
      "Epoch 227: val_mae improved to 0.1840\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4252 - mae: 0.0439\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.4252 - mae: 0.0439 - val_loss: 0.4648 - val_mae: 0.1840\n",
      "Epoch 228/600\n",
      "Epoch 228: val_mae improved to 0.1819\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4202 - mae: 0.0436\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.4202 - mae: 0.0436 - val_loss: 0.4589 - val_mae: 0.1819\n",
      "Epoch 229/600\n",
      "Epoch 229: val_mae improved to 0.1798\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.4153 - mae: 0.0433\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.4153 - mae: 0.0433 - val_loss: 0.4531 - val_mae: 0.1798\n",
      "Epoch 230/600\n",
      "Epoch 230: val_mae improved to 0.1778\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4105 - mae: 0.0431\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.4105 - mae: 0.0431 - val_loss: 0.4475 - val_mae: 0.1778\n",
      "Epoch 231/600\n",
      "Epoch 231: val_mae improved to 0.1758\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.4057 - mae: 0.0428\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4057 - mae: 0.0428 - val_loss: 0.4418 - val_mae: 0.1758\n",
      "Epoch 232/600\n",
      "Epoch 232: val_mae improved to 0.1738\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4010 - mae: 0.0426\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.4010 - mae: 0.0426 - val_loss: 0.4363 - val_mae: 0.1738\n",
      "Epoch 233/600\n",
      "Epoch 233: val_mae improved to 0.1719\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.3964 - mae: 0.0423\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.3964 - mae: 0.0423 - val_loss: 0.4309 - val_mae: 0.1719\n",
      "Epoch 234/600\n",
      "Epoch 234: val_mae improved to 0.1700\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.3918 - mae: 0.0420\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.3918 - mae: 0.0420 - val_loss: 0.4256 - val_mae: 0.1700\n",
      "Epoch 235/600\n",
      "Epoch 235: val_mae improved to 0.1681\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3873 - mae: 0.0418\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.3873 - mae: 0.0418 - val_loss: 0.4203 - val_mae: 0.1681\n",
      "Epoch 236/600\n",
      "Epoch 236: val_mae improved to 0.1661\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3828 - mae: 0.0415\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.3828 - mae: 0.0415 - val_loss: 0.4151 - val_mae: 0.1661\n",
      "Epoch 237/600\n",
      "Epoch 237: val_mae improved to 0.1642\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3785 - mae: 0.0413\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.3785 - mae: 0.0413 - val_loss: 0.4100 - val_mae: 0.1642\n",
      "Epoch 238/600\n",
      "Epoch 238: val_mae improved to 0.1624\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.3741 - mae: 0.0411\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.3741 - mae: 0.0411 - val_loss: 0.4049 - val_mae: 0.1624\n",
      "Epoch 239/600\n",
      "Epoch 239: val_mae improved to 0.1605\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.3699 - mae: 0.0408\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.3699 - mae: 0.0408 - val_loss: 0.4000 - val_mae: 0.1605\n",
      "Epoch 240/600\n",
      "Epoch 240: val_mae improved to 0.1587\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.3657 - mae: 0.0405\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.3657 - mae: 0.0405 - val_loss: 0.3951 - val_mae: 0.1587\n",
      "Epoch 241/600\n",
      "Epoch 241: val_mae improved to 0.1568\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3615 - mae: 0.0403\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.3615 - mae: 0.0403 - val_loss: 0.3903 - val_mae: 0.1568\n",
      "Epoch 242/600\n",
      "Epoch 242: val_mae improved to 0.1550\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3574 - mae: 0.0402\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3574 - mae: 0.0402 - val_loss: 0.3855 - val_mae: 0.1550\n",
      "Epoch 243/600\n",
      "Epoch 243: val_mae improved to 0.1532\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.3534 - mae: 0.0400\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.3534 - mae: 0.0400 - val_loss: 0.3808 - val_mae: 0.1532\n",
      "Epoch 244/600\n",
      "Epoch 244: val_mae improved to 0.1513\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3494 - mae: 0.0398\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3494 - mae: 0.0398 - val_loss: 0.3762 - val_mae: 0.1513\n",
      "Epoch 245/600\n",
      "Epoch 245: val_mae improved to 0.1495\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3455 - mae: 0.0397\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.3455 - mae: 0.0397 - val_loss: 0.3716 - val_mae: 0.1495\n",
      "Epoch 246/600\n",
      "Epoch 246: val_mae improved to 0.1477\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3416 - mae: 0.0395\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3416 - mae: 0.0395 - val_loss: 0.3672 - val_mae: 0.1477\n",
      "Epoch 247/600\n",
      "Epoch 247: val_mae improved to 0.1460\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3378 - mae: 0.0394\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.3378 - mae: 0.0394 - val_loss: 0.3628 - val_mae: 0.1460\n",
      "Epoch 248/600\n",
      "Epoch 248: val_mae improved to 0.1443\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.3340 - mae: 0.0392\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.3340 - mae: 0.0392 - val_loss: 0.3584 - val_mae: 0.1443\n",
      "Epoch 249/600\n",
      "Epoch 249: val_mae improved to 0.1424\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3303 - mae: 0.0391\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.3303 - mae: 0.0391 - val_loss: 0.3541 - val_mae: 0.1424\n",
      "Epoch 250/600\n",
      "Epoch 250: val_mae improved to 0.1407\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3267 - mae: 0.0389\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.3267 - mae: 0.0389 - val_loss: 0.3498 - val_mae: 0.1407\n",
      "Epoch 251/600\n",
      "Epoch 251: val_mae improved to 0.1390\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.3231 - mae: 0.0388\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3231 - mae: 0.0388 - val_loss: 0.3457 - val_mae: 0.1390\n",
      "Epoch 252/600\n",
      "Epoch 252: val_mae improved to 0.1372\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.3195 - mae: 0.0386\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.3195 - mae: 0.0386 - val_loss: 0.3415 - val_mae: 0.1372\n",
      "Epoch 253/600\n",
      "Epoch 253: val_mae improved to 0.1355\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.3160 - mae: 0.0385\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.3160 - mae: 0.0385 - val_loss: 0.3375 - val_mae: 0.1355\n",
      "Epoch 254/600\n",
      "Epoch 254: val_mae improved to 0.1339\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3125 - mae: 0.0382\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.3125 - mae: 0.0382 - val_loss: 0.3335 - val_mae: 0.1339\n",
      "Epoch 255/600\n",
      "Epoch 255: val_mae improved to 0.1323\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.3091 - mae: 0.0381\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3091 - mae: 0.0381 - val_loss: 0.3295 - val_mae: 0.1323\n",
      "Epoch 256/600\n",
      "Epoch 256: val_mae improved to 0.1307\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3057 - mae: 0.0380\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3057 - mae: 0.0380 - val_loss: 0.3256 - val_mae: 0.1307\n",
      "Epoch 257/600\n",
      "Epoch 257: val_mae improved to 0.1291\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3024 - mae: 0.0377\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.3024 - mae: 0.0377 - val_loss: 0.3218 - val_mae: 0.1291\n",
      "Epoch 258/600\n",
      "Epoch 258: val_mae improved to 0.1274\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2991 - mae: 0.0376\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.2991 - mae: 0.0376 - val_loss: 0.3180 - val_mae: 0.1274\n",
      "Epoch 259/600\n",
      "Epoch 259: val_mae improved to 0.1259\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2959 - mae: 0.0374\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2959 - mae: 0.0374 - val_loss: 0.3143 - val_mae: 0.1259\n",
      "Epoch 260/600\n",
      "Epoch 260: val_mae improved to 0.1242\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2927 - mae: 0.0373\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2927 - mae: 0.0373 - val_loss: 0.3106 - val_mae: 0.1242\n",
      "Epoch 261/600\n",
      "Epoch 261: val_mae improved to 0.1227\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2895 - mae: 0.0372\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.2895 - mae: 0.0372 - val_loss: 0.3070 - val_mae: 0.1227\n",
      "Epoch 262/600\n",
      "Epoch 262: val_mae improved to 0.1211\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2864 - mae: 0.0370\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2864 - mae: 0.0370 - val_loss: 0.3035 - val_mae: 0.1211\n",
      "Epoch 263/600\n",
      "Epoch 263: val_mae improved to 0.1195\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2833 - mae: 0.0369\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.2833 - mae: 0.0369 - val_loss: 0.2999 - val_mae: 0.1195\n",
      "Epoch 264/600\n",
      "Epoch 264: val_mae improved to 0.1180\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2803 - mae: 0.0367\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2803 - mae: 0.0367 - val_loss: 0.2965 - val_mae: 0.1180\n",
      "Epoch 265/600\n",
      "Epoch 265: val_mae improved to 0.1164\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2773 - mae: 0.0366\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2773 - mae: 0.0366 - val_loss: 0.2930 - val_mae: 0.1164\n",
      "Epoch 266/600\n",
      "Epoch 266: val_mae improved to 0.1149\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2743 - mae: 0.0364\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2743 - mae: 0.0364 - val_loss: 0.2897 - val_mae: 0.1149\n",
      "Epoch 267/600\n",
      "Epoch 267: val_mae improved to 0.1133\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2714 - mae: 0.0363\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2714 - mae: 0.0363 - val_loss: 0.2863 - val_mae: 0.1133\n",
      "Epoch 268/600\n",
      "Epoch 268: val_mae improved to 0.1118\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2686 - mae: 0.0361\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.2686 - mae: 0.0361 - val_loss: 0.2831 - val_mae: 0.1118\n",
      "Epoch 269/600\n",
      "Epoch 269: val_mae improved to 0.1103\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2657 - mae: 0.0360\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2657 - mae: 0.0360 - val_loss: 0.2798 - val_mae: 0.1103\n",
      "Epoch 270/600\n",
      "Epoch 270: val_mae improved to 0.1088\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2629 - mae: 0.0358\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2629 - mae: 0.0358 - val_loss: 0.2766 - val_mae: 0.1088\n",
      "Epoch 271/600\n",
      "Epoch 271: val_mae improved to 0.1074\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2602 - mae: 0.0356\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.2602 - mae: 0.0356 - val_loss: 0.2735 - val_mae: 0.1074\n",
      "Epoch 272/600\n",
      "Epoch 272: val_mae improved to 0.1059\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2574 - mae: 0.0355\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2574 - mae: 0.0355 - val_loss: 0.2704 - val_mae: 0.1059\n",
      "Epoch 273/600\n",
      "Epoch 273: val_mae improved to 0.1044\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2547 - mae: 0.0353\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.2547 - mae: 0.0353 - val_loss: 0.2673 - val_mae: 0.1044\n",
      "Epoch 274/600\n",
      "Epoch 274: val_mae improved to 0.1030\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2521 - mae: 0.0352\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2521 - mae: 0.0352 - val_loss: 0.2643 - val_mae: 0.1030\n",
      "Epoch 275/600\n",
      "Epoch 275: val_mae improved to 0.1016\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2495 - mae: 0.0350\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2495 - mae: 0.0350 - val_loss: 0.2614 - val_mae: 0.1016\n",
      "Epoch 276/600\n",
      "Epoch 276: val_mae improved to 0.1002\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2469 - mae: 0.0348\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2469 - mae: 0.0348 - val_loss: 0.2584 - val_mae: 0.1002\n",
      "Epoch 277/600\n",
      "Epoch 277: val_mae improved to 0.0987\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2443 - mae: 0.0347\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.2443 - mae: 0.0347 - val_loss: 0.2556 - val_mae: 0.0987\n",
      "Epoch 278/600\n",
      "Epoch 278: val_mae improved to 0.0974\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2418 - mae: 0.0345\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.2418 - mae: 0.0345 - val_loss: 0.2527 - val_mae: 0.0974\n",
      "Epoch 279/600\n",
      "Epoch 279: val_mae improved to 0.0959\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2393 - mae: 0.0343\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.2393 - mae: 0.0343 - val_loss: 0.2499 - val_mae: 0.0959\n",
      "Epoch 280/600\n",
      "Epoch 280: val_mae improved to 0.0947\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2369 - mae: 0.0341\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2369 - mae: 0.0341 - val_loss: 0.2471 - val_mae: 0.0947\n",
      "Epoch 281/600\n",
      "Epoch 281: val_mae improved to 0.0933\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2344 - mae: 0.0340\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2344 - mae: 0.0340 - val_loss: 0.2444 - val_mae: 0.0933\n",
      "Epoch 282/600\n",
      "Epoch 282: val_mae improved to 0.0919\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2320 - mae: 0.0339\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.2320 - mae: 0.0339 - val_loss: 0.2417 - val_mae: 0.0919\n",
      "Epoch 283/600\n",
      "Epoch 283: val_mae improved to 0.0906\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.2297 - mae: 0.0337\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 0.2297 - mae: 0.0337 - val_loss: 0.2391 - val_mae: 0.0906\n",
      "Epoch 284/600\n",
      "Epoch 284: val_mae improved to 0.0893\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2273 - mae: 0.0335\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2273 - mae: 0.0335 - val_loss: 0.2365 - val_mae: 0.0893\n",
      "Epoch 285/600\n",
      "Epoch 285: val_mae improved to 0.0880\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2250 - mae: 0.0334\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.2250 - mae: 0.0334 - val_loss: 0.2339 - val_mae: 0.0880\n",
      "Epoch 286/600\n",
      "Epoch 286: val_mae improved to 0.0867\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2228 - mae: 0.0332\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.2228 - mae: 0.0332 - val_loss: 0.2314 - val_mae: 0.0867\n",
      "Epoch 287/600\n",
      "Epoch 287: val_mae improved to 0.0854\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.2205 - mae: 0.0330\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.2205 - mae: 0.0330 - val_loss: 0.2289 - val_mae: 0.0854\n",
      "Epoch 288/600\n",
      "Epoch 288: val_mae improved to 0.0842\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2183 - mae: 0.0328\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.2183 - mae: 0.0328 - val_loss: 0.2264 - val_mae: 0.0842\n",
      "Epoch 289/600\n",
      "Epoch 289: val_mae improved to 0.0829\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2161 - mae: 0.0327\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2161 - mae: 0.0327 - val_loss: 0.2240 - val_mae: 0.0829\n",
      "Epoch 290/600\n",
      "Epoch 290: val_mae improved to 0.0817\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2140 - mae: 0.0325\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2140 - mae: 0.0325 - val_loss: 0.2216 - val_mae: 0.0817\n",
      "Epoch 291/600\n",
      "Epoch 291: val_mae improved to 0.0805\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2118 - mae: 0.0323\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2118 - mae: 0.0323 - val_loss: 0.2192 - val_mae: 0.0805\n",
      "Epoch 292/600\n",
      "Epoch 292: val_mae improved to 0.0794\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2097 - mae: 0.0322\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2097 - mae: 0.0322 - val_loss: 0.2169 - val_mae: 0.0794\n",
      "Epoch 293/600\n",
      "Epoch 293: val_mae improved to 0.0781\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2077 - mae: 0.0321\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.2077 - mae: 0.0321 - val_loss: 0.2146 - val_mae: 0.0781\n",
      "Epoch 294/600\n",
      "Epoch 294: val_mae improved to 0.0770\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2056 - mae: 0.0320\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.2056 - mae: 0.0320 - val_loss: 0.2123 - val_mae: 0.0770\n",
      "Epoch 295/600\n",
      "Epoch 295: val_mae improved to 0.0758\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2036 - mae: 0.0318\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2036 - mae: 0.0318 - val_loss: 0.2101 - val_mae: 0.0758\n",
      "Epoch 296/600\n",
      "Epoch 296: val_mae improved to 0.0747\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2016 - mae: 0.0316\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2016 - mae: 0.0316 - val_loss: 0.2079 - val_mae: 0.0747\n",
      "Epoch 297/600\n",
      "Epoch 297: val_mae improved to 0.0736\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1996 - mae: 0.0316\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1996 - mae: 0.0316 - val_loss: 0.2057 - val_mae: 0.0736\n",
      "Epoch 298/600\n",
      "Epoch 298: val_mae improved to 0.0725\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1976 - mae: 0.0314\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.1976 - mae: 0.0314 - val_loss: 0.2036 - val_mae: 0.0725\n",
      "Epoch 299/600\n",
      "Epoch 299: val_mae improved to 0.0714\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1957 - mae: 0.0312\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1957 - mae: 0.0312 - val_loss: 0.2014 - val_mae: 0.0714\n",
      "Epoch 300/600\n",
      "Epoch 300: val_mae improved to 0.0702\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1938 - mae: 0.0311\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1938 - mae: 0.0311 - val_loss: 0.1993 - val_mae: 0.0702\n",
      "Epoch 301/600\n",
      "Epoch 301: val_mae improved to 0.0692\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1919 - mae: 0.0309\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1919 - mae: 0.0309 - val_loss: 0.1973 - val_mae: 0.0692\n",
      "Epoch 302/600\n",
      "Epoch 302: val_mae improved to 0.0681\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1901 - mae: 0.0308\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1901 - mae: 0.0308 - val_loss: 0.1953 - val_mae: 0.0681\n",
      "Epoch 303/600\n",
      "Epoch 303: val_mae improved to 0.0671\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1882 - mae: 0.0306\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.1882 - mae: 0.0306 - val_loss: 0.1933 - val_mae: 0.0671\n",
      "Epoch 304/600\n",
      "Epoch 304: val_mae improved to 0.0660\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1864 - mae: 0.0305\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1864 - mae: 0.0305 - val_loss: 0.1913 - val_mae: 0.0660\n",
      "Epoch 305/600\n",
      "Epoch 305: val_mae improved to 0.0650\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1846 - mae: 0.0303\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1846 - mae: 0.0303 - val_loss: 0.1893 - val_mae: 0.0650\n",
      "Epoch 306/600\n",
      "Epoch 306: val_mae improved to 0.0640\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1829 - mae: 0.0302\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1829 - mae: 0.0302 - val_loss: 0.1874 - val_mae: 0.0640\n",
      "Epoch 307/600\n",
      "Epoch 307: val_mae improved to 0.0630\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1811 - mae: 0.0300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.1811 - mae: 0.0300 - val_loss: 0.1855 - val_mae: 0.0630\n",
      "Epoch 308/600\n",
      "Epoch 308: val_mae improved to 0.0620\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1794 - mae: 0.0299\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.1794 - mae: 0.0299 - val_loss: 0.1836 - val_mae: 0.0620\n",
      "Epoch 309/600\n",
      "Epoch 309: val_mae improved to 0.0610\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1777 - mae: 0.0298\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1777 - mae: 0.0298 - val_loss: 0.1818 - val_mae: 0.0610\n",
      "Epoch 310/600\n",
      "Epoch 310: val_mae improved to 0.0601\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1760 - mae: 0.0296\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1760 - mae: 0.0296 - val_loss: 0.1800 - val_mae: 0.0601\n",
      "Epoch 311/600\n",
      "Epoch 311: val_mae improved to 0.0591\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1744 - mae: 0.0296\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1744 - mae: 0.0296 - val_loss: 0.1782 - val_mae: 0.0591\n",
      "Epoch 312/600\n",
      "Epoch 312: val_mae improved to 0.0581\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1727 - mae: 0.0294\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1727 - mae: 0.0294 - val_loss: 0.1764 - val_mae: 0.0581\n",
      "Epoch 313/600\n",
      "Epoch 313: val_mae improved to 0.0572\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1711 - mae: 0.0293\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.1711 - mae: 0.0293 - val_loss: 0.1746 - val_mae: 0.0572\n",
      "Epoch 314/600\n",
      "Epoch 314: val_mae improved to 0.0562\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1695 - mae: 0.0291\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1695 - mae: 0.0291 - val_loss: 0.1729 - val_mae: 0.0562\n",
      "Epoch 315/600\n",
      "Epoch 315: val_mae improved to 0.0553\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1679 - mae: 0.0291\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1679 - mae: 0.0291 - val_loss: 0.1712 - val_mae: 0.0553\n",
      "Epoch 316/600\n",
      "Epoch 316: val_mae improved to 0.0544\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1664 - mae: 0.0290\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.1664 - mae: 0.0290 - val_loss: 0.1695 - val_mae: 0.0544\n",
      "Epoch 317/600\n",
      "Epoch 317: val_mae improved to 0.0534\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1648 - mae: 0.0288\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.1648 - mae: 0.0288 - val_loss: 0.1678 - val_mae: 0.0534\n",
      "Epoch 318/600\n",
      "Epoch 318: val_mae improved to 0.0526\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1633 - mae: 0.0288\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.1633 - mae: 0.0288 - val_loss: 0.1662 - val_mae: 0.0526\n",
      "Epoch 319/600\n",
      "Epoch 319: val_mae improved to 0.0516\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1618 - mae: 0.0287\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1618 - mae: 0.0287 - val_loss: 0.1646 - val_mae: 0.0516\n",
      "Epoch 320/600\n",
      "Epoch 320: val_mae improved to 0.0507\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1603 - mae: 0.0286\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1603 - mae: 0.0286 - val_loss: 0.1630 - val_mae: 0.0507\n",
      "Epoch 321/600\n",
      "Epoch 321: val_mae improved to 0.0498\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1588 - mae: 0.0285\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1588 - mae: 0.0285 - val_loss: 0.1614 - val_mae: 0.0498\n",
      "Epoch 322/600\n",
      "Epoch 322: val_mae improved to 0.0489\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1574 - mae: 0.0284\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1574 - mae: 0.0284 - val_loss: 0.1598 - val_mae: 0.0489\n",
      "Epoch 323/600\n",
      "Epoch 323: val_mae improved to 0.0480\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1560 - mae: 0.0283\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.1560 - mae: 0.0283 - val_loss: 0.1582 - val_mae: 0.0480\n",
      "Epoch 324/600\n",
      "Epoch 324: val_mae improved to 0.0471\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1545 - mae: 0.0281\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1545 - mae: 0.0281 - val_loss: 0.1567 - val_mae: 0.0471\n",
      "Epoch 325/600\n",
      "Epoch 325: val_mae improved to 0.0462\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1531 - mae: 0.0281\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1531 - mae: 0.0281 - val_loss: 0.1552 - val_mae: 0.0462\n",
      "Epoch 326/600\n",
      "Epoch 326: val_mae improved to 0.0453\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1518 - mae: 0.0279\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1518 - mae: 0.0279 - val_loss: 0.1537 - val_mae: 0.0453\n",
      "Epoch 327/600\n",
      "Epoch 327: val_mae improved to 0.0444\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1504 - mae: 0.0279\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1504 - mae: 0.0279 - val_loss: 0.1522 - val_mae: 0.0444\n",
      "Epoch 328/600\n",
      "Epoch 328: val_mae improved to 0.0436\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1490 - mae: 0.0278\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.1490 - mae: 0.0278 - val_loss: 0.1508 - val_mae: 0.0436\n",
      "Epoch 329/600\n",
      "Epoch 329: val_mae improved to 0.0431\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1477 - mae: 0.0277\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1477 - mae: 0.0277 - val_loss: 0.1493 - val_mae: 0.0431\n",
      "Epoch 330/600\n",
      "Epoch 330: val_mae improved to 0.0427\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1464 - mae: 0.0276\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1464 - mae: 0.0276 - val_loss: 0.1479 - val_mae: 0.0427\n",
      "Epoch 331/600\n",
      "Epoch 331: val_mae improved to 0.0422\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1451 - mae: 0.0276\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1451 - mae: 0.0276 - val_loss: 0.1465 - val_mae: 0.0422\n",
      "Epoch 332/600\n",
      "Epoch 332: val_mae improved to 0.0418\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1438 - mae: 0.0275\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1438 - mae: 0.0275 - val_loss: 0.1452 - val_mae: 0.0418\n",
      "Epoch 333/600\n",
      "Epoch 333: val_mae improved to 0.0413\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1425 - mae: 0.0274\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.1425 - mae: 0.0274 - val_loss: 0.1438 - val_mae: 0.0413\n",
      "Epoch 334/600\n",
      "Epoch 334: val_mae improved to 0.0409\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1413 - mae: 0.0273\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1413 - mae: 0.0273 - val_loss: 0.1425 - val_mae: 0.0409\n",
      "Epoch 335/600\n",
      "Epoch 335: val_mae improved to 0.0404\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1400 - mae: 0.0272\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1400 - mae: 0.0272 - val_loss: 0.1411 - val_mae: 0.0404\n",
      "Epoch 336/600\n",
      "Epoch 336: val_mae improved to 0.0400\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1388 - mae: 0.0272\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1388 - mae: 0.0272 - val_loss: 0.1398 - val_mae: 0.0400\n",
      "Epoch 337/600\n",
      "Epoch 337: val_mae improved to 0.0396\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1376 - mae: 0.0271\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.1376 - mae: 0.0271 - val_loss: 0.1385 - val_mae: 0.0396\n",
      "Epoch 338/600\n",
      "Epoch 338: val_mae improved to 0.0391\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1364 - mae: 0.0269\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.1364 - mae: 0.0269 - val_loss: 0.1373 - val_mae: 0.0391\n",
      "Epoch 339/600\n",
      "Epoch 339: val_mae improved to 0.0388\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1352 - mae: 0.0269\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1352 - mae: 0.0269 - val_loss: 0.1360 - val_mae: 0.0388\n",
      "Epoch 340/600\n",
      "Epoch 340: val_mae improved to 0.0383\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1340 - mae: 0.0268\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1340 - mae: 0.0268 - val_loss: 0.1348 - val_mae: 0.0383\n",
      "Epoch 341/600\n",
      "Epoch 341: val_mae improved to 0.0379\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1329 - mae: 0.0267\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1329 - mae: 0.0267 - val_loss: 0.1335 - val_mae: 0.0379\n",
      "Epoch 342/600\n",
      "Epoch 342: val_mae improved to 0.0375\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1317 - mae: 0.0266\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1317 - mae: 0.0266 - val_loss: 0.1323 - val_mae: 0.0375\n",
      "Epoch 343/600\n",
      "Epoch 343: val_mae improved to 0.0371\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1306 - mae: 0.0265\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.1306 - mae: 0.0265 - val_loss: 0.1311 - val_mae: 0.0371\n",
      "Epoch 344/600\n",
      "Epoch 344: val_mae improved to 0.0367\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1295 - mae: 0.0264\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1295 - mae: 0.0264 - val_loss: 0.1299 - val_mae: 0.0367\n",
      "Epoch 345/600\n",
      "Epoch 345: val_mae improved to 0.0363\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1284 - mae: 0.0264\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1284 - mae: 0.0264 - val_loss: 0.1288 - val_mae: 0.0363\n",
      "Epoch 346/600\n",
      "Epoch 346: val_mae improved to 0.0359\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1273 - mae: 0.0263\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1273 - mae: 0.0263 - val_loss: 0.1276 - val_mae: 0.0359\n",
      "Epoch 347/600\n",
      "Epoch 347: val_mae improved to 0.0355\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1262 - mae: 0.0262\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.1262 - mae: 0.0262 - val_loss: 0.1265 - val_mae: 0.0355\n",
      "Epoch 348/600\n",
      "Epoch 348: val_mae improved to 0.0352\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1251 - mae: 0.0261\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.1251 - mae: 0.0261 - val_loss: 0.1254 - val_mae: 0.0352\n",
      "Epoch 349/600\n",
      "Epoch 349: val_mae improved to 0.0348\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1241 - mae: 0.0261\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1241 - mae: 0.0261 - val_loss: 0.1242 - val_mae: 0.0348\n",
      "Epoch 350/600\n",
      "Epoch 350: val_mae improved to 0.0344\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1230 - mae: 0.0261\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1230 - mae: 0.0261 - val_loss: 0.1231 - val_mae: 0.0344\n",
      "Epoch 351/600\n",
      "Epoch 351: val_mae improved to 0.0340\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1220 - mae: 0.0260\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1220 - mae: 0.0260 - val_loss: 0.1221 - val_mae: 0.0340\n",
      "Epoch 352/600\n",
      "Epoch 352: val_mae improved to 0.0337\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1210 - mae: 0.0259\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1210 - mae: 0.0259 - val_loss: 0.1210 - val_mae: 0.0337\n",
      "Epoch 353/600\n",
      "Epoch 353: val_mae improved to 0.0333\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1200 - mae: 0.0258\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.1200 - mae: 0.0258 - val_loss: 0.1199 - val_mae: 0.0333\n",
      "Epoch 354/600\n",
      "Epoch 354: val_mae improved to 0.0329\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1190 - mae: 0.0258\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1190 - mae: 0.0258 - val_loss: 0.1189 - val_mae: 0.0329\n",
      "Epoch 355/600\n",
      "Epoch 355: val_mae improved to 0.0326\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1180 - mae: 0.0257\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1180 - mae: 0.0257 - val_loss: 0.1179 - val_mae: 0.0326\n",
      "Epoch 356/600\n",
      "Epoch 356: val_mae improved to 0.0323\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1170 - mae: 0.0257\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.1170 - mae: 0.0257 - val_loss: 0.1168 - val_mae: 0.0323\n",
      "Epoch 357/600\n",
      "Epoch 357: val_mae improved to 0.0320\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1160 - mae: 0.0256\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.1160 - mae: 0.0256 - val_loss: 0.1158 - val_mae: 0.0320\n",
      "Epoch 358/600\n",
      "Epoch 358: val_mae improved to 0.0316\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1151 - mae: 0.0255\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1151 - mae: 0.0255 - val_loss: 0.1148 - val_mae: 0.0316\n",
      "Epoch 359/600\n",
      "Epoch 359: val_mae improved to 0.0314\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1141 - mae: 0.0255\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1141 - mae: 0.0255 - val_loss: 0.1139 - val_mae: 0.0314\n",
      "Epoch 360/600\n",
      "Epoch 360: val_mae improved to 0.0310\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1132 - mae: 0.0254\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.1132 - mae: 0.0254 - val_loss: 0.1129 - val_mae: 0.0310\n",
      "Epoch 361/600\n",
      "Epoch 361: val_mae improved to 0.0307\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1123 - mae: 0.0254\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.1123 - mae: 0.0254 - val_loss: 0.1119 - val_mae: 0.0307\n",
      "Epoch 362/600\n",
      "Epoch 362: val_mae improved to 0.0304\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1113 - mae: 0.0253\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1113 - mae: 0.0253 - val_loss: 0.1110 - val_mae: 0.0304\n",
      "Epoch 363/600\n",
      "Epoch 363: val_mae improved to 0.0301\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1104 - mae: 0.0253\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1104 - mae: 0.0253 - val_loss: 0.1101 - val_mae: 0.0301\n",
      "Epoch 364/600\n",
      "Epoch 364: val_mae improved to 0.0298\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1095 - mae: 0.0252\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1095 - mae: 0.0252 - val_loss: 0.1091 - val_mae: 0.0298\n",
      "Epoch 365/600\n",
      "Epoch 365: val_mae improved to 0.0295\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1087 - mae: 0.0251\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.1087 - mae: 0.0251 - val_loss: 0.1082 - val_mae: 0.0295\n",
      "Epoch 366/600\n",
      "Epoch 366: val_mae improved to 0.0292\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1078 - mae: 0.0251\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.1078 - mae: 0.0251 - val_loss: 0.1073 - val_mae: 0.0292\n",
      "Epoch 367/600\n",
      "Epoch 367: val_mae improved to 0.0289\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1069 - mae: 0.0251\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1069 - mae: 0.0251 - val_loss: 0.1064 - val_mae: 0.0289\n",
      "Epoch 368/600\n",
      "Epoch 368: val_mae improved to 0.0286\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1061 - mae: 0.0250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1061 - mae: 0.0250 - val_loss: 0.1055 - val_mae: 0.0286\n",
      "Epoch 369/600\n",
      "Epoch 369: val_mae improved to 0.0284\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1052 - mae: 0.0249\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.1052 - mae: 0.0249 - val_loss: 0.1047 - val_mae: 0.0284\n",
      "Epoch 370/600\n",
      "Epoch 370: val_mae improved to 0.0281\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1044 - mae: 0.0249\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.1044 - mae: 0.0249 - val_loss: 0.1038 - val_mae: 0.0281\n",
      "Epoch 371/600\n",
      "Epoch 371: val_mae improved to 0.0278\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1035 - mae: 0.0248\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.1035 - mae: 0.0248 - val_loss: 0.1030 - val_mae: 0.0278\n",
      "Epoch 372/600\n",
      "Epoch 372: val_mae improved to 0.0276\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1027 - mae: 0.0248\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.1027 - mae: 0.0248 - val_loss: 0.1021 - val_mae: 0.0276\n",
      "Epoch 373/600\n",
      "Epoch 373: val_mae improved to 0.0273\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1019 - mae: 0.0247\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1019 - mae: 0.0247 - val_loss: 0.1013 - val_mae: 0.0273\n",
      "Epoch 374/600\n",
      "Epoch 374: val_mae improved to 0.0271\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1011 - mae: 0.0247\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1011 - mae: 0.0247 - val_loss: 0.1005 - val_mae: 0.0271\n",
      "Epoch 375/600\n",
      "Epoch 375: val_mae improved to 0.0268\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1003 - mae: 0.0246\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.1003 - mae: 0.0246 - val_loss: 0.0997 - val_mae: 0.0268\n",
      "Epoch 376/600\n",
      "Epoch 376: val_mae improved to 0.0266\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0995 - mae: 0.0246\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0995 - mae: 0.0246 - val_loss: 0.0989 - val_mae: 0.0266\n",
      "Epoch 377/600\n",
      "Epoch 377: val_mae improved to 0.0264\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0987 - mae: 0.0245\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0987 - mae: 0.0245 - val_loss: 0.0981 - val_mae: 0.0264\n",
      "Epoch 378/600\n",
      "Epoch 378: val_mae improved to 0.0262\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0980 - mae: 0.0245\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0980 - mae: 0.0245 - val_loss: 0.0973 - val_mae: 0.0262\n",
      "Epoch 379/600\n",
      "Epoch 379: val_mae improved to 0.0259\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0972 - mae: 0.0244\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0972 - mae: 0.0244 - val_loss: 0.0965 - val_mae: 0.0259\n",
      "Epoch 380/600\n",
      "Epoch 380: val_mae improved to 0.0257\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0965 - mae: 0.0244\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.0965 - mae: 0.0244 - val_loss: 0.0958 - val_mae: 0.0257\n",
      "Epoch 381/600\n",
      "Epoch 381: val_mae improved to 0.0255\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0957 - mae: 0.0243\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.0957 - mae: 0.0243 - val_loss: 0.0950 - val_mae: 0.0255\n",
      "Epoch 382/600\n",
      "Epoch 382: val_mae improved to 0.0253\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0950 - mae: 0.0243\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0950 - mae: 0.0243 - val_loss: 0.0943 - val_mae: 0.0253\n",
      "Epoch 383/600\n",
      "Epoch 383: val_mae improved to 0.0251\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0942 - mae: 0.0242\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0942 - mae: 0.0242 - val_loss: 0.0935 - val_mae: 0.0251\n",
      "Epoch 384/600\n",
      "Epoch 384: val_mae improved to 0.0249\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0935 - mae: 0.0241\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0935 - mae: 0.0241 - val_loss: 0.0928 - val_mae: 0.0249\n",
      "Epoch 385/600\n",
      "Epoch 385: val_mae improved to 0.0247\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0928 - mae: 0.0241\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.0928 - mae: 0.0241 - val_loss: 0.0921 - val_mae: 0.0247\n",
      "Epoch 386/600\n",
      "Epoch 386: val_mae improved to 0.0245\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0921 - mae: 0.0241\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0921 - mae: 0.0241 - val_loss: 0.0913 - val_mae: 0.0245\n",
      "Epoch 387/600\n",
      "Epoch 387: val_mae improved to 0.0243\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0914 - mae: 0.0240\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0914 - mae: 0.0240 - val_loss: 0.0906 - val_mae: 0.0243\n",
      "Epoch 388/600\n",
      "Epoch 388: val_mae improved to 0.0242\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0907 - mae: 0.0240\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0907 - mae: 0.0240 - val_loss: 0.0899 - val_mae: 0.0242\n",
      "Epoch 389/600\n",
      "Epoch 389: val_mae improved to 0.0240\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0900 - mae: 0.0239\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0900 - mae: 0.0239 - val_loss: 0.0893 - val_mae: 0.0240\n",
      "Epoch 390/600\n",
      "Epoch 390: val_mae improved to 0.0238\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0893 - mae: 0.0239\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.0893 - mae: 0.0239 - val_loss: 0.0886 - val_mae: 0.0238\n",
      "Epoch 391/600\n",
      "Epoch 391: val_mae improved to 0.0236\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0887 - mae: 0.0238\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0887 - mae: 0.0238 - val_loss: 0.0879 - val_mae: 0.0236\n",
      "Epoch 392/600\n",
      "Epoch 392: val_mae improved to 0.0235\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0880 - mae: 0.0238\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0880 - mae: 0.0238 - val_loss: 0.0872 - val_mae: 0.0235\n",
      "Epoch 393/600\n",
      "Epoch 393: val_mae improved to 0.0233\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0873 - mae: 0.0237\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0873 - mae: 0.0237 - val_loss: 0.0866 - val_mae: 0.0233\n",
      "Epoch 394/600\n",
      "Epoch 394: val_mae improved to 0.0231\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0867 - mae: 0.0237\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0867 - mae: 0.0237 - val_loss: 0.0859 - val_mae: 0.0231\n",
      "Epoch 395/600\n",
      "Epoch 395: val_mae improved to 0.0230\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0860 - mae: 0.0236\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.0860 - mae: 0.0236 - val_loss: 0.0853 - val_mae: 0.0230\n",
      "Epoch 396/600\n",
      "Epoch 396: val_mae improved to 0.0229\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0854 - mae: 0.0236\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0854 - mae: 0.0236 - val_loss: 0.0846 - val_mae: 0.0229\n",
      "Epoch 397/600\n",
      "Epoch 397: val_mae improved to 0.0227\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0848 - mae: 0.0235\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0848 - mae: 0.0235 - val_loss: 0.0840 - val_mae: 0.0227\n",
      "Epoch 398/600\n",
      "Epoch 398: val_mae improved to 0.0226\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0841 - mae: 0.0235\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0841 - mae: 0.0235 - val_loss: 0.0834 - val_mae: 0.0226\n",
      "Epoch 399/600\n",
      "Epoch 399: val_mae improved to 0.0225\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0835 - mae: 0.0235\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0835 - mae: 0.0235 - val_loss: 0.0828 - val_mae: 0.0225\n",
      "Epoch 400/600\n",
      "Epoch 400: val_mae improved to 0.0223\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0829 - mae: 0.0234\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.0829 - mae: 0.0234 - val_loss: 0.0822 - val_mae: 0.0223\n",
      "Epoch 401/600\n",
      "Epoch 401: val_mae improved to 0.0222\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0823 - mae: 0.0234\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0823 - mae: 0.0234 - val_loss: 0.0816 - val_mae: 0.0222\n",
      "Epoch 402/600\n",
      "Epoch 402: val_mae improved to 0.0221\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0817 - mae: 0.0233\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0817 - mae: 0.0233 - val_loss: 0.0810 - val_mae: 0.0221\n",
      "Epoch 403/600\n",
      "Epoch 403: val_mae improved to 0.0219\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0811 - mae: 0.0233\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0811 - mae: 0.0233 - val_loss: 0.0805 - val_mae: 0.0219\n",
      "Epoch 404/600\n",
      "Epoch 404: val_mae improved to 0.0218\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0806 - mae: 0.0233\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0806 - mae: 0.0233 - val_loss: 0.0799 - val_mae: 0.0218\n",
      "Epoch 405/600\n",
      "Epoch 405: val_mae improved to 0.0217\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0800 - mae: 0.0233\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.0800 - mae: 0.0233 - val_loss: 0.0793 - val_mae: 0.0217\n",
      "Epoch 406/600\n",
      "Epoch 406: val_mae improved to 0.0216\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0795 - mae: 0.0233\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0795 - mae: 0.0233 - val_loss: 0.0788 - val_mae: 0.0216\n",
      "Epoch 407/600\n",
      "Epoch 407: val_mae improved to 0.0216\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0789 - mae: 0.0233\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0789 - mae: 0.0233 - val_loss: 0.0783 - val_mae: 0.0216\n",
      "Epoch 408/600\n",
      "Epoch 408: val_mae improved to 0.0214\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0784 - mae: 0.0232\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0784 - mae: 0.0232 - val_loss: 0.0777 - val_mae: 0.0214\n",
      "Epoch 409/600\n",
      "Epoch 409: val_mae improved to 0.0213\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0778 - mae: 0.0232\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0778 - mae: 0.0232 - val_loss: 0.0772 - val_mae: 0.0213\n",
      "Epoch 410/600\n",
      "Epoch 410: val_mae improved to 0.0213\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0773 - mae: 0.0232\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.0773 - mae: 0.0232 - val_loss: 0.0767 - val_mae: 0.0213\n",
      "Epoch 411/600\n",
      "Epoch 411: val_mae improved to 0.0212\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0768 - mae: 0.0231\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0768 - mae: 0.0231 - val_loss: 0.0762 - val_mae: 0.0212\n",
      "Epoch 412/600\n",
      "Epoch 412: val_mae improved to 0.0211\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0763 - mae: 0.0232\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0763 - mae: 0.0232 - val_loss: 0.0756 - val_mae: 0.0211\n",
      "Epoch 413/600\n",
      "Epoch 413: val_mae improved to 0.0210\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0757 - mae: 0.0231\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0757 - mae: 0.0231 - val_loss: 0.0751 - val_mae: 0.0210\n",
      "Epoch 414/600\n",
      "Epoch 414: val_mae did not improve. Wait count: 1/5\u001b[1m0s\u001b[0m 22ms/step - loss: 0.0752 - mae: 0.0231\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0752 - mae: 0.0231 - val_loss: 0.0746 - val_mae: 0.0211\n",
      "Epoch 415/600\n",
      "Epoch 415: val_mae did not improve. Wait count: 2/5\u001b[1m0s\u001b[0m 23ms/step - loss: 0.0747 - mae: 0.0231\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 0.0747 - mae: 0.0231 - val_loss: 0.0741 - val_mae: 0.0212\n",
      "Epoch 416/600\n",
      "Epoch 416: val_mae did not improve. Wait count: 3/5\u001b[1m0s\u001b[0m 25ms/step - loss: 0.0742 - mae: 0.0231\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0742 - mae: 0.0231 - val_loss: 0.0736 - val_mae: 0.0213\n",
      "Epoch 417/600\n",
      "Epoch 417: val_mae did not improve. Wait count: 4/5\u001b[1m0s\u001b[0m 23ms/step - loss: 0.0737 - mae: 0.0231\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0737 - mae: 0.0231 - val_loss: 0.0731 - val_mae: 0.0213\n",
      "Epoch 418/600\n",
      "Epoch 418: val_mae did not improve. Wait count: 5/5\u001b[1m0s\u001b[0m 22ms/step - loss: 0.0732 - mae: 0.0230\n",
      "Restoring model weights from epoch 413\n",
      "EarlyStopping triggered at epoch 418\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.0732 - mae: 0.0230 - val_loss: 0.0727 - val_mae: 0.0215\n",
      "Training stopped at epoch 418 due to early stopping.\n"
     ]
    }
   ],
   "source": [
    "class EarlyStoppingWithWarmup(Callback):\n",
    "    def __init__(self, monitor='val_mae', mode='min', patience=5, warmup_epochs=150, restore_best_weights=True, verbose=1):\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.patience = patience\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "        self.stopped_epoch = 0\n",
    "        \n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.inf\n",
    "        else:\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            print(f\"Warning: EarlyStopping requires {self.monitor} available!\")\n",
    "            return\n",
    "        \n",
    "        # Warmup: Skip monitoring during the warmup period\n",
    "        if epoch < self.warmup_epochs:\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1}: Warmup phase ({epoch+1}/{self.warmup_epochs}) - {self.monitor}: {current:.4f}\")\n",
    "            return\n",
    "\n",
    "        if self.monitor_op(current, self.best):\n",
    "            self.best = current\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.wait = 0\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1}: {self.monitor} improved to {current:.4f}\")\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1}: {self.monitor} did not improve. Wait count: {self.wait}/{self.patience}\")\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                if self.restore_best_weights:\n",
    "                    if self.best_weights is not None:\n",
    "                        self.model.set_weights(self.best_weights)\n",
    "                        print(f\"Restoring model weights from epoch {self.stopped_epoch - self.patience + 1}\")\n",
    "                print(f\"EarlyStopping triggered at epoch {self.stopped_epoch + 1}\")\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0 and self.verbose:\n",
    "            print(f\"Training stopped at epoch {self.stopped_epoch + 1} due to early stopping.\")\n",
    "\n",
    "\n",
    "# Setting learning rate decay\n",
    "initial_lr = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    decay_steps=400,\n",
    "    decay_rate=0.95,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Create an EarlyStopping callback\n",
    "early_stop = EarlyStoppingWithWarmup(\n",
    "    monitor='val_mae',          # Metric to monitor\n",
    "    patience=5,                 # Wait 5 epochs after min before stopping\n",
    "    mode='min',                 # Stop when val_mae stops decreasing\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.06), input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.06)),\n",
    "    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.06)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=600, verbose=1, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dec18bd6-96ef-40ad-9492-ea83b03b40f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0751 - mae: 0.0210\n",
      "Validation MAE from model.evaluate: 0.0210\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "val_loss, val_mae = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation MAE from model.evaluate: {val_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9760d26-1c9c-4c8e-837f-4fbd52e82838",
   "metadata": {},
   "source": [
    "The MAE for training set is 0.0231 eV and validation set is 0.0210 eV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c9e3f88-67fe-4980-816d-ba34adf186bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml = pd.DataFrame({\n",
    "    'epoch': np.arange(1, len(history.history['loss']) + 1),\n",
    "    'training loss': history.history['loss'],\n",
    "    'training mae': history.history['mae'],\n",
    "    'validation loss': history.history['val_loss'],\n",
    "    'validation mae': history.history['val_mae']\n",
    "})\n",
    "\n",
    "df_ml.to_csv('training_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bf52dcf-1978-4935-9fc5-7d7b2e63fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_target</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.791583</td>\n",
       "      <td>1.765288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.722750</td>\n",
       "      <td>1.743398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.707833</td>\n",
       "      <td>1.722822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.774417</td>\n",
       "      <td>1.762145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.790833</td>\n",
       "      <td>1.723104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.753917</td>\n",
       "      <td>1.723392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.759083</td>\n",
       "      <td>1.725825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.724667</td>\n",
       "      <td>1.725800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.755583</td>\n",
       "      <td>1.725352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.720583</td>\n",
       "      <td>1.713421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.681583</td>\n",
       "      <td>1.705170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.687500</td>\n",
       "      <td>1.699356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.718583</td>\n",
       "      <td>1.700636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.699333</td>\n",
       "      <td>1.673435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.680250</td>\n",
       "      <td>1.663622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.670000</td>\n",
       "      <td>1.657615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.638250</td>\n",
       "      <td>1.640926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.709167</td>\n",
       "      <td>1.648203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.672500</td>\n",
       "      <td>1.646928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.586167</td>\n",
       "      <td>1.636616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.622583</td>\n",
       "      <td>1.622533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.619417</td>\n",
       "      <td>1.630807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.607417</td>\n",
       "      <td>1.624956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    y_target    y_pred\n",
       "0   1.791583  1.765288\n",
       "1   1.722750  1.743398\n",
       "2   1.707833  1.722822\n",
       "3   1.774417  1.762145\n",
       "4   1.790833  1.723104\n",
       "5   1.753917  1.723392\n",
       "6   1.759083  1.725825\n",
       "7   1.724667  1.725800\n",
       "8   1.755583  1.725352\n",
       "9   1.720583  1.713421\n",
       "10  1.681583  1.705170\n",
       "11  1.687500  1.699356\n",
       "12  1.718583  1.700636\n",
       "13  1.699333  1.673435\n",
       "14  1.680250  1.663622\n",
       "15  1.670000  1.657615\n",
       "16  1.638250  1.640926\n",
       "17  1.709167  1.648203\n",
       "18  1.672500  1.646928\n",
       "19  1.586167  1.636616\n",
       "20  1.622583  1.622533\n",
       "21  1.619417  1.630807\n",
       "22  1.607417  1.624956"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_scaled)\n",
    "\n",
    "df_mlresult = pd.DataFrame({\n",
    "    'y_target': np.ravel(y),  # Flatten y to 1D too, if needed\n",
    "    'y_pred': np.ravel(y_pred)\n",
    "})\n",
    "df_mlresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5c03bfe-d864-44c5-8a47-6d6223e0bf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "mu      0.118 +/- 0.131\n",
      "t       0.047 +/- 0.055\n",
      "mu/t    0.045 +/- 0.040\n",
      "mu/RA   0.041 +/- 0.034\n",
      "RA      0.034 +/- 0.060\n",
      "RA*t    0.025 +/- 0.050\n",
      "XB      0.014 +/- 0.028\n",
      "XA      0.001 +/- 0.040\n",
      "Nd      0.001 +/- 0.126\n",
      "QA      -0.003 +/- 0.026\n"
     ]
    }
   ],
   "source": [
    "r = permutation_importance(model, X_val, y_val,\n",
    "                           scoring='r2',\n",
    "                           n_repeats=30,\n",
    "                           random_state=0)\n",
    "\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    print(f\"{features[i]:<8}\"\n",
    "            f\"{r.importances_mean[i]:.3f}\"\n",
    "            f\" +/- {r.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccbe404-a0cf-442f-b565-e70ea21d4821",
   "metadata": {},
   "source": [
    "The new features mu/t and mu/RA make significant contribution to the model performance. Therefore, they cen be added to the feature pool for gplearn analysis to generate more specific mathematical formula to map these features to the OER activities. The result obtained by such neural network study aligns with the results obtained only by gplearn as demonstrated in the paper: Weng, B., Song, Z., Zhu, R. et al. Simple descriptor derived from symbolic regression accelerating the discovery of new perovskite catalysts. Nat Commun 11, 3513 (2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e52cd2aa-ca83-4b3e-9624-547446054a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.importances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edad112d-6eb2-4c53-bf82-57f1eeeb868a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>mu</th>\n",
       "      <th>RA</th>\n",
       "      <th>XA</th>\n",
       "      <th>XB</th>\n",
       "      <th>QA</th>\n",
       "      <th>Nd</th>\n",
       "      <th>mu/t</th>\n",
       "      <th>RA*t</th>\n",
       "      <th>mu/RA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.113791</td>\n",
       "      <td>0.006610</td>\n",
       "      <td>0.120206</td>\n",
       "      <td>-0.020125</td>\n",
       "      <td>-0.016782</td>\n",
       "      <td>-0.005156</td>\n",
       "      <td>-0.079315</td>\n",
       "      <td>0.047743</td>\n",
       "      <td>0.103825</td>\n",
       "      <td>0.043477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.117087</td>\n",
       "      <td>0.347818</td>\n",
       "      <td>0.063372</td>\n",
       "      <td>-0.010459</td>\n",
       "      <td>-0.028582</td>\n",
       "      <td>-0.022199</td>\n",
       "      <td>0.175677</td>\n",
       "      <td>0.113774</td>\n",
       "      <td>0.053104</td>\n",
       "      <td>0.102599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.003665</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>-0.055759</td>\n",
       "      <td>0.044438</td>\n",
       "      <td>-0.028347</td>\n",
       "      <td>-0.191259</td>\n",
       "      <td>-0.015435</td>\n",
       "      <td>-0.019450</td>\n",
       "      <td>0.004882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.020899</td>\n",
       "      <td>0.258272</td>\n",
       "      <td>-0.026930</td>\n",
       "      <td>-0.034422</td>\n",
       "      <td>0.018160</td>\n",
       "      <td>-0.033434</td>\n",
       "      <td>-0.028641</td>\n",
       "      <td>0.063193</td>\n",
       "      <td>-0.014125</td>\n",
       "      <td>0.044385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022605</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>0.062245</td>\n",
       "      <td>0.084474</td>\n",
       "      <td>0.040626</td>\n",
       "      <td>0.053589</td>\n",
       "      <td>-0.180993</td>\n",
       "      <td>0.004736</td>\n",
       "      <td>0.032192</td>\n",
       "      <td>0.025287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.117087</td>\n",
       "      <td>0.347818</td>\n",
       "      <td>0.063372</td>\n",
       "      <td>-0.010459</td>\n",
       "      <td>-0.028582</td>\n",
       "      <td>-0.022199</td>\n",
       "      <td>0.175677</td>\n",
       "      <td>0.113774</td>\n",
       "      <td>0.053104</td>\n",
       "      <td>0.102599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.032882</td>\n",
       "      <td>0.151308</td>\n",
       "      <td>-0.059979</td>\n",
       "      <td>0.078222</td>\n",
       "      <td>0.043598</td>\n",
       "      <td>0.016652</td>\n",
       "      <td>-0.103581</td>\n",
       "      <td>0.045255</td>\n",
       "      <td>-0.048300</td>\n",
       "      <td>0.015323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.129761</td>\n",
       "      <td>0.235697</td>\n",
       "      <td>0.097267</td>\n",
       "      <td>-0.038551</td>\n",
       "      <td>0.035967</td>\n",
       "      <td>-0.043525</td>\n",
       "      <td>-0.042123</td>\n",
       "      <td>0.088222</td>\n",
       "      <td>0.075286</td>\n",
       "      <td>0.100528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.063562</td>\n",
       "      <td>-0.078326</td>\n",
       "      <td>0.102609</td>\n",
       "      <td>-0.007967</td>\n",
       "      <td>0.069732</td>\n",
       "      <td>-0.000634</td>\n",
       "      <td>-0.265688</td>\n",
       "      <td>0.006058</td>\n",
       "      <td>0.069624</td>\n",
       "      <td>0.026787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.029390</td>\n",
       "      <td>0.116197</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>-0.006809</td>\n",
       "      <td>-0.001813</td>\n",
       "      <td>0.010948</td>\n",
       "      <td>0.162430</td>\n",
       "      <td>0.052868</td>\n",
       "      <td>-0.004101</td>\n",
       "      <td>0.030935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.129761</td>\n",
       "      <td>0.235697</td>\n",
       "      <td>0.097267</td>\n",
       "      <td>-0.038551</td>\n",
       "      <td>0.035967</td>\n",
       "      <td>-0.043525</td>\n",
       "      <td>-0.042123</td>\n",
       "      <td>0.088222</td>\n",
       "      <td>0.075286</td>\n",
       "      <td>0.100528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.029390</td>\n",
       "      <td>0.116197</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>-0.006809</td>\n",
       "      <td>-0.001813</td>\n",
       "      <td>0.010948</td>\n",
       "      <td>0.162430</td>\n",
       "      <td>0.052868</td>\n",
       "      <td>-0.004101</td>\n",
       "      <td>0.030935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.006301</td>\n",
       "      <td>0.029608</td>\n",
       "      <td>-0.009964</td>\n",
       "      <td>-0.003685</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>-0.003979</td>\n",
       "      <td>-0.014490</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>-0.010887</td>\n",
       "      <td>-0.002015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.107490</td>\n",
       "      <td>0.036219</td>\n",
       "      <td>0.110242</td>\n",
       "      <td>-0.023811</td>\n",
       "      <td>-0.014776</td>\n",
       "      <td>-0.009135</td>\n",
       "      <td>-0.093805</td>\n",
       "      <td>0.051196</td>\n",
       "      <td>0.092938</td>\n",
       "      <td>0.041463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.009484</td>\n",
       "      <td>0.255675</td>\n",
       "      <td>-0.037733</td>\n",
       "      <td>-0.044137</td>\n",
       "      <td>0.015660</td>\n",
       "      <td>-0.046566</td>\n",
       "      <td>-0.047333</td>\n",
       "      <td>0.058752</td>\n",
       "      <td>-0.034102</td>\n",
       "      <td>0.042350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.169081</td>\n",
       "      <td>0.256793</td>\n",
       "      <td>0.163938</td>\n",
       "      <td>0.026733</td>\n",
       "      <td>0.013279</td>\n",
       "      <td>0.025321</td>\n",
       "      <td>0.083302</td>\n",
       "      <td>0.091703</td>\n",
       "      <td>0.140438</td>\n",
       "      <td>0.107027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.117087</td>\n",
       "      <td>0.347818</td>\n",
       "      <td>0.063372</td>\n",
       "      <td>-0.010459</td>\n",
       "      <td>-0.028582</td>\n",
       "      <td>-0.022199</td>\n",
       "      <td>0.175677</td>\n",
       "      <td>0.113774</td>\n",
       "      <td>0.053104</td>\n",
       "      <td>0.102599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001613</td>\n",
       "      <td>0.039664</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.074640</td>\n",
       "      <td>0.041787</td>\n",
       "      <td>0.041356</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>0.034598</td>\n",
       "      <td>-0.009366</td>\n",
       "      <td>0.023329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005524</td>\n",
       "      <td>0.294022</td>\n",
       "      <td>-0.058713</td>\n",
       "      <td>-0.013944</td>\n",
       "      <td>-0.027940</td>\n",
       "      <td>-0.022199</td>\n",
       "      <td>0.147377</td>\n",
       "      <td>0.075138</td>\n",
       "      <td>-0.040119</td>\n",
       "      <td>0.044048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.080566</td>\n",
       "      <td>0.106599</td>\n",
       "      <td>0.099901</td>\n",
       "      <td>-0.008184</td>\n",
       "      <td>0.010779</td>\n",
       "      <td>0.014678</td>\n",
       "      <td>0.104792</td>\n",
       "      <td>0.037863</td>\n",
       "      <td>0.073587</td>\n",
       "      <td>0.053109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.032923</td>\n",
       "      <td>-0.032229</td>\n",
       "      <td>0.076142</td>\n",
       "      <td>-0.016224</td>\n",
       "      <td>0.041367</td>\n",
       "      <td>0.008194</td>\n",
       "      <td>-0.153325</td>\n",
       "      <td>-0.025667</td>\n",
       "      <td>0.049305</td>\n",
       "      <td>0.007634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.029390</td>\n",
       "      <td>0.116197</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>-0.006809</td>\n",
       "      <td>-0.001813</td>\n",
       "      <td>0.010948</td>\n",
       "      <td>0.162430</td>\n",
       "      <td>0.052868</td>\n",
       "      <td>-0.004101</td>\n",
       "      <td>0.030935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.012690</td>\n",
       "      <td>0.080401</td>\n",
       "      <td>0.017990</td>\n",
       "      <td>0.063140</td>\n",
       "      <td>0.017519</td>\n",
       "      <td>0.027811</td>\n",
       "      <td>0.091402</td>\n",
       "      <td>0.039475</td>\n",
       "      <td>0.013939</td>\n",
       "      <td>0.031954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.009989</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>0.020102</td>\n",
       "      <td>0.055526</td>\n",
       "      <td>0.035967</td>\n",
       "      <td>0.017719</td>\n",
       "      <td>0.049619</td>\n",
       "      <td>0.025868</td>\n",
       "      <td>0.010128</td>\n",
       "      <td>0.029546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.018560</td>\n",
       "      <td>-0.002333</td>\n",
       "      <td>-0.022160</td>\n",
       "      <td>-0.017928</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012121</td>\n",
       "      <td>-0.010120</td>\n",
       "      <td>-0.032469</td>\n",
       "      <td>-0.006611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.036692</td>\n",
       "      <td>-0.108179</td>\n",
       "      <td>0.082750</td>\n",
       "      <td>0.022208</td>\n",
       "      <td>0.064322</td>\n",
       "      <td>0.029470</td>\n",
       "      <td>-0.163139</td>\n",
       "      <td>-0.028770</td>\n",
       "      <td>0.061750</td>\n",
       "      <td>-0.002009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.165626</td>\n",
       "      <td>-0.037774</td>\n",
       "      <td>-0.038084</td>\n",
       "      <td>0.038616</td>\n",
       "      <td>-0.043525</td>\n",
       "      <td>-0.076244</td>\n",
       "      <td>0.041970</td>\n",
       "      <td>-0.035827</td>\n",
       "      <td>0.026771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.021127</td>\n",
       "      <td>0.154456</td>\n",
       "      <td>-0.054295</td>\n",
       "      <td>0.075365</td>\n",
       "      <td>0.036609</td>\n",
       "      <td>0.015161</td>\n",
       "      <td>-0.111478</td>\n",
       "      <td>0.050225</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>0.023953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.120954</td>\n",
       "      <td>0.066972</td>\n",
       "      <td>-0.032816</td>\n",
       "      <td>-0.022610</td>\n",
       "      <td>-0.018880</td>\n",
       "      <td>0.104391</td>\n",
       "      <td>0.082575</td>\n",
       "      <td>0.073865</td>\n",
       "      <td>0.050150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           t        mu        RA        XA        XB        QA        Nd  \\\n",
       "0   0.113791  0.006610  0.120206 -0.020125 -0.016782 -0.005156 -0.079315   \n",
       "1   0.117087  0.347818  0.063372 -0.010459 -0.028582 -0.022199  0.175677   \n",
       "2  -0.003665  0.005204  0.001813 -0.055759  0.044438 -0.028347 -0.191259   \n",
       "3   0.020899  0.258272 -0.026930 -0.034422  0.018160 -0.033434 -0.028641   \n",
       "4   0.022605 -0.071532  0.062245  0.084474  0.040626  0.053589 -0.180993   \n",
       "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.117087  0.347818  0.063372 -0.010459 -0.028582 -0.022199  0.175677   \n",
       "7  -0.032882  0.151308 -0.059979  0.078222  0.043598  0.016652 -0.103581   \n",
       "8   0.129761  0.235697  0.097267 -0.038551  0.035967 -0.043525 -0.042123   \n",
       "9   0.063562 -0.078326  0.102609 -0.007967  0.069732 -0.000634 -0.265688   \n",
       "10  0.029390  0.116197  0.003284 -0.006809 -0.001813  0.010948  0.162430   \n",
       "11  0.129761  0.235697  0.097267 -0.038551  0.035967 -0.043525 -0.042123   \n",
       "12  0.029390  0.116197  0.003284 -0.006809 -0.001813  0.010948  0.162430   \n",
       "13 -0.006301  0.029608 -0.009964 -0.003685  0.002007 -0.003979 -0.014490   \n",
       "14  0.107490  0.036219  0.110242 -0.023811 -0.014776 -0.009135 -0.093805   \n",
       "15  0.009484  0.255675 -0.037733 -0.044137  0.015660 -0.046566 -0.047333   \n",
       "16  0.169081  0.256793  0.163938  0.026733  0.013279  0.025321  0.083302   \n",
       "17  0.117087  0.347818  0.063372 -0.010459 -0.028582 -0.022199  0.175677   \n",
       "18  0.001613  0.039664  0.003326  0.074640  0.041787  0.041356  0.010610   \n",
       "19  0.005524  0.294022 -0.058713 -0.013944 -0.027940 -0.022199  0.147377   \n",
       "20  0.080566  0.106599  0.099901 -0.008184  0.010779  0.014678  0.104792   \n",
       "21  0.032923 -0.032229  0.076142 -0.016224  0.041367  0.008194 -0.153325   \n",
       "22  0.029390  0.116197  0.003284 -0.006809 -0.001813  0.010948  0.162430   \n",
       "23  0.012690  0.080401  0.017990  0.063140  0.017519  0.027811  0.091402   \n",
       "24  0.009989  0.004029  0.020102  0.055526  0.035967  0.017719  0.049619   \n",
       "25 -0.018560 -0.002333 -0.022160 -0.017928  0.000838  0.000000  0.012121   \n",
       "26  0.036692 -0.108179  0.082750  0.022208  0.064322  0.029470 -0.163139   \n",
       "27  0.000963  0.165626 -0.037774 -0.038084  0.038616 -0.043525 -0.076244   \n",
       "28 -0.021127  0.154456 -0.054295  0.075365  0.036609  0.015161 -0.111478   \n",
       "29  0.104553  0.120954  0.066972 -0.032816 -0.022610 -0.018880  0.104391   \n",
       "\n",
       "        mu/t      RA*t     mu/RA  \n",
       "0   0.047743  0.103825  0.043477  \n",
       "1   0.113774  0.053104  0.102599  \n",
       "2  -0.015435 -0.019450  0.004882  \n",
       "3   0.063193 -0.014125  0.044385  \n",
       "4   0.004736  0.032192  0.025287  \n",
       "5   0.000000  0.000000  0.000000  \n",
       "6   0.113774  0.053104  0.102599  \n",
       "7   0.045255 -0.048300  0.015323  \n",
       "8   0.088222  0.075286  0.100528  \n",
       "9   0.006058  0.069624  0.026787  \n",
       "10  0.052868 -0.004101  0.030935  \n",
       "11  0.088222  0.075286  0.100528  \n",
       "12  0.052868 -0.004101  0.030935  \n",
       "13  0.003454 -0.010887 -0.002015  \n",
       "14  0.051196  0.092938  0.041463  \n",
       "15  0.058752 -0.034102  0.042350  \n",
       "16  0.091703  0.140438  0.107027  \n",
       "17  0.113774  0.053104  0.102599  \n",
       "18  0.034598 -0.009366  0.023329  \n",
       "19  0.075138 -0.040119  0.044048  \n",
       "20  0.037863  0.073587  0.053109  \n",
       "21 -0.025667  0.049305  0.007634  \n",
       "22  0.052868 -0.004101  0.030935  \n",
       "23  0.039475  0.013939  0.031954  \n",
       "24  0.025868  0.010128  0.029546  \n",
       "25 -0.010120 -0.032469 -0.006611  \n",
       "26 -0.028770  0.061750 -0.002009  \n",
       "27  0.041970 -0.035827  0.026771  \n",
       "28  0.050225 -0.036703  0.023953  \n",
       "29  0.082575  0.073865  0.050150  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imp = pd.DataFrame()\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    df_imp[feature] = r.importances[i]\n",
    "\n",
    "df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e325f09-e97d-461e-aad4-eccfc7777264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
