{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ffc6d3-bb35-440d-92da-0c1d956bf8ca",
   "metadata": {},
   "source": [
    "Train a neural network using tensorflow to correlate basic oxide perovskite properties to OER activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1489ba04-9f04-427e-93c2-18c30283b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f3f23-5b56-472c-a0c7-f048df3965de",
   "metadata": {},
   "source": [
    "Load, normalize and split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2f86a07-ccb3-47d4-b626-9bf91c3ac53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>mu</th>\n",
       "      <th>RA</th>\n",
       "      <th>XA</th>\n",
       "      <th>XB</th>\n",
       "      <th>QA</th>\n",
       "      <th>Nd</th>\n",
       "      <th>VRHE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.993000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.791583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.422000</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.7300</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1.722750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.003000</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.9100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>1.707833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.988000</td>\n",
       "      <td>0.437000</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.7250</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.774417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.004000</td>\n",
       "      <td>0.414000</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.9020</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.80</td>\n",
       "      <td>1.790833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.004000</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.8940</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.60</td>\n",
       "      <td>1.753917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.009000</td>\n",
       "      <td>0.407000</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.8300</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.759083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.407000</td>\n",
       "      <td>1.3650</td>\n",
       "      <td>1.1150</td>\n",
       "      <td>1.8300</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.724667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.012000</td>\n",
       "      <td>0.407000</td>\n",
       "      <td>1.3700</td>\n",
       "      <td>1.1300</td>\n",
       "      <td>1.8300</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.755583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.011000</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>1.3600</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>1.8800</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.720583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.011000</td>\n",
       "      <td>0.398000</td>\n",
       "      <td>1.3500</td>\n",
       "      <td>1.0500</td>\n",
       "      <td>1.8800</td>\n",
       "      <td>2.50</td>\n",
       "      <td>5.50</td>\n",
       "      <td>1.681583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.019000</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>1.3760</td>\n",
       "      <td>1.0700</td>\n",
       "      <td>1.8800</td>\n",
       "      <td>2.80</td>\n",
       "      <td>5.80</td>\n",
       "      <td>1.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.020000</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>1.3800</td>\n",
       "      <td>1.0630</td>\n",
       "      <td>1.8550</td>\n",
       "      <td>2.75</td>\n",
       "      <td>5.25</td>\n",
       "      <td>1.718583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.034000</td>\n",
       "      <td>0.397000</td>\n",
       "      <td>1.4080</td>\n",
       "      <td>1.0100</td>\n",
       "      <td>1.8800</td>\n",
       "      <td>2.40</td>\n",
       "      <td>5.40</td>\n",
       "      <td>1.699333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.042000</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>1.4240</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>1.8800</td>\n",
       "      <td>2.20</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.680250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.049000</td>\n",
       "      <td>0.393000</td>\n",
       "      <td>1.4400</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>1.8800</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.082000</td>\n",
       "      <td>0.391000</td>\n",
       "      <td>1.5250</td>\n",
       "      <td>0.9200</td>\n",
       "      <td>1.8760</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.80</td>\n",
       "      <td>1.638250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.119000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>1.6100</td>\n",
       "      <td>0.8900</td>\n",
       "      <td>1.8300</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.709167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.063938</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>1.4900</td>\n",
       "      <td>1.0225</td>\n",
       "      <td>1.7300</td>\n",
       "      <td>2.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.672500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.093738</td>\n",
       "      <td>0.397407</td>\n",
       "      <td>1.5680</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>1.7975</td>\n",
       "      <td>2.20</td>\n",
       "      <td>4.70</td>\n",
       "      <td>1.586167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.088382</td>\n",
       "      <td>0.379259</td>\n",
       "      <td>1.5160</td>\n",
       "      <td>1.0070</td>\n",
       "      <td>1.9100</td>\n",
       "      <td>2.40</td>\n",
       "      <td>6.40</td>\n",
       "      <td>1.622583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.070734</td>\n",
       "      <td>0.364815</td>\n",
       "      <td>1.4400</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>1.9025</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.75</td>\n",
       "      <td>1.619417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.127314</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>1.5675</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>1.9100</td>\n",
       "      <td>2.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.607417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           t        mu      RA      XA      XB    QA    Nd      VRHE\n",
       "0   0.993000  0.430000  1.3600  1.1000  1.5500  3.00  4.00  1.791583\n",
       "1   0.998000  0.422000  1.3600  1.1000  1.7300  3.00  5.50  1.722750\n",
       "2   1.003000  0.415000  1.3600  1.1000  1.9100  3.00  7.00  1.707833\n",
       "3   0.988000  0.437000  1.3600  1.1000  1.7250  3.00  6.00  1.774417\n",
       "4   1.004000  0.414000  1.3600  1.1000  1.9020  3.00  6.80  1.790833\n",
       "5   1.004000  0.413000  1.3600  1.1000  1.8940  3.00  6.60  1.753917\n",
       "6   1.009000  0.407000  1.3600  1.1000  1.8300  3.00  5.00  1.759083\n",
       "7   1.010000  0.407000  1.3650  1.1150  1.8300  3.00  5.00  1.724667\n",
       "8   1.012000  0.407000  1.3700  1.1300  1.8300  3.00  5.00  1.755583\n",
       "9   1.011000  0.404000  1.3600  1.1000  1.8800  3.00  6.00  1.720583\n",
       "10  1.011000  0.398000  1.3500  1.0500  1.8800  2.50  5.50  1.681583\n",
       "11  1.019000  0.401000  1.3760  1.0700  1.8800  2.80  5.80  1.687500\n",
       "12  1.020000  0.401000  1.3800  1.0630  1.8550  2.75  5.25  1.718583\n",
       "13  1.034000  0.397000  1.4080  1.0100  1.8800  2.40  5.40  1.699333\n",
       "14  1.042000  0.395000  1.4240  0.9800  1.8800  2.20  5.20  1.680250\n",
       "15  1.049000  0.393000  1.4400  0.9500  1.8800  2.00  5.00  1.670000\n",
       "16  1.082000  0.391000  1.5250  0.9200  1.8760  2.00  4.80  1.638250\n",
       "17  1.119000  0.385000  1.6100  0.8900  1.8300  2.00  4.00  1.709167\n",
       "18  1.063938  0.398148  1.4900  1.0225  1.7300  2.50  5.00  1.672500\n",
       "19  1.093738  0.397407  1.5680  0.9760  1.7975  2.20  4.70  1.586167\n",
       "20  1.088382  0.379259  1.5160  1.0070  1.9100  2.40  6.40  1.622583\n",
       "21  1.070734  0.364815  1.4400  0.9500  1.9025  2.00  5.75  1.619417\n",
       "22  1.127314  0.355556  1.5675  0.9050  1.9100  2.00  6.00  1.607417"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_excel(\"data_gp.xlsx\")\n",
    "df = df.drop(['mu/t', 'mu*RA', 'mu*t', 'RA*t', 'mu/RA', 'RA/t'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2fcd88-95ca-47f8-8233-9c449ad7f6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.08223085,  1.62676953, -0.77345319,  0.86166497, -3.46038823,\n",
       "          0.97887708, -1.88632043],\n",
       "        [-0.95938445,  1.18516573, -0.77345319,  0.86166497, -1.30350668,\n",
       "          0.97887708,  0.04477912],\n",
       "        [-0.83653805,  0.79876242, -0.77345319,  0.86166497,  0.85337488,\n",
       "          0.97887708,  1.97587866],\n",
       "        [-1.20507726,  2.01317285, -0.77345319,  0.86166497, -1.36342005,\n",
       "          0.97887708,  0.68847897],\n",
       "        [-0.81196877,  0.74356194, -0.77345319,  0.86166497,  0.75751347,\n",
       "          0.97887708,  1.71839873],\n",
       "        [-0.81196877,  0.68836147, -0.77345319,  0.86166497,  0.66165207,\n",
       "          0.97887708,  1.46091879],\n",
       "        [-0.68912236,  0.35715862, -0.77345319,  0.86166497, -0.10523915,\n",
       "          0.97887708, -0.59892073],\n",
       "        [-0.66455308,  0.35715862, -0.71123065,  1.0650686 , -0.10523915,\n",
       "          0.97887708, -0.59892073],\n",
       "        [-0.61541452,  0.35715862, -0.64900811,  1.26847224, -0.10523915,\n",
       "          0.97887708, -0.59892073],\n",
       "        [-0.6399838 ,  0.1915572 , -0.77345319,  0.86166497,  0.49389462,\n",
       "          0.97887708,  0.68847897],\n",
       "        [-0.6399838 , -0.13964564, -0.89789826,  0.18365285,  0.49389462,\n",
       "         -0.23810524,  0.04477912],\n",
       "        [-0.44342955,  0.02595578, -0.57434107,  0.4548577 ,  0.49389462,\n",
       "          0.49208416,  0.43099903],\n",
       "        [-0.41886027,  0.02595578, -0.52456304,  0.359936  ,  0.19432773,\n",
       "          0.37038592, -0.2770708 ],\n",
       "        [-0.07489034, -0.19484612, -0.17611683, -0.35875685,  0.49389462,\n",
       "         -0.4815017 , -0.08396085],\n",
       "        [ 0.12166391, -0.30524707,  0.02299529, -0.76556412,  0.49389462,\n",
       "         -0.96829463, -0.34144079],\n",
       "        [ 0.29364887, -0.41564801,  0.2221074 , -1.17237139,  0.49389462,\n",
       "         -1.45508756, -0.59892073],\n",
       "        [ 1.10443514, -0.52604896,  1.27989053, -1.57917866,  0.44596392,\n",
       "         -1.45508756, -0.85640067],\n",
       "        [ 2.01349853, -0.85725181,  2.33767366, -1.98598594, -0.10523915,\n",
       "         -1.45508756, -1.88632043]]),\n",
       " array([1.79158333, 1.72275   , 1.70783333, 1.77441667, 1.79083333,\n",
       "        1.75391667, 1.75908333, 1.72466667, 1.75558333, 1.72058333,\n",
       "        1.68158333, 1.6875    , 1.71858333, 1.69933333, 1.68025   ,\n",
       "        1.67      , 1.63825   , 1.70916667]),\n",
       " array([[ 0.66066848, -0.1314678 ,  0.84433277, -0.18925382, -1.30350668,\n",
       "         -0.23810524, -0.59892073],\n",
       "        [ 1.39284044, -0.17235704,  1.81500435, -0.81980509, -0.4946761 ,\n",
       "         -0.96829463, -0.98514064],\n",
       "        [ 1.2612462 , -1.17414342,  1.16788996, -0.39943758,  0.85337488,\n",
       "         -0.4815017 ,  1.20343885],\n",
       "        [ 0.82764462, -1.9714836 ,  0.2221074 , -1.17237139,  0.76350481,\n",
       "         -1.45508756,  0.36662904],\n",
       "        [ 2.21775967, -2.4825991 ,  1.80878209, -1.7825823 ,  0.85337488,\n",
       "         -1.45508756,  0.68847897]]),\n",
       " array([1.6725    , 1.58616667, 1.62258333, 1.61941667, 1.60741667]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select input features and target\n",
    "features = ['t', 'mu', 'RA', 'XA', 'XB', 'QA', 'Nd']\n",
    "target = 'VRHE'\n",
    "X = df[features].values\n",
    "y = df[target].values\n",
    "\n",
    "# Normalize input features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train = X_scaled[:18, :]\n",
    "y_train = y[:18]\n",
    "X_val = X_scaled[18:, :]\n",
    "y_val = y[18:]\n",
    "\n",
    "X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50a6fba-b014-4164-afcd-95bddaad3b31",
   "metadata": {},
   "source": [
    "Build a 4-layer neural network with early stopping and learning rate decay, and train with 500 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ec4e58d-180f-4782-ace4-236dbc44b3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "Epoch 1: Warmup phase (1/150) - val_mae: 1.4173[0m \u001b[1m0s\u001b[0m 2s/step - loss: 10.2921 - mae: 1.6133\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 10.2921 - mae: 1.6133 - val_loss: 9.6172 - val_mae: 1.4173\n",
      "Epoch 2/600\n",
      "Epoch 2: Warmup phase (2/150) - val_mae: 1.3367[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 9.9765 - mae: 1.5323\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 9.9765 - mae: 1.5323 - val_loss: 9.3347 - val_mae: 1.3367\n",
      "Epoch 3/600\n",
      "Epoch 3: Warmup phase (3/150) - val_mae: 1.2541[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.6785 - mae: 1.4523\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 9.6785 - mae: 1.4523 - val_loss: 9.0623 - val_mae: 1.2541\n",
      "Epoch 4/600\n",
      "Epoch 4: Warmup phase (4/150) - val_mae: 1.1752[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.4048 - mae: 1.3760\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 9.4048 - mae: 1.3760 - val_loss: 8.8137 - val_mae: 1.1752\n",
      "Epoch 5/600\n",
      "Epoch 5: Warmup phase (5/150) - val_mae: 1.0985[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.1496 - mae: 1.3015\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 9.1496 - mae: 1.3015 - val_loss: 8.5835 - val_mae: 1.0985\n",
      "Epoch 6/600\n",
      "Epoch 6: Warmup phase (6/150) - val_mae: 1.0223[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.9105 - mae: 1.2282\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 8.9105 - mae: 1.2282 - val_loss: 8.3671 - val_mae: 1.0223\n",
      "Epoch 7/600\n",
      "Epoch 7: Warmup phase (7/150) - val_mae: 0.9470[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 8.6825 - mae: 1.1548\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 8.6825 - mae: 1.1548 - val_loss: 8.1653 - val_mae: 0.9470\n",
      "Epoch 8/600\n",
      "Epoch 8: Warmup phase (8/150) - val_mae: 0.8738[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 8.4700 - mae: 1.0832\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 8.4700 - mae: 1.0832 - val_loss: 7.9776 - val_mae: 0.8738\n",
      "Epoch 9/600\n",
      "Epoch 9: Warmup phase (9/150) - val_mae: 0.8026[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.2682 - mae: 1.0120\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 8.2682 - mae: 1.0120 - val_loss: 7.8036 - val_mae: 0.8026\n",
      "Epoch 10/600\n",
      "Epoch 10: Warmup phase (10/150) - val_mae: 0.7345m \u001b[1m0s\u001b[0m 23ms/step - loss: 8.0734 - mae: 0.9463\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 8.0734 - mae: 0.9463 - val_loss: 7.6440 - val_mae: 0.7345\n",
      "Epoch 11/600\n",
      "Epoch 11: Warmup phase (11/150) - val_mae: 0.6700m \u001b[1m0s\u001b[0m 24ms/step - loss: 7.8877 - mae: 0.8843\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 7.8877 - mae: 0.8843 - val_loss: 7.4976 - val_mae: 0.6700\n",
      "Epoch 12/600\n",
      "Epoch 12: Warmup phase (12/150) - val_mae: 0.6093m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.7118 - mae: 0.8213\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 7.7118 - mae: 0.8213 - val_loss: 7.3635 - val_mae: 0.6093\n",
      "Epoch 13/600\n",
      "Epoch 13: Warmup phase (13/150) - val_mae: 0.5569m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.5431 - mae: 0.7593\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 7.5431 - mae: 0.7593 - val_loss: 7.2407 - val_mae: 0.5569\n",
      "Epoch 14/600\n",
      "Epoch 14: Warmup phase (14/150) - val_mae: 0.5347m \u001b[1m0s\u001b[0m 21ms/step - loss: 7.3854 - mae: 0.7055\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 7.3854 - mae: 0.7055 - val_loss: 7.1284 - val_mae: 0.5347\n",
      "Epoch 15/600\n",
      "Epoch 15: Warmup phase (15/150) - val_mae: 0.5142m \u001b[1m0s\u001b[0m 23ms/step - loss: 7.2389 - mae: 0.6500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 7.2389 - mae: 0.6500 - val_loss: 7.0250 - val_mae: 0.5142\n",
      "Epoch 16/600\n",
      "Epoch 16: Warmup phase (16/150) - val_mae: 0.4963m \u001b[1m0s\u001b[0m 22ms/step - loss: 7.1047 - mae: 0.5937\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 7.1047 - mae: 0.5937 - val_loss: 6.9292 - val_mae: 0.4963\n",
      "Epoch 17/600\n",
      "Epoch 17: Warmup phase (17/150) - val_mae: 0.4804m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.9822 - mae: 0.5374\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.9822 - mae: 0.5374 - val_loss: 6.8395 - val_mae: 0.4804\n",
      "Epoch 18/600\n",
      "Epoch 18: Warmup phase (18/150) - val_mae: 0.4671m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.8715 - mae: 0.4937\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.8715 - mae: 0.4937 - val_loss: 6.7550 - val_mae: 0.4671\n",
      "Epoch 19/600\n",
      "Epoch 19: Warmup phase (19/150) - val_mae: 0.4562m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.7716 - mae: 0.4644\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 6.7716 - mae: 0.4644 - val_loss: 6.6745 - val_mae: 0.4562\n",
      "Epoch 20/600\n",
      "Epoch 20: Warmup phase (20/150) - val_mae: 0.4479m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.6803 - mae: 0.4435\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.6803 - mae: 0.4435 - val_loss: 6.5972 - val_mae: 0.4479\n",
      "Epoch 21/600\n",
      "Epoch 21: Warmup phase (21/150) - val_mae: 0.4421m \u001b[1m0s\u001b[0m 24ms/step - loss: 6.5958 - mae: 0.4425\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 6.5958 - mae: 0.4425 - val_loss: 6.5225 - val_mae: 0.4421\n",
      "Epoch 22/600\n",
      "Epoch 22: Warmup phase (22/150) - val_mae: 0.4388m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.5163 - mae: 0.4411\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 6.5163 - mae: 0.4411 - val_loss: 6.4499 - val_mae: 0.4388\n",
      "Epoch 23/600\n",
      "Epoch 23: Warmup phase (23/150) - val_mae: 0.4374m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.4391 - mae: 0.4372\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 6.4391 - mae: 0.4372 - val_loss: 6.3789 - val_mae: 0.4374\n",
      "Epoch 24/600\n",
      "Epoch 24: Warmup phase (24/150) - val_mae: 0.4384m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.3628 - mae: 0.4326\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 6.3628 - mae: 0.4326 - val_loss: 6.3096 - val_mae: 0.4384\n",
      "Epoch 25/600\n",
      "Epoch 25: Warmup phase (25/150) - val_mae: 0.4409m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.2851 - mae: 0.4294\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 6.2851 - mae: 0.4294 - val_loss: 6.2419 - val_mae: 0.4409\n",
      "Epoch 26/600\n",
      "Epoch 26: Warmup phase (26/150) - val_mae: 0.4450m \u001b[1m0s\u001b[0m 23ms/step - loss: 6.2050 - mae: 0.4233\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 6.2050 - mae: 0.4233 - val_loss: 6.1758 - val_mae: 0.4450\n",
      "Epoch 27/600\n",
      "Epoch 27: Warmup phase (27/150) - val_mae: 0.4505m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.1223 - mae: 0.4145\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.1223 - mae: 0.4145 - val_loss: 6.1115 - val_mae: 0.4505\n",
      "Epoch 28/600\n",
      "Epoch 28: Warmup phase (28/150) - val_mae: 0.4567m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.0373 - mae: 0.4034\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.0373 - mae: 0.4034 - val_loss: 6.0488 - val_mae: 0.4567\n",
      "Epoch 29/600\n",
      "Epoch 29: Warmup phase (29/150) - val_mae: 0.4636m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.9509 - mae: 0.3904\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 5.9509 - mae: 0.3904 - val_loss: 5.9877 - val_mae: 0.4636\n",
      "Epoch 30/600\n",
      "Epoch 30: Warmup phase (30/150) - val_mae: 0.4706m \u001b[1m0s\u001b[0m 24ms/step - loss: 5.8639 - mae: 0.3758\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 5.8639 - mae: 0.3758 - val_loss: 5.9279 - val_mae: 0.4706\n",
      "Epoch 31/600\n",
      "Epoch 31: Warmup phase (31/150) - val_mae: 0.4778m \u001b[1m0s\u001b[0m 23ms/step - loss: 5.7777 - mae: 0.3605\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 5.7777 - mae: 0.3605 - val_loss: 5.8695 - val_mae: 0.4778\n",
      "Epoch 32/600\n",
      "Epoch 32: Warmup phase (32/150) - val_mae: 0.4849m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.6928 - mae: 0.3444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 5.6928 - mae: 0.3444 - val_loss: 5.8119 - val_mae: 0.4849\n",
      "Epoch 33/600\n",
      "Epoch 33: Warmup phase (33/150) - val_mae: 0.4916m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.6101 - mae: 0.3282\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 5.6101 - mae: 0.3282 - val_loss: 5.7552 - val_mae: 0.4916\n",
      "Epoch 34/600\n",
      "Epoch 34: Warmup phase (34/150) - val_mae: 0.4978m \u001b[1m0s\u001b[0m 23ms/step - loss: 5.5301 - mae: 0.3135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 5.5301 - mae: 0.3135 - val_loss: 5.6988 - val_mae: 0.4978\n",
      "Epoch 35/600\n",
      "Epoch 35: Warmup phase (35/150) - val_mae: 0.5034m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.4526 - mae: 0.3023\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 5.4526 - mae: 0.3023 - val_loss: 5.6426 - val_mae: 0.5034\n",
      "Epoch 36/600\n",
      "Epoch 36: Warmup phase (36/150) - val_mae: 0.5081m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.3778 - mae: 0.2929\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 5.3778 - mae: 0.2929 - val_loss: 5.5861 - val_mae: 0.5081\n",
      "Epoch 37/600\n",
      "Epoch 37: Warmup phase (37/150) - val_mae: 0.5122m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.3052 - mae: 0.2838\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 5.3052 - mae: 0.2838 - val_loss: 5.5295 - val_mae: 0.5122\n",
      "Epoch 38/600\n",
      "Epoch 38: Warmup phase (38/150) - val_mae: 0.5154m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.2345 - mae: 0.2761\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 5.2345 - mae: 0.2761 - val_loss: 5.4725 - val_mae: 0.5154\n",
      "Epoch 39/600\n",
      "Epoch 39: Warmup phase (39/150) - val_mae: 0.5180m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.1655 - mae: 0.2697\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 5.1655 - mae: 0.2697 - val_loss: 5.4151 - val_mae: 0.5180\n",
      "Epoch 40/600\n",
      "Epoch 40: Warmup phase (40/150) - val_mae: 0.5195m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.0976 - mae: 0.2658\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 5.0976 - mae: 0.2658 - val_loss: 5.3569 - val_mae: 0.5195\n",
      "Epoch 41/600\n",
      "Epoch 41: Warmup phase (41/150) - val_mae: 0.5205m \u001b[1m0s\u001b[0m 23ms/step - loss: 5.0306 - mae: 0.2610\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 5.0306 - mae: 0.2610 - val_loss: 5.2985 - val_mae: 0.5205\n",
      "Epoch 42/600\n",
      "Epoch 42: Warmup phase (42/150) - val_mae: 0.5207m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.9644 - mae: 0.2560\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 4.9644 - mae: 0.2560 - val_loss: 5.2392 - val_mae: 0.5207\n",
      "Epoch 43/600\n",
      "Epoch 43: Warmup phase (43/150) - val_mae: 0.5202m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.8987 - mae: 0.2518\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 4.8987 - mae: 0.2518 - val_loss: 5.1796 - val_mae: 0.5202\n",
      "Epoch 44/600\n",
      "Epoch 44: Warmup phase (44/150) - val_mae: 0.5188m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.8334 - mae: 0.2463\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 4.8334 - mae: 0.2463 - val_loss: 5.1196 - val_mae: 0.5188\n",
      "Epoch 45/600\n",
      "Epoch 45: Warmup phase (45/150) - val_mae: 0.5222m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.7685 - mae: 0.2398\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 4.7685 - mae: 0.2398 - val_loss: 5.0596 - val_mae: 0.5222\n",
      "Epoch 46/600\n",
      "Epoch 46: Warmup phase (46/150) - val_mae: 0.5252m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.7041 - mae: 0.2322\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 4.7041 - mae: 0.2322 - val_loss: 4.9996 - val_mae: 0.5252\n",
      "Epoch 47/600\n",
      "Epoch 47: Warmup phase (47/150) - val_mae: 0.5282m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.6404 - mae: 0.2239\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 4.6404 - mae: 0.2239 - val_loss: 4.9399 - val_mae: 0.5282\n",
      "Epoch 48/600\n",
      "Epoch 48: Warmup phase (48/150) - val_mae: 0.5306m \u001b[1m0s\u001b[0m 24ms/step - loss: 4.5773 - mae: 0.2147\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 4.5773 - mae: 0.2147 - val_loss: 4.8803 - val_mae: 0.5306\n",
      "Epoch 49/600\n",
      "Epoch 49: Warmup phase (49/150) - val_mae: 0.5325m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.5150 - mae: 0.2055\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 4.5150 - mae: 0.2055 - val_loss: 4.8210 - val_mae: 0.5325\n",
      "Epoch 50/600\n",
      "Epoch 50: Warmup phase (50/150) - val_mae: 0.5344m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.4536 - mae: 0.1965\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 4.4536 - mae: 0.1965 - val_loss: 4.7621 - val_mae: 0.5344\n",
      "Epoch 51/600\n",
      "Epoch 51: Warmup phase (51/150) - val_mae: 0.5358m \u001b[1m0s\u001b[0m 40ms/step - loss: 4.3932 - mae: 0.1870\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 4.3932 - mae: 0.1870 - val_loss: 4.7038 - val_mae: 0.5358\n",
      "Epoch 52/600\n",
      "Epoch 52: Warmup phase (52/150) - val_mae: 0.5368m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.3338 - mae: 0.1773\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 4.3338 - mae: 0.1773 - val_loss: 4.6462 - val_mae: 0.5368\n",
      "Epoch 53/600\n",
      "Epoch 53: Warmup phase (53/150) - val_mae: 0.5379m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.2754 - mae: 0.1679\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 4.2754 - mae: 0.1679 - val_loss: 4.5894 - val_mae: 0.5379\n",
      "Epoch 54/600\n",
      "Epoch 54: Warmup phase (54/150) - val_mae: 0.5386m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.2181 - mae: 0.1597\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 4.2181 - mae: 0.1597 - val_loss: 4.5333 - val_mae: 0.5386\n",
      "Epoch 55/600\n",
      "Epoch 55: Warmup phase (55/150) - val_mae: 0.5393m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.1617 - mae: 0.1551\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 4.1617 - mae: 0.1551 - val_loss: 4.4783 - val_mae: 0.5393\n",
      "Epoch 56/600\n",
      "Epoch 56: Warmup phase (56/150) - val_mae: 0.5396m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.1063 - mae: 0.1525\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 4.1063 - mae: 0.1525 - val_loss: 4.4239 - val_mae: 0.5396\n",
      "Epoch 57/600\n",
      "Epoch 57: Warmup phase (57/150) - val_mae: 0.5395m \u001b[1m0s\u001b[0m 23ms/step - loss: 4.0517 - mae: 0.1522\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 4.0517 - mae: 0.1522 - val_loss: 4.3702 - val_mae: 0.5395\n",
      "Epoch 58/600\n",
      "Epoch 58: Warmup phase (58/150) - val_mae: 0.5393m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.9980 - mae: 0.1524\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 3.9980 - mae: 0.1524 - val_loss: 4.3177 - val_mae: 0.5393\n",
      "Epoch 59/600\n",
      "Epoch 59: Warmup phase (59/150) - val_mae: 0.5388m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.9449 - mae: 0.1518\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 3.9449 - mae: 0.1518 - val_loss: 4.2659 - val_mae: 0.5388\n",
      "Epoch 60/600\n",
      "Epoch 60: Warmup phase (60/150) - val_mae: 0.5384m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.8923 - mae: 0.1505\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 3.8923 - mae: 0.1505 - val_loss: 4.2152 - val_mae: 0.5384\n",
      "Epoch 61/600\n",
      "Epoch 61: Warmup phase (61/150) - val_mae: 0.5376m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.8403 - mae: 0.1482\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 3.8403 - mae: 0.1482 - val_loss: 4.1651 - val_mae: 0.5376\n",
      "Epoch 62/600\n",
      "Epoch 62: Warmup phase (62/150) - val_mae: 0.5366m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.7888 - mae: 0.1448\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 3.7888 - mae: 0.1448 - val_loss: 4.1156 - val_mae: 0.5366\n",
      "Epoch 63/600\n",
      "Epoch 63: Warmup phase (63/150) - val_mae: 0.5353m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.7380 - mae: 0.1411\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 3.7380 - mae: 0.1411 - val_loss: 4.0667 - val_mae: 0.5353\n",
      "Epoch 64/600\n",
      "Epoch 64: Warmup phase (64/150) - val_mae: 0.5339m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.6877 - mae: 0.1368\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 3.6877 - mae: 0.1368 - val_loss: 4.0185 - val_mae: 0.5339\n",
      "Epoch 65/600\n",
      "Epoch 65: Warmup phase (65/150) - val_mae: 0.5325m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.6381 - mae: 0.1319\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 3.6381 - mae: 0.1319 - val_loss: 3.9708 - val_mae: 0.5325\n",
      "Epoch 66/600\n",
      "Epoch 66: Warmup phase (66/150) - val_mae: 0.5310m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.5891 - mae: 0.1268\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 3.5891 - mae: 0.1268 - val_loss: 3.9238 - val_mae: 0.5310\n",
      "Epoch 67/600\n",
      "Epoch 67: Warmup phase (67/150) - val_mae: 0.5293m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.5408 - mae: 0.1216\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 3.5408 - mae: 0.1216 - val_loss: 3.8771 - val_mae: 0.5293\n",
      "Epoch 68/600\n",
      "Epoch 68: Warmup phase (68/150) - val_mae: 0.5275m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.4931 - mae: 0.1165\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 3.4931 - mae: 0.1165 - val_loss: 3.8308 - val_mae: 0.5275\n",
      "Epoch 69/600\n",
      "Epoch 69: Warmup phase (69/150) - val_mae: 0.5256m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.4462 - mae: 0.1136\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 3.4462 - mae: 0.1136 - val_loss: 3.7845 - val_mae: 0.5256\n",
      "Epoch 70/600\n",
      "Epoch 70: Warmup phase (70/150) - val_mae: 0.5235m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.3999 - mae: 0.1116\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 3.3999 - mae: 0.1116 - val_loss: 3.7381 - val_mae: 0.5235\n",
      "Epoch 71/600\n",
      "Epoch 71: Warmup phase (71/150) - val_mae: 0.5214m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.3543 - mae: 0.1100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 3.3543 - mae: 0.1100 - val_loss: 3.6920 - val_mae: 0.5214\n",
      "Epoch 72/600\n",
      "Epoch 72: Warmup phase (72/150) - val_mae: 0.5190m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.3092 - mae: 0.1095\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 3.3092 - mae: 0.1095 - val_loss: 3.6459 - val_mae: 0.5190\n",
      "Epoch 73/600\n",
      "Epoch 73: Warmup phase (73/150) - val_mae: 0.5167m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.2647 - mae: 0.1090\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 3.2647 - mae: 0.1090 - val_loss: 3.6001 - val_mae: 0.5167\n",
      "Epoch 74/600\n",
      "Epoch 74: Warmup phase (74/150) - val_mae: 0.5144m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.2208 - mae: 0.1082\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 3.2208 - mae: 0.1082 - val_loss: 3.5545 - val_mae: 0.5144\n",
      "Epoch 75/600\n",
      "Epoch 75: Warmup phase (75/150) - val_mae: 0.5120m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.1775 - mae: 0.1072\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 3.1775 - mae: 0.1072 - val_loss: 3.5090 - val_mae: 0.5120\n",
      "Epoch 76/600\n",
      "Epoch 76: Warmup phase (76/150) - val_mae: 0.5092m \u001b[1m0s\u001b[0m 26ms/step - loss: 3.1347 - mae: 0.1058\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 3.1347 - mae: 0.1058 - val_loss: 3.4637 - val_mae: 0.5092\n",
      "Epoch 77/600\n",
      "Epoch 77: Warmup phase (77/150) - val_mae: 0.5065m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.0924 - mae: 0.1043\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 3.0924 - mae: 0.1043 - val_loss: 3.4188 - val_mae: 0.5065\n",
      "Epoch 78/600\n",
      "Epoch 78: Warmup phase (78/150) - val_mae: 0.5037m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.0507 - mae: 0.1028\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 3.0507 - mae: 0.1028 - val_loss: 3.3741 - val_mae: 0.5037\n",
      "Epoch 79/600\n",
      "Epoch 79: Warmup phase (79/150) - val_mae: 0.5010m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.0096 - mae: 0.1016\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 3.0096 - mae: 0.1016 - val_loss: 3.3299 - val_mae: 0.5010\n",
      "Epoch 80/600\n",
      "Epoch 80: Warmup phase (80/150) - val_mae: 0.4987m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.9691 - mae: 0.1005\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 2.9691 - mae: 0.1005 - val_loss: 3.2863 - val_mae: 0.4987\n",
      "Epoch 81/600\n",
      "Epoch 81: Warmup phase (81/150) - val_mae: 0.4963m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.9291 - mae: 0.0997\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.9291 - mae: 0.0997 - val_loss: 3.2432 - val_mae: 0.4963\n",
      "Epoch 82/600\n",
      "Epoch 82: Warmup phase (82/150) - val_mae: 0.4937m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.8897 - mae: 0.0992\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.8897 - mae: 0.0992 - val_loss: 3.2006 - val_mae: 0.4937\n",
      "Epoch 83/600\n",
      "Epoch 83: Warmup phase (83/150) - val_mae: 0.4912m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.8508 - mae: 0.0988\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.8508 - mae: 0.0988 - val_loss: 3.1587 - val_mae: 0.4912\n",
      "Epoch 84/600\n",
      "Epoch 84: Warmup phase (84/150) - val_mae: 0.4886m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.8125 - mae: 0.0981\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 2.8125 - mae: 0.0981 - val_loss: 3.1174 - val_mae: 0.4886\n",
      "Epoch 85/600\n",
      "Epoch 85: Warmup phase (85/150) - val_mae: 0.4859m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.7746 - mae: 0.0974\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 2.7746 - mae: 0.0974 - val_loss: 3.0768 - val_mae: 0.4859\n",
      "Epoch 86/600\n",
      "Epoch 86: Warmup phase (86/150) - val_mae: 0.4831m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.7373 - mae: 0.0964\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 2.7373 - mae: 0.0964 - val_loss: 3.0369 - val_mae: 0.4831\n",
      "Epoch 87/600\n",
      "Epoch 87: Warmup phase (87/150) - val_mae: 0.4804m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.7004 - mae: 0.0953\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.7004 - mae: 0.0953 - val_loss: 2.9977 - val_mae: 0.4804\n",
      "Epoch 88/600\n",
      "Epoch 88: Warmup phase (88/150) - val_mae: 0.4777m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.6640 - mae: 0.0941\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 2.6640 - mae: 0.0941 - val_loss: 2.9591 - val_mae: 0.4777\n",
      "Epoch 89/600\n",
      "Epoch 89: Warmup phase (89/150) - val_mae: 0.4750m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.6281 - mae: 0.0929\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.6281 - mae: 0.0929 - val_loss: 2.9211 - val_mae: 0.4750\n",
      "Epoch 90/600\n",
      "Epoch 90: Warmup phase (90/150) - val_mae: 0.4722m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.5927 - mae: 0.0917\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 2.5927 - mae: 0.0917 - val_loss: 2.8837 - val_mae: 0.4722\n",
      "Epoch 91/600\n",
      "Epoch 91: Warmup phase (91/150) - val_mae: 0.4694m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.5578 - mae: 0.0905\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 2.5578 - mae: 0.0905 - val_loss: 2.8469 - val_mae: 0.4694\n",
      "Epoch 92/600\n",
      "Epoch 92: Warmup phase (92/150) - val_mae: 0.4664m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.5234 - mae: 0.0896\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 2.5234 - mae: 0.0896 - val_loss: 2.8103 - val_mae: 0.4664\n",
      "Epoch 93/600\n",
      "Epoch 93: Warmup phase (93/150) - val_mae: 0.4635m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.4894 - mae: 0.0888\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 2.4894 - mae: 0.0888 - val_loss: 2.7742 - val_mae: 0.4635\n",
      "Epoch 94/600\n",
      "Epoch 94: Warmup phase (94/150) - val_mae: 0.4605m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.4559 - mae: 0.0879\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.4559 - mae: 0.0879 - val_loss: 2.7385 - val_mae: 0.4605\n",
      "Epoch 95/600\n",
      "Epoch 95: Warmup phase (95/150) - val_mae: 0.4577m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.4229 - mae: 0.0871\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 2.4229 - mae: 0.0871 - val_loss: 2.7032 - val_mae: 0.4577\n",
      "Epoch 96/600\n",
      "Epoch 96: Warmup phase (96/150) - val_mae: 0.4548m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.3903 - mae: 0.0862\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.3903 - mae: 0.0862 - val_loss: 2.6682 - val_mae: 0.4548\n",
      "Epoch 97/600\n",
      "Epoch 97: Warmup phase (97/150) - val_mae: 0.4519m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.3582 - mae: 0.0854\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 2.3582 - mae: 0.0854 - val_loss: 2.6335 - val_mae: 0.4519\n",
      "Epoch 98/600\n",
      "Epoch 98: Warmup phase (98/150) - val_mae: 0.4489m \u001b[1m0s\u001b[0m 24ms/step - loss: 2.3265 - mae: 0.0846\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 2.3265 - mae: 0.0846 - val_loss: 2.5991 - val_mae: 0.4489\n",
      "Epoch 99/600\n",
      "Epoch 99: Warmup phase (99/150) - val_mae: 0.4460m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.2953 - mae: 0.0838\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.2953 - mae: 0.0838 - val_loss: 2.5648 - val_mae: 0.4460\n",
      "Epoch 100/600\n",
      "Epoch 100: Warmup phase (100/150) - val_mae: 0.4431\u001b[1m0s\u001b[0m 22ms/step - loss: 2.2644 - mae: 0.0831\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 2.2644 - mae: 0.0831 - val_loss: 2.5309 - val_mae: 0.4431\n",
      "Epoch 101/600\n",
      "Epoch 101: Warmup phase (101/150) - val_mae: 0.4402\u001b[1m0s\u001b[0m 22ms/step - loss: 2.2340 - mae: 0.0825\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 2.2340 - mae: 0.0825 - val_loss: 2.4974 - val_mae: 0.4402\n",
      "Epoch 102/600\n",
      "Epoch 102: Warmup phase (102/150) - val_mae: 0.4372\u001b[1m0s\u001b[0m 22ms/step - loss: 2.2041 - mae: 0.0820\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 2.2041 - mae: 0.0820 - val_loss: 2.4643 - val_mae: 0.4372\n",
      "Epoch 103/600\n",
      "Epoch 103: Warmup phase (103/150) - val_mae: 0.4345\u001b[1m0s\u001b[0m 23ms/step - loss: 2.1745 - mae: 0.0814\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.1745 - mae: 0.0814 - val_loss: 2.4316 - val_mae: 0.4345\n",
      "Epoch 104/600\n",
      "Epoch 104: Warmup phase (104/150) - val_mae: 0.4316\u001b[1m0s\u001b[0m 22ms/step - loss: 2.1453 - mae: 0.0808\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 2.1453 - mae: 0.0808 - val_loss: 2.3992 - val_mae: 0.4316\n",
      "Epoch 105/600\n",
      "Epoch 105: Warmup phase (105/150) - val_mae: 0.4287\u001b[1m0s\u001b[0m 22ms/step - loss: 2.1166 - mae: 0.0802\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 2.1166 - mae: 0.0802 - val_loss: 2.3672 - val_mae: 0.4287\n",
      "Epoch 106/600\n",
      "Epoch 106: Warmup phase (106/150) - val_mae: 0.4258\u001b[1m0s\u001b[0m 22ms/step - loss: 2.0882 - mae: 0.0797\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 2.0882 - mae: 0.0797 - val_loss: 2.3356 - val_mae: 0.4258\n",
      "Epoch 107/600\n",
      "Epoch 107: Warmup phase (107/150) - val_mae: 0.4230\u001b[1m0s\u001b[0m 22ms/step - loss: 2.0603 - mae: 0.0791\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.0603 - mae: 0.0791 - val_loss: 2.3045 - val_mae: 0.4230\n",
      "Epoch 108/600\n",
      "Epoch 108: Warmup phase (108/150) - val_mae: 0.4203\u001b[1m0s\u001b[0m 22ms/step - loss: 2.0327 - mae: 0.0786\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 2.0327 - mae: 0.0786 - val_loss: 2.2739 - val_mae: 0.4203\n",
      "Epoch 109/600\n",
      "Epoch 109: Warmup phase (109/150) - val_mae: 0.4177\u001b[1m0s\u001b[0m 23ms/step - loss: 2.0055 - mae: 0.0780\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 2.0055 - mae: 0.0780 - val_loss: 2.2438 - val_mae: 0.4177\n",
      "Epoch 110/600\n",
      "Epoch 110: Warmup phase (110/150) - val_mae: 0.4151\u001b[1m0s\u001b[0m 24ms/step - loss: 1.9787 - mae: 0.0774\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 1.9787 - mae: 0.0774 - val_loss: 2.2140 - val_mae: 0.4151\n",
      "Epoch 111/600\n",
      "Epoch 111: Warmup phase (111/150) - val_mae: 0.4124\u001b[1m0s\u001b[0m 23ms/step - loss: 1.9523 - mae: 0.0769\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 1.9523 - mae: 0.0769 - val_loss: 2.1847 - val_mae: 0.4124\n",
      "Epoch 112/600\n",
      "Epoch 112: Warmup phase (112/150) - val_mae: 0.4098\u001b[1m0s\u001b[0m 25ms/step - loss: 1.9262 - mae: 0.0764\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.9262 - mae: 0.0764 - val_loss: 2.1560 - val_mae: 0.4098\n",
      "Epoch 113/600\n",
      "Epoch 113: Warmup phase (113/150) - val_mae: 0.4073\u001b[1m0s\u001b[0m 22ms/step - loss: 1.9005 - mae: 0.0759\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.9005 - mae: 0.0759 - val_loss: 2.1276 - val_mae: 0.4073\n",
      "Epoch 114/600\n",
      "Epoch 114: Warmup phase (114/150) - val_mae: 0.4047\u001b[1m0s\u001b[0m 22ms/step - loss: 1.8751 - mae: 0.0753\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.8751 - mae: 0.0753 - val_loss: 2.0996 - val_mae: 0.4047\n",
      "Epoch 115/600\n",
      "Epoch 115: Warmup phase (115/150) - val_mae: 0.4021\u001b[1m0s\u001b[0m 22ms/step - loss: 1.8501 - mae: 0.0748\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.8501 - mae: 0.0748 - val_loss: 2.0721 - val_mae: 0.4021\n",
      "Epoch 116/600\n",
      "Epoch 116: Warmup phase (116/150) - val_mae: 0.3997\u001b[1m0s\u001b[0m 22ms/step - loss: 1.8254 - mae: 0.0743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.8254 - mae: 0.0743 - val_loss: 2.0450 - val_mae: 0.3997\n",
      "Epoch 117/600\n",
      "Epoch 117: Warmup phase (117/150) - val_mae: 0.3972\u001b[1m0s\u001b[0m 22ms/step - loss: 1.8011 - mae: 0.0738\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.8011 - mae: 0.0738 - val_loss: 2.0182 - val_mae: 0.3972\n",
      "Epoch 118/600\n",
      "Epoch 118: Warmup phase (118/150) - val_mae: 0.3947\u001b[1m0s\u001b[0m 24ms/step - loss: 1.7771 - mae: 0.0732\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.7771 - mae: 0.0732 - val_loss: 1.9918 - val_mae: 0.3947\n",
      "Epoch 119/600\n",
      "Epoch 119: Warmup phase (119/150) - val_mae: 0.3922\u001b[1m0s\u001b[0m 22ms/step - loss: 1.7534 - mae: 0.0727\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.7534 - mae: 0.0727 - val_loss: 1.9657 - val_mae: 0.3922\n",
      "Epoch 120/600\n",
      "Epoch 120: Warmup phase (120/150) - val_mae: 0.3899\u001b[1m0s\u001b[0m 22ms/step - loss: 1.7301 - mae: 0.0722\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.7301 - mae: 0.0722 - val_loss: 1.9400 - val_mae: 0.3899\n",
      "Epoch 121/600\n",
      "Epoch 121: Warmup phase (121/150) - val_mae: 0.3874\u001b[1m0s\u001b[0m 22ms/step - loss: 1.7071 - mae: 0.0716\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 1.7071 - mae: 0.0716 - val_loss: 1.9147 - val_mae: 0.3874\n",
      "Epoch 122/600\n",
      "Epoch 122: Warmup phase (122/150) - val_mae: 0.3850\u001b[1m0s\u001b[0m 22ms/step - loss: 1.6844 - mae: 0.0710\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.6844 - mae: 0.0710 - val_loss: 1.8896 - val_mae: 0.3850\n",
      "Epoch 123/600\n",
      "Epoch 123: Warmup phase (123/150) - val_mae: 0.3826\u001b[1m0s\u001b[0m 23ms/step - loss: 1.6620 - mae: 0.0705\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.6620 - mae: 0.0705 - val_loss: 1.8650 - val_mae: 0.3826\n",
      "Epoch 124/600\n",
      "Epoch 124: Warmup phase (124/150) - val_mae: 0.3803\u001b[1m0s\u001b[0m 22ms/step - loss: 1.6399 - mae: 0.0699\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.6399 - mae: 0.0699 - val_loss: 1.8407 - val_mae: 0.3803\n",
      "Epoch 125/600\n",
      "Epoch 125: Warmup phase (125/150) - val_mae: 0.3778\u001b[1m0s\u001b[0m 23ms/step - loss: 1.6181 - mae: 0.0693\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.6181 - mae: 0.0693 - val_loss: 1.8166 - val_mae: 0.3778\n",
      "Epoch 126/600\n",
      "Epoch 126: Warmup phase (126/150) - val_mae: 0.3754\u001b[1m0s\u001b[0m 22ms/step - loss: 1.5967 - mae: 0.0688\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.5967 - mae: 0.0688 - val_loss: 1.7930 - val_mae: 0.3754\n",
      "Epoch 127/600\n",
      "Epoch 127: Warmup phase (127/150) - val_mae: 0.3730\u001b[1m0s\u001b[0m 24ms/step - loss: 1.5755 - mae: 0.0684\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.5755 - mae: 0.0684 - val_loss: 1.7695 - val_mae: 0.3730\n",
      "Epoch 128/600\n",
      "Epoch 128: Warmup phase (128/150) - val_mae: 0.3706\u001b[1m0s\u001b[0m 22ms/step - loss: 1.5546 - mae: 0.0680\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.5546 - mae: 0.0680 - val_loss: 1.7465 - val_mae: 0.3706\n",
      "Epoch 129/600\n",
      "Epoch 129: Warmup phase (129/150) - val_mae: 0.3682\u001b[1m0s\u001b[0m 23ms/step - loss: 1.5340 - mae: 0.0676\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 1.5340 - mae: 0.0676 - val_loss: 1.7237 - val_mae: 0.3682\n",
      "Epoch 130/600\n",
      "Epoch 130: Warmup phase (130/150) - val_mae: 0.3657\u001b[1m0s\u001b[0m 22ms/step - loss: 1.5137 - mae: 0.0671\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.5137 - mae: 0.0671 - val_loss: 1.7012 - val_mae: 0.3657\n",
      "Epoch 131/600\n",
      "Epoch 131: Warmup phase (131/150) - val_mae: 0.3633\u001b[1m0s\u001b[0m 22ms/step - loss: 1.4937 - mae: 0.0666\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 1.4937 - mae: 0.0666 - val_loss: 1.6790 - val_mae: 0.3633\n",
      "Epoch 132/600\n",
      "Epoch 132: Warmup phase (132/150) - val_mae: 0.3609\u001b[1m0s\u001b[0m 24ms/step - loss: 1.4740 - mae: 0.0662\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.4740 - mae: 0.0662 - val_loss: 1.6572 - val_mae: 0.3609\n",
      "Epoch 133/600\n",
      "Epoch 133: Warmup phase (133/150) - val_mae: 0.3584\u001b[1m0s\u001b[0m 23ms/step - loss: 1.4545 - mae: 0.0658\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.4545 - mae: 0.0658 - val_loss: 1.6356 - val_mae: 0.3584\n",
      "Epoch 134/600\n",
      "Epoch 134: Warmup phase (134/150) - val_mae: 0.3560\u001b[1m0s\u001b[0m 22ms/step - loss: 1.4353 - mae: 0.0654\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.4353 - mae: 0.0654 - val_loss: 1.6142 - val_mae: 0.3560\n",
      "Epoch 135/600\n",
      "Epoch 135: Warmup phase (135/150) - val_mae: 0.3536\u001b[1m0s\u001b[0m 22ms/step - loss: 1.4164 - mae: 0.0649\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.4164 - mae: 0.0649 - val_loss: 1.5931 - val_mae: 0.3536\n",
      "Epoch 136/600\n",
      "Epoch 136: Warmup phase (136/150) - val_mae: 0.3510\u001b[1m0s\u001b[0m 22ms/step - loss: 1.3977 - mae: 0.0645\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.3977 - mae: 0.0645 - val_loss: 1.5722 - val_mae: 0.3510\n",
      "Epoch 137/600\n",
      "Epoch 137: Warmup phase (137/150) - val_mae: 0.3487\u001b[1m0s\u001b[0m 23ms/step - loss: 1.3793 - mae: 0.0641\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.3793 - mae: 0.0641 - val_loss: 1.5516 - val_mae: 0.3487\n",
      "Epoch 138/600\n",
      "Epoch 138: Warmup phase (138/150) - val_mae: 0.3463\u001b[1m0s\u001b[0m 23ms/step - loss: 1.3611 - mae: 0.0637\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.3611 - mae: 0.0637 - val_loss: 1.5312 - val_mae: 0.3463\n",
      "Epoch 139/600\n",
      "Epoch 139: Warmup phase (139/150) - val_mae: 0.3438\u001b[1m0s\u001b[0m 23ms/step - loss: 1.3432 - mae: 0.0634\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.3432 - mae: 0.0634 - val_loss: 1.5111 - val_mae: 0.3438\n",
      "Epoch 140/600\n",
      "Epoch 140: Warmup phase (140/150) - val_mae: 0.3415\u001b[1m0s\u001b[0m 23ms/step - loss: 1.3256 - mae: 0.0630\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.3256 - mae: 0.0630 - val_loss: 1.4913 - val_mae: 0.3415\n",
      "Epoch 141/600\n",
      "Epoch 141: Warmup phase (141/150) - val_mae: 0.3390\u001b[1m0s\u001b[0m 23ms/step - loss: 1.3082 - mae: 0.0625\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.3082 - mae: 0.0625 - val_loss: 1.4716 - val_mae: 0.3390\n",
      "Epoch 142/600\n",
      "Epoch 142: Warmup phase (142/150) - val_mae: 0.3367\u001b[1m0s\u001b[0m 22ms/step - loss: 1.2910 - mae: 0.0622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.2910 - mae: 0.0622 - val_loss: 1.4523 - val_mae: 0.3367\n",
      "Epoch 143/600\n",
      "Epoch 143: Warmup phase (143/150) - val_mae: 0.3343\u001b[1m0s\u001b[0m 22ms/step - loss: 1.2741 - mae: 0.0618\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.2741 - mae: 0.0618 - val_loss: 1.4334 - val_mae: 0.3343\n",
      "Epoch 144/600\n",
      "Epoch 144: Warmup phase (144/150) - val_mae: 0.3320\u001b[1m0s\u001b[0m 22ms/step - loss: 1.2574 - mae: 0.0614\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.2574 - mae: 0.0614 - val_loss: 1.4146 - val_mae: 0.3320\n",
      "Epoch 145/600\n",
      "Epoch 145: Warmup phase (145/150) - val_mae: 0.3296\u001b[1m0s\u001b[0m 21ms/step - loss: 1.2409 - mae: 0.0610\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.2409 - mae: 0.0610 - val_loss: 1.3960 - val_mae: 0.3296\n",
      "Epoch 146/600\n",
      "Epoch 146: Warmup phase (146/150) - val_mae: 0.3272\u001b[1m0s\u001b[0m 23ms/step - loss: 1.2247 - mae: 0.0606\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.2247 - mae: 0.0606 - val_loss: 1.3777 - val_mae: 0.3272\n",
      "Epoch 147/600\n",
      "Epoch 147: Warmup phase (147/150) - val_mae: 0.3248\u001b[1m0s\u001b[0m 22ms/step - loss: 1.2087 - mae: 0.0603\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.2087 - mae: 0.0603 - val_loss: 1.3596 - val_mae: 0.3248\n",
      "Epoch 148/600\n",
      "Epoch 148: Warmup phase (148/150) - val_mae: 0.3223\u001b[1m0s\u001b[0m 23ms/step - loss: 1.1929 - mae: 0.0599\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 1.1929 - mae: 0.0599 - val_loss: 1.3418 - val_mae: 0.3223\n",
      "Epoch 149/600\n",
      "Epoch 149: Warmup phase (149/150) - val_mae: 0.3200\u001b[1m0s\u001b[0m 22ms/step - loss: 1.1774 - mae: 0.0596\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.1774 - mae: 0.0596 - val_loss: 1.3242 - val_mae: 0.3200\n",
      "Epoch 150/600\n",
      "Epoch 150: Warmup phase (150/150) - val_mae: 0.3175\u001b[1m0s\u001b[0m 22ms/step - loss: 1.1620 - mae: 0.0592\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.1620 - mae: 0.0592 - val_loss: 1.3068 - val_mae: 0.3175\n",
      "Epoch 151/600\n",
      "Epoch 151: val_mae improved to 0.3151\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.1469 - mae: 0.0589\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.1469 - mae: 0.0589 - val_loss: 1.2897 - val_mae: 0.3151\n",
      "Epoch 152/600\n",
      "Epoch 152: val_mae improved to 0.3127\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.1320 - mae: 0.0586\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.1320 - mae: 0.0586 - val_loss: 1.2727 - val_mae: 0.3127\n",
      "Epoch 153/600\n",
      "Epoch 153: val_mae improved to 0.3103\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.1173 - mae: 0.0583\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.1173 - mae: 0.0583 - val_loss: 1.2561 - val_mae: 0.3103\n",
      "Epoch 154/600\n",
      "Epoch 154: val_mae improved to 0.3079\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.1028 - mae: 0.0579\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.1028 - mae: 0.0579 - val_loss: 1.2397 - val_mae: 0.3079\n",
      "Epoch 155/600\n",
      "Epoch 155: val_mae improved to 0.3055\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0885 - mae: 0.0576\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.0885 - mae: 0.0576 - val_loss: 1.2235 - val_mae: 0.3055\n",
      "Epoch 156/600\n",
      "Epoch 156: val_mae improved to 0.3032\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0744 - mae: 0.0572\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.0744 - mae: 0.0572 - val_loss: 1.2074 - val_mae: 0.3032\n",
      "Epoch 157/600\n",
      "Epoch 157: val_mae improved to 0.3009\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0604 - mae: 0.0569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.0604 - mae: 0.0569 - val_loss: 1.1916 - val_mae: 0.3009\n",
      "Epoch 158/600\n",
      "Epoch 158: val_mae improved to 0.2985\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0467 - mae: 0.0565\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.0467 - mae: 0.0565 - val_loss: 1.1760 - val_mae: 0.2985\n",
      "Epoch 159/600\n",
      "Epoch 159: val_mae improved to 0.2961\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0332 - mae: 0.0561\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.0332 - mae: 0.0561 - val_loss: 1.1606 - val_mae: 0.2961\n",
      "Epoch 160/600\n",
      "Epoch 160: val_mae improved to 0.2937\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0199 - mae: 0.0557\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.0199 - mae: 0.0557 - val_loss: 1.1454 - val_mae: 0.2937\n",
      "Epoch 161/600\n",
      "Epoch 161: val_mae improved to 0.2913\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0067 - mae: 0.0553\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 1.0067 - mae: 0.0553 - val_loss: 1.1305 - val_mae: 0.2913\n",
      "Epoch 162/600\n",
      "Epoch 162: val_mae improved to 0.2889\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.9937 - mae: 0.0549\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.9937 - mae: 0.0549 - val_loss: 1.1157 - val_mae: 0.2889\n",
      "Epoch 163/600\n",
      "Epoch 163: val_mae improved to 0.2866\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.9810 - mae: 0.0545\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.9810 - mae: 0.0545 - val_loss: 1.1011 - val_mae: 0.2866\n",
      "Epoch 164/600\n",
      "Epoch 164: val_mae improved to 0.2843\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.9683 - mae: 0.0541\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.9683 - mae: 0.0541 - val_loss: 1.0868 - val_mae: 0.2843\n",
      "Epoch 165/600\n",
      "Epoch 165: val_mae improved to 0.2818\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.9559 - mae: 0.0537\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.9559 - mae: 0.0537 - val_loss: 1.0725 - val_mae: 0.2818\n",
      "Epoch 166/600\n",
      "Epoch 166: val_mae improved to 0.2796\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.9436 - mae: 0.0533\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.9436 - mae: 0.0533 - val_loss: 1.0586 - val_mae: 0.2796\n",
      "Epoch 167/600\n",
      "Epoch 167: val_mae improved to 0.2773\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.9316 - mae: 0.0530\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.9316 - mae: 0.0530 - val_loss: 1.0448 - val_mae: 0.2773\n",
      "Epoch 168/600\n",
      "Epoch 168: val_mae improved to 0.2750\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.9196 - mae: 0.0526\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.9196 - mae: 0.0526 - val_loss: 1.0312 - val_mae: 0.2750\n",
      "Epoch 169/600\n",
      "Epoch 169: val_mae improved to 0.2727\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.9079 - mae: 0.0523\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.9079 - mae: 0.0523 - val_loss: 1.0178 - val_mae: 0.2727\n",
      "Epoch 170/600\n",
      "Epoch 170: val_mae improved to 0.2705\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.8963 - mae: 0.0519\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8963 - mae: 0.0519 - val_loss: 1.0046 - val_mae: 0.2705\n",
      "Epoch 171/600\n",
      "Epoch 171: val_mae improved to 0.2681\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.8848 - mae: 0.0516\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8848 - mae: 0.0516 - val_loss: 0.9916 - val_mae: 0.2681\n",
      "Epoch 172/600\n",
      "Epoch 172: val_mae improved to 0.2659\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.8736 - mae: 0.0513\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8736 - mae: 0.0513 - val_loss: 0.9788 - val_mae: 0.2659\n",
      "Epoch 173/600\n",
      "Epoch 173: val_mae improved to 0.2636\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.8625 - mae: 0.0509\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8625 - mae: 0.0509 - val_loss: 0.9660 - val_mae: 0.2636\n",
      "Epoch 174/600\n",
      "Epoch 174: val_mae improved to 0.2614\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.8515 - mae: 0.0506\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8515 - mae: 0.0506 - val_loss: 0.9535 - val_mae: 0.2614\n",
      "Epoch 175/600\n",
      "Epoch 175: val_mae improved to 0.2591\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.8407 - mae: 0.0503\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8407 - mae: 0.0503 - val_loss: 0.9411 - val_mae: 0.2591\n",
      "Epoch 176/600\n",
      "Epoch 176: val_mae improved to 0.2569\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.8300 - mae: 0.0502\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.8300 - mae: 0.0502 - val_loss: 0.9289 - val_mae: 0.2569\n",
      "Epoch 177/600\n",
      "Epoch 177: val_mae improved to 0.2546\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.8195 - mae: 0.0498\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.8195 - mae: 0.0498 - val_loss: 0.9168 - val_mae: 0.2546\n",
      "Epoch 178/600\n",
      "Epoch 178: val_mae improved to 0.2524\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.8092 - mae: 0.0495\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8092 - mae: 0.0495 - val_loss: 0.9049 - val_mae: 0.2524\n",
      "Epoch 179/600\n",
      "Epoch 179: val_mae improved to 0.2502\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.7989 - mae: 0.0493\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7989 - mae: 0.0493 - val_loss: 0.8932 - val_mae: 0.2502\n",
      "Epoch 180/600\n",
      "Epoch 180: val_mae improved to 0.2481\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.7889 - mae: 0.0490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7889 - mae: 0.0490 - val_loss: 0.8816 - val_mae: 0.2481\n",
      "Epoch 181/600\n",
      "Epoch 181: val_mae improved to 0.2460\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.7789 - mae: 0.0487\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7789 - mae: 0.0487 - val_loss: 0.8702 - val_mae: 0.2460\n",
      "Epoch 182/600\n",
      "Epoch 182: val_mae improved to 0.2440\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.7691 - mae: 0.0483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7691 - mae: 0.0483 - val_loss: 0.8590 - val_mae: 0.2440\n",
      "Epoch 183/600\n",
      "Epoch 183: val_mae improved to 0.2420\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.7595 - mae: 0.0481\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7595 - mae: 0.0481 - val_loss: 0.8479 - val_mae: 0.2420\n",
      "Epoch 184/600\n",
      "Epoch 184: val_mae improved to 0.2399\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.7499 - mae: 0.0478\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7499 - mae: 0.0478 - val_loss: 0.8369 - val_mae: 0.2399\n",
      "Epoch 185/600\n",
      "Epoch 185: val_mae improved to 0.2378\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.7406 - mae: 0.0474\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7406 - mae: 0.0474 - val_loss: 0.8261 - val_mae: 0.2378\n",
      "Epoch 186/600\n",
      "Epoch 186: val_mae improved to 0.2359\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.7313 - mae: 0.0472\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7313 - mae: 0.0472 - val_loss: 0.8154 - val_mae: 0.2359\n",
      "Epoch 187/600\n",
      "Epoch 187: val_mae improved to 0.2339\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.7222 - mae: 0.0469\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7222 - mae: 0.0469 - val_loss: 0.8049 - val_mae: 0.2339\n",
      "Epoch 188/600\n",
      "Epoch 188: val_mae improved to 0.2319\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.7132 - mae: 0.0466\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7132 - mae: 0.0466 - val_loss: 0.7944 - val_mae: 0.2319\n",
      "Epoch 189/600\n",
      "Epoch 189: val_mae improved to 0.2299\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.7043 - mae: 0.0463\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.7043 - mae: 0.0463 - val_loss: 0.7842 - val_mae: 0.2299\n",
      "Epoch 190/600\n",
      "Epoch 190: val_mae improved to 0.2279\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.6955 - mae: 0.0460\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.6955 - mae: 0.0460 - val_loss: 0.7741 - val_mae: 0.2279\n",
      "Epoch 191/600\n",
      "Epoch 191: val_mae improved to 0.2259\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.6869 - mae: 0.0458\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6869 - mae: 0.0458 - val_loss: 0.7641 - val_mae: 0.2259\n",
      "Epoch 192/600\n",
      "Epoch 192: val_mae improved to 0.2238\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.6784 - mae: 0.0455\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6784 - mae: 0.0455 - val_loss: 0.7543 - val_mae: 0.2238\n",
      "Epoch 193/600\n",
      "Epoch 193: val_mae improved to 0.2217\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.6700 - mae: 0.0453\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6700 - mae: 0.0453 - val_loss: 0.7445 - val_mae: 0.2217\n",
      "Epoch 194/600\n",
      "Epoch 194: val_mae improved to 0.2198\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.6617 - mae: 0.0450\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6617 - mae: 0.0450 - val_loss: 0.7350 - val_mae: 0.2198\n",
      "Epoch 195/600\n",
      "Epoch 195: val_mae improved to 0.2179\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.6535 - mae: 0.0447\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.6535 - mae: 0.0447 - val_loss: 0.7256 - val_mae: 0.2179\n",
      "Epoch 196/600\n",
      "Epoch 196: val_mae improved to 0.2159\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.6455 - mae: 0.0446\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.6455 - mae: 0.0446 - val_loss: 0.7163 - val_mae: 0.2159\n",
      "Epoch 197/600\n",
      "Epoch 197: val_mae improved to 0.2140\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.6375 - mae: 0.0444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6375 - mae: 0.0444 - val_loss: 0.7071 - val_mae: 0.2140\n",
      "Epoch 198/600\n",
      "Epoch 198: val_mae improved to 0.2121\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.6297 - mae: 0.0441\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.6297 - mae: 0.0441 - val_loss: 0.6980 - val_mae: 0.2121\n",
      "Epoch 199/600\n",
      "Epoch 199: val_mae improved to 0.2101\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.6220 - mae: 0.0439\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6220 - mae: 0.0439 - val_loss: 0.6891 - val_mae: 0.2101\n",
      "Epoch 200/600\n",
      "Epoch 200: val_mae improved to 0.2082\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.6143 - mae: 0.0437\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6143 - mae: 0.0437 - val_loss: 0.6803 - val_mae: 0.2082\n",
      "Epoch 201/600\n",
      "Epoch 201: val_mae improved to 0.2062\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.6068 - mae: 0.0434\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.6068 - mae: 0.0434 - val_loss: 0.6716 - val_mae: 0.2062\n",
      "Epoch 202/600\n",
      "Epoch 202: val_mae improved to 0.2042\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5994 - mae: 0.0432\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.5994 - mae: 0.0432 - val_loss: 0.6630 - val_mae: 0.2042\n",
      "Epoch 203/600\n",
      "Epoch 203: val_mae improved to 0.2023\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5921 - mae: 0.0429\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5921 - mae: 0.0429 - val_loss: 0.6545 - val_mae: 0.2023\n",
      "Epoch 204/600\n",
      "Epoch 204: val_mae improved to 0.2004\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5849 - mae: 0.0427\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.5849 - mae: 0.0427 - val_loss: 0.6462 - val_mae: 0.2004\n",
      "Epoch 205/600\n",
      "Epoch 205: val_mae improved to 0.1985\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5778 - mae: 0.0425\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.5778 - mae: 0.0425 - val_loss: 0.6379 - val_mae: 0.1985\n",
      "Epoch 206/600\n",
      "Epoch 206: val_mae improved to 0.1967\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5708 - mae: 0.0422\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.5708 - mae: 0.0422 - val_loss: 0.6298 - val_mae: 0.1967\n",
      "Epoch 207/600\n",
      "Epoch 207: val_mae improved to 0.1946\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5639 - mae: 0.0420\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.5639 - mae: 0.0420 - val_loss: 0.6218 - val_mae: 0.1946\n",
      "Epoch 208/600\n",
      "Epoch 208: val_mae improved to 0.1927\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5571 - mae: 0.0417\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.5571 - mae: 0.0417 - val_loss: 0.6139 - val_mae: 0.1927\n",
      "Epoch 209/600\n",
      "Epoch 209: val_mae improved to 0.1909\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5503 - mae: 0.0414\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5503 - mae: 0.0414 - val_loss: 0.6061 - val_mae: 0.1909\n",
      "Epoch 210/600\n",
      "Epoch 210: val_mae improved to 0.1892\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5437 - mae: 0.0413\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.5437 - mae: 0.0413 - val_loss: 0.5984 - val_mae: 0.1892\n",
      "Epoch 211/600\n",
      "Epoch 211: val_mae improved to 0.1873\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5372 - mae: 0.0411\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.5372 - mae: 0.0411 - val_loss: 0.5908 - val_mae: 0.1873\n",
      "Epoch 212/600\n",
      "Epoch 212: val_mae improved to 0.1854\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.5307 - mae: 0.0408\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.5307 - mae: 0.0408 - val_loss: 0.5833 - val_mae: 0.1854\n",
      "Epoch 213/600\n",
      "Epoch 213: val_mae improved to 0.1836\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5243 - mae: 0.0406\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5243 - mae: 0.0406 - val_loss: 0.5759 - val_mae: 0.1836\n",
      "Epoch 214/600\n",
      "Epoch 214: val_mae improved to 0.1818\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5181 - mae: 0.0404\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.5181 - mae: 0.0404 - val_loss: 0.5687 - val_mae: 0.1818\n",
      "Epoch 215/600\n",
      "Epoch 215: val_mae improved to 0.1799\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.5119 - mae: 0.0402\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.5119 - mae: 0.0402 - val_loss: 0.5615 - val_mae: 0.1799\n",
      "Epoch 216/600\n",
      "Epoch 216: val_mae improved to 0.1782\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.5058 - mae: 0.0401\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.5058 - mae: 0.0401 - val_loss: 0.5544 - val_mae: 0.1782\n",
      "Epoch 217/600\n",
      "Epoch 217: val_mae improved to 0.1764\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4998 - mae: 0.0399\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4998 - mae: 0.0399 - val_loss: 0.5474 - val_mae: 0.1764\n",
      "Epoch 218/600\n",
      "Epoch 218: val_mae improved to 0.1746\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4938 - mae: 0.0397\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.4938 - mae: 0.0397 - val_loss: 0.5405 - val_mae: 0.1746\n",
      "Epoch 219/600\n",
      "Epoch 219: val_mae improved to 0.1729\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4879 - mae: 0.0394\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4879 - mae: 0.0394 - val_loss: 0.5338 - val_mae: 0.1729\n",
      "Epoch 220/600\n",
      "Epoch 220: val_mae improved to 0.1711\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4822 - mae: 0.0393\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4822 - mae: 0.0393 - val_loss: 0.5270 - val_mae: 0.1711\n",
      "Epoch 221/600\n",
      "Epoch 221: val_mae improved to 0.1693\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4765 - mae: 0.0390\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4765 - mae: 0.0390 - val_loss: 0.5204 - val_mae: 0.1693\n",
      "Epoch 222/600\n",
      "Epoch 222: val_mae improved to 0.1677\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4708 - mae: 0.0389\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4708 - mae: 0.0389 - val_loss: 0.5139 - val_mae: 0.1677\n",
      "Epoch 223/600\n",
      "Epoch 223: val_mae improved to 0.1660\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4653 - mae: 0.0387\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.4653 - mae: 0.0387 - val_loss: 0.5075 - val_mae: 0.1660\n",
      "Epoch 224/600\n",
      "Epoch 224: val_mae improved to 0.1643\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4598 - mae: 0.0385\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4598 - mae: 0.0385 - val_loss: 0.5012 - val_mae: 0.1643\n",
      "Epoch 225/600\n",
      "Epoch 225: val_mae improved to 0.1626\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4544 - mae: 0.0383\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4544 - mae: 0.0383 - val_loss: 0.4949 - val_mae: 0.1626\n",
      "Epoch 226/600\n",
      "Epoch 226: val_mae improved to 0.1609\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4491 - mae: 0.0381\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.4491 - mae: 0.0381 - val_loss: 0.4887 - val_mae: 0.1609\n",
      "Epoch 227/600\n",
      "Epoch 227: val_mae improved to 0.1592\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.4438 - mae: 0.0378\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.4438 - mae: 0.0378 - val_loss: 0.4826 - val_mae: 0.1592\n",
      "Epoch 228/600\n",
      "Epoch 228: val_mae improved to 0.1575\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.4386 - mae: 0.0377\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4386 - mae: 0.0377 - val_loss: 0.4766 - val_mae: 0.1575\n",
      "Epoch 229/600\n",
      "Epoch 229: val_mae improved to 0.1559\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4335 - mae: 0.0374\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4335 - mae: 0.0374 - val_loss: 0.4707 - val_mae: 0.1559\n",
      "Epoch 230/600\n",
      "Epoch 230: val_mae improved to 0.1542\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4285 - mae: 0.0373\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.4285 - mae: 0.0373 - val_loss: 0.4649 - val_mae: 0.1542\n",
      "Epoch 231/600\n",
      "Epoch 231: val_mae improved to 0.1525\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4235 - mae: 0.0371\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4235 - mae: 0.0371 - val_loss: 0.4591 - val_mae: 0.1525\n",
      "Epoch 232/600\n",
      "Epoch 232: val_mae improved to 0.1509\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4186 - mae: 0.0370\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4186 - mae: 0.0370 - val_loss: 0.4534 - val_mae: 0.1509\n",
      "Epoch 233/600\n",
      "Epoch 233: val_mae improved to 0.1493\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4137 - mae: 0.0368\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.4137 - mae: 0.0368 - val_loss: 0.4478 - val_mae: 0.1493\n",
      "Epoch 234/600\n",
      "Epoch 234: val_mae improved to 0.1476\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4089 - mae: 0.0367\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.4089 - mae: 0.0367 - val_loss: 0.4423 - val_mae: 0.1476\n",
      "Epoch 235/600\n",
      "Epoch 235: val_mae improved to 0.1461\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.4042 - mae: 0.0366\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.4042 - mae: 0.0366 - val_loss: 0.4368 - val_mae: 0.1461\n",
      "Epoch 236/600\n",
      "Epoch 236: val_mae improved to 0.1444\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3996 - mae: 0.0365\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3996 - mae: 0.0365 - val_loss: 0.4314 - val_mae: 0.1444\n",
      "Epoch 237/600\n",
      "Epoch 237: val_mae improved to 0.1427\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3950 - mae: 0.0364\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.3950 - mae: 0.0364 - val_loss: 0.4261 - val_mae: 0.1427\n",
      "Epoch 238/600\n",
      "Epoch 238: val_mae improved to 0.1411\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3905 - mae: 0.0363\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.3905 - mae: 0.0363 - val_loss: 0.4208 - val_mae: 0.1411\n",
      "Epoch 239/600\n",
      "Epoch 239: val_mae improved to 0.1394\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.3860 - mae: 0.0362\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.3860 - mae: 0.0362 - val_loss: 0.4156 - val_mae: 0.1394\n",
      "Epoch 240/600\n",
      "Epoch 240: val_mae improved to 0.1378\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3816 - mae: 0.0360\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.3816 - mae: 0.0360 - val_loss: 0.4105 - val_mae: 0.1378\n",
      "Epoch 241/600\n",
      "Epoch 241: val_mae improved to 0.1359\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3772 - mae: 0.0359\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3772 - mae: 0.0359 - val_loss: 0.4054 - val_mae: 0.1359\n",
      "Epoch 242/600\n",
      "Epoch 242: val_mae improved to 0.1343\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3729 - mae: 0.0358\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3729 - mae: 0.0358 - val_loss: 0.4003 - val_mae: 0.1343\n",
      "Epoch 243/600\n",
      "Epoch 243: val_mae improved to 0.1326\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3687 - mae: 0.0357\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3687 - mae: 0.0357 - val_loss: 0.3954 - val_mae: 0.1326\n",
      "Epoch 244/600\n",
      "Epoch 244: val_mae improved to 0.1311\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3645 - mae: 0.0356\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3645 - mae: 0.0356 - val_loss: 0.3906 - val_mae: 0.1311\n",
      "Epoch 245/600\n",
      "Epoch 245: val_mae improved to 0.1296\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3604 - mae: 0.0355\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3604 - mae: 0.0355 - val_loss: 0.3858 - val_mae: 0.1296\n",
      "Epoch 246/600\n",
      "Epoch 246: val_mae improved to 0.1280\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3563 - mae: 0.0353\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.3563 - mae: 0.0353 - val_loss: 0.3811 - val_mae: 0.1280\n",
      "Epoch 247/600\n",
      "Epoch 247: val_mae improved to 0.1263\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3523 - mae: 0.0353\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3523 - mae: 0.0353 - val_loss: 0.3764 - val_mae: 0.1263\n",
      "Epoch 248/600\n",
      "Epoch 248: val_mae improved to 0.1248\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3483 - mae: 0.0351\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.3483 - mae: 0.0351 - val_loss: 0.3719 - val_mae: 0.1248\n",
      "Epoch 249/600\n",
      "Epoch 249: val_mae improved to 0.1233\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3444 - mae: 0.0350\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3444 - mae: 0.0350 - val_loss: 0.3674 - val_mae: 0.1233\n",
      "Epoch 250/600\n",
      "Epoch 250: val_mae improved to 0.1218\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.3406 - mae: 0.0348\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3406 - mae: 0.0348 - val_loss: 0.3629 - val_mae: 0.1218\n",
      "Epoch 251/600\n",
      "Epoch 251: val_mae improved to 0.1203\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3368 - mae: 0.0347\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3368 - mae: 0.0347 - val_loss: 0.3585 - val_mae: 0.1203\n",
      "Epoch 252/600\n",
      "Epoch 252: val_mae improved to 0.1188\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3330 - mae: 0.0346\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3330 - mae: 0.0346 - val_loss: 0.3542 - val_mae: 0.1188\n",
      "Epoch 253/600\n",
      "Epoch 253: val_mae improved to 0.1174\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3293 - mae: 0.0345\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.3293 - mae: 0.0345 - val_loss: 0.3499 - val_mae: 0.1174\n",
      "Epoch 254/600\n",
      "Epoch 254: val_mae improved to 0.1160\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3257 - mae: 0.0344\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3257 - mae: 0.0344 - val_loss: 0.3457 - val_mae: 0.1160\n",
      "Epoch 255/600\n",
      "Epoch 255: val_mae improved to 0.1145\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3220 - mae: 0.0343\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.3220 - mae: 0.0343 - val_loss: 0.3416 - val_mae: 0.1145\n",
      "Epoch 256/600\n",
      "Epoch 256: val_mae improved to 0.1130\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3185 - mae: 0.0342\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3185 - mae: 0.0342 - val_loss: 0.3374 - val_mae: 0.1130\n",
      "Epoch 257/600\n",
      "Epoch 257: val_mae improved to 0.1116\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3150 - mae: 0.0341\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.3150 - mae: 0.0341 - val_loss: 0.3334 - val_mae: 0.1116\n",
      "Epoch 258/600\n",
      "Epoch 258: val_mae improved to 0.1102\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3115 - mae: 0.0340\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.3115 - mae: 0.0340 - val_loss: 0.3294 - val_mae: 0.1102\n",
      "Epoch 259/600\n",
      "Epoch 259: val_mae improved to 0.1088\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.3081 - mae: 0.0339\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.3081 - mae: 0.0339 - val_loss: 0.3255 - val_mae: 0.1088\n",
      "Epoch 260/600\n",
      "Epoch 260: val_mae improved to 0.1075\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3047 - mae: 0.0337\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3047 - mae: 0.0337 - val_loss: 0.3216 - val_mae: 0.1075\n",
      "Epoch 261/600\n",
      "Epoch 261: val_mae improved to 0.1060\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3013 - mae: 0.0336\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.3013 - mae: 0.0336 - val_loss: 0.3178 - val_mae: 0.1060\n",
      "Epoch 262/600\n",
      "Epoch 262: val_mae improved to 0.1046\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2980 - mae: 0.0335\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.2980 - mae: 0.0335 - val_loss: 0.3140 - val_mae: 0.1046\n",
      "Epoch 263/600\n",
      "Epoch 263: val_mae improved to 0.1034\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2948 - mae: 0.0333\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2948 - mae: 0.0333 - val_loss: 0.3104 - val_mae: 0.1034\n",
      "Epoch 264/600\n",
      "Epoch 264: val_mae improved to 0.1020\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2916 - mae: 0.0332\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.2916 - mae: 0.0332 - val_loss: 0.3067 - val_mae: 0.1020\n",
      "Epoch 265/600\n",
      "Epoch 265: val_mae improved to 0.1007\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2884 - mae: 0.0331\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2884 - mae: 0.0331 - val_loss: 0.3031 - val_mae: 0.1007\n",
      "Epoch 266/600\n",
      "Epoch 266: val_mae improved to 0.0993\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2853 - mae: 0.0330\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.2853 - mae: 0.0330 - val_loss: 0.2995 - val_mae: 0.0993\n",
      "Epoch 267/600\n",
      "Epoch 267: val_mae improved to 0.0980\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2822 - mae: 0.0328\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.2822 - mae: 0.0328 - val_loss: 0.2960 - val_mae: 0.0980\n",
      "Epoch 268/600\n",
      "Epoch 268: val_mae improved to 0.0967\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2791 - mae: 0.0327\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2791 - mae: 0.0327 - val_loss: 0.2926 - val_mae: 0.0967\n",
      "Epoch 269/600\n",
      "Epoch 269: val_mae improved to 0.0954\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2761 - mae: 0.0326\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2761 - mae: 0.0326 - val_loss: 0.2892 - val_mae: 0.0954\n",
      "Epoch 270/600\n",
      "Epoch 270: val_mae improved to 0.0940\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2732 - mae: 0.0325\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2732 - mae: 0.0325 - val_loss: 0.2858 - val_mae: 0.0940\n",
      "Epoch 271/600\n",
      "Epoch 271: val_mae improved to 0.0928\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2702 - mae: 0.0324\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2702 - mae: 0.0324 - val_loss: 0.2825 - val_mae: 0.0928\n",
      "Epoch 272/600\n",
      "Epoch 272: val_mae improved to 0.0914\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2673 - mae: 0.0323\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.2673 - mae: 0.0323 - val_loss: 0.2793 - val_mae: 0.0914\n",
      "Epoch 273/600\n",
      "Epoch 273: val_mae improved to 0.0901\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2645 - mae: 0.0322\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.2645 - mae: 0.0322 - val_loss: 0.2761 - val_mae: 0.0901\n",
      "Epoch 274/600\n",
      "Epoch 274: val_mae improved to 0.0889\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2617 - mae: 0.0321\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.2617 - mae: 0.0321 - val_loss: 0.2729 - val_mae: 0.0889\n",
      "Epoch 275/600\n",
      "Epoch 275: val_mae improved to 0.0876\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2589 - mae: 0.0320\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2589 - mae: 0.0320 - val_loss: 0.2698 - val_mae: 0.0876\n",
      "Epoch 276/600\n",
      "Epoch 276: val_mae improved to 0.0864\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2561 - mae: 0.0319\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2561 - mae: 0.0319 - val_loss: 0.2667 - val_mae: 0.0864\n",
      "Epoch 277/600\n",
      "Epoch 277: val_mae improved to 0.0851\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2534 - mae: 0.0318\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2534 - mae: 0.0318 - val_loss: 0.2637 - val_mae: 0.0851\n",
      "Epoch 278/600\n",
      "Epoch 278: val_mae improved to 0.0838\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2507 - mae: 0.0318\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2507 - mae: 0.0318 - val_loss: 0.2606 - val_mae: 0.0838\n",
      "Epoch 279/600\n",
      "Epoch 279: val_mae improved to 0.0827\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2481 - mae: 0.0317\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2481 - mae: 0.0317 - val_loss: 0.2577 - val_mae: 0.0827\n",
      "Epoch 280/600\n",
      "Epoch 280: val_mae improved to 0.0814\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2455 - mae: 0.0316\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2455 - mae: 0.0316 - val_loss: 0.2548 - val_mae: 0.0814\n",
      "Epoch 281/600\n",
      "Epoch 281: val_mae improved to 0.0801\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2429 - mae: 0.0315\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2429 - mae: 0.0315 - val_loss: 0.2519 - val_mae: 0.0801\n",
      "Epoch 282/600\n",
      "Epoch 282: val_mae improved to 0.0790\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2404 - mae: 0.0313\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2404 - mae: 0.0313 - val_loss: 0.2491 - val_mae: 0.0790\n",
      "Epoch 283/600\n",
      "Epoch 283: val_mae improved to 0.0778\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2378 - mae: 0.0313\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2378 - mae: 0.0313 - val_loss: 0.2463 - val_mae: 0.0778\n",
      "Epoch 284/600\n",
      "Epoch 284: val_mae improved to 0.0766\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2354 - mae: 0.0312\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2354 - mae: 0.0312 - val_loss: 0.2435 - val_mae: 0.0766\n",
      "Epoch 285/600\n",
      "Epoch 285: val_mae improved to 0.0754\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2329 - mae: 0.0310\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2329 - mae: 0.0310 - val_loss: 0.2408 - val_mae: 0.0754\n",
      "Epoch 286/600\n",
      "Epoch 286: val_mae improved to 0.0743\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2305 - mae: 0.0309\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.2305 - mae: 0.0309 - val_loss: 0.2381 - val_mae: 0.0743\n",
      "Epoch 287/600\n",
      "Epoch 287: val_mae improved to 0.0731\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2281 - mae: 0.0308\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2281 - mae: 0.0308 - val_loss: 0.2355 - val_mae: 0.0731\n",
      "Epoch 288/600\n",
      "Epoch 288: val_mae improved to 0.0719\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2257 - mae: 0.0307\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2257 - mae: 0.0307 - val_loss: 0.2329 - val_mae: 0.0719\n",
      "Epoch 289/600\n",
      "Epoch 289: val_mae improved to 0.0708\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2234 - mae: 0.0306\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2234 - mae: 0.0306 - val_loss: 0.2303 - val_mae: 0.0708\n",
      "Epoch 290/600\n",
      "Epoch 290: val_mae improved to 0.0698\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2211 - mae: 0.0304\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2211 - mae: 0.0304 - val_loss: 0.2278 - val_mae: 0.0698\n",
      "Epoch 291/600\n",
      "Epoch 291: val_mae improved to 0.0687\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2188 - mae: 0.0304\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.2188 - mae: 0.0304 - val_loss: 0.2253 - val_mae: 0.0687\n",
      "Epoch 292/600\n",
      "Epoch 292: val_mae improved to 0.0677\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2166 - mae: 0.0303\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2166 - mae: 0.0303 - val_loss: 0.2228 - val_mae: 0.0677\n",
      "Epoch 293/600\n",
      "Epoch 293: val_mae improved to 0.0666\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2144 - mae: 0.0302\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2144 - mae: 0.0302 - val_loss: 0.2204 - val_mae: 0.0666\n",
      "Epoch 294/600\n",
      "Epoch 294: val_mae improved to 0.0658\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2122 - mae: 0.0300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2122 - mae: 0.0300 - val_loss: 0.2180 - val_mae: 0.0658\n",
      "Epoch 295/600\n",
      "Epoch 295: val_mae improved to 0.0648\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2100 - mae: 0.0300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2100 - mae: 0.0300 - val_loss: 0.2156 - val_mae: 0.0648\n",
      "Epoch 296/600\n",
      "Epoch 296: val_mae improved to 0.0638\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2079 - mae: 0.0299\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.2079 - mae: 0.0299 - val_loss: 0.2132 - val_mae: 0.0638\n",
      "Epoch 297/600\n",
      "Epoch 297: val_mae improved to 0.0629\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2058 - mae: 0.0297\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.2058 - mae: 0.0297 - val_loss: 0.2109 - val_mae: 0.0629\n",
      "Epoch 298/600\n",
      "Epoch 298: val_mae improved to 0.0620\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2037 - mae: 0.0296\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2037 - mae: 0.0296 - val_loss: 0.2087 - val_mae: 0.0620\n",
      "Epoch 299/600\n",
      "Epoch 299: val_mae improved to 0.0610\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2016 - mae: 0.0295\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2016 - mae: 0.0295 - val_loss: 0.2064 - val_mae: 0.0610\n",
      "Epoch 300/600\n",
      "Epoch 300: val_mae improved to 0.0602\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1996 - mae: 0.0295\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1996 - mae: 0.0295 - val_loss: 0.2042 - val_mae: 0.0602\n",
      "Epoch 301/600\n",
      "Epoch 301: val_mae improved to 0.0592\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1976 - mae: 0.0293\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1976 - mae: 0.0293 - val_loss: 0.2020 - val_mae: 0.0592\n",
      "Epoch 302/600\n",
      "Epoch 302: val_mae improved to 0.0583\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1956 - mae: 0.0292\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1956 - mae: 0.0292 - val_loss: 0.1999 - val_mae: 0.0583\n",
      "Epoch 303/600\n",
      "Epoch 303: val_mae improved to 0.0574\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1937 - mae: 0.0291\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1937 - mae: 0.0291 - val_loss: 0.1977 - val_mae: 0.0574\n",
      "Epoch 304/600\n",
      "Epoch 304: val_mae improved to 0.0565\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1917 - mae: 0.0290\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1917 - mae: 0.0290 - val_loss: 0.1956 - val_mae: 0.0565\n",
      "Epoch 305/600\n",
      "Epoch 305: val_mae improved to 0.0557\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1898 - mae: 0.0289\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1898 - mae: 0.0289 - val_loss: 0.1936 - val_mae: 0.0557\n",
      "Epoch 306/600\n",
      "Epoch 306: val_mae improved to 0.0547\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1879 - mae: 0.0288\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.1879 - mae: 0.0288 - val_loss: 0.1915 - val_mae: 0.0547\n",
      "Epoch 307/600\n",
      "Epoch 307: val_mae improved to 0.0537\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1861 - mae: 0.0287\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1861 - mae: 0.0287 - val_loss: 0.1895 - val_mae: 0.0537\n",
      "Epoch 308/600\n",
      "Epoch 308: val_mae improved to 0.0529\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1842 - mae: 0.0285\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1842 - mae: 0.0285 - val_loss: 0.1875 - val_mae: 0.0529\n",
      "Epoch 309/600\n",
      "Epoch 309: val_mae improved to 0.0520\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1824 - mae: 0.0285\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1824 - mae: 0.0285 - val_loss: 0.1855 - val_mae: 0.0520\n",
      "Epoch 310/600\n",
      "Epoch 310: val_mae improved to 0.0510\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1806 - mae: 0.0283\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1806 - mae: 0.0283 - val_loss: 0.1836 - val_mae: 0.0510\n",
      "Epoch 311/600\n",
      "Epoch 311: val_mae improved to 0.0502\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1789 - mae: 0.0283\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1789 - mae: 0.0283 - val_loss: 0.1817 - val_mae: 0.0502\n",
      "Epoch 312/600\n",
      "Epoch 312: val_mae improved to 0.0493\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1771 - mae: 0.0281\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1771 - mae: 0.0281 - val_loss: 0.1798 - val_mae: 0.0493\n",
      "Epoch 313/600\n",
      "Epoch 313: val_mae improved to 0.0484\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1754 - mae: 0.0281\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1754 - mae: 0.0281 - val_loss: 0.1779 - val_mae: 0.0484\n",
      "Epoch 314/600\n",
      "Epoch 314: val_mae improved to 0.0476\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1737 - mae: 0.0280\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1737 - mae: 0.0280 - val_loss: 0.1761 - val_mae: 0.0476\n",
      "Epoch 315/600\n",
      "Epoch 315: val_mae improved to 0.0467\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1720 - mae: 0.0279\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1720 - mae: 0.0279 - val_loss: 0.1743 - val_mae: 0.0467\n",
      "Epoch 316/600\n",
      "Epoch 316: val_mae improved to 0.0459\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1703 - mae: 0.0278\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1703 - mae: 0.0278 - val_loss: 0.1725 - val_mae: 0.0459\n",
      "Epoch 317/600\n",
      "Epoch 317: val_mae improved to 0.0451\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1687 - mae: 0.0278\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1687 - mae: 0.0278 - val_loss: 0.1707 - val_mae: 0.0451\n",
      "Epoch 318/600\n",
      "Epoch 318: val_mae improved to 0.0442\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1670 - mae: 0.0277\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1670 - mae: 0.0277 - val_loss: 0.1690 - val_mae: 0.0442\n",
      "Epoch 319/600\n",
      "Epoch 319: val_mae improved to 0.0435\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1654 - mae: 0.0275\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1654 - mae: 0.0275 - val_loss: 0.1672 - val_mae: 0.0435\n",
      "Epoch 320/600\n",
      "Epoch 320: val_mae improved to 0.0426\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1638 - mae: 0.0274\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1638 - mae: 0.0274 - val_loss: 0.1655 - val_mae: 0.0426\n",
      "Epoch 321/600\n",
      "Epoch 321: val_mae improved to 0.0419\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1622 - mae: 0.0273\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1622 - mae: 0.0273 - val_loss: 0.1639 - val_mae: 0.0419\n",
      "Epoch 322/600\n",
      "Epoch 322: val_mae improved to 0.0410\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1607 - mae: 0.0272\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1607 - mae: 0.0272 - val_loss: 0.1622 - val_mae: 0.0410\n",
      "Epoch 323/600\n",
      "Epoch 323: val_mae improved to 0.0405\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1591 - mae: 0.0272\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1591 - mae: 0.0272 - val_loss: 0.1606 - val_mae: 0.0405\n",
      "Epoch 324/600\n",
      "Epoch 324: val_mae improved to 0.0401\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1576 - mae: 0.0270\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1576 - mae: 0.0270 - val_loss: 0.1590 - val_mae: 0.0401\n",
      "Epoch 325/600\n",
      "Epoch 325: val_mae improved to 0.0397\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1561 - mae: 0.0270\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1561 - mae: 0.0270 - val_loss: 0.1574 - val_mae: 0.0397\n",
      "Epoch 326/600\n",
      "Epoch 326: val_mae improved to 0.0393\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1546 - mae: 0.0269\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1546 - mae: 0.0269 - val_loss: 0.1558 - val_mae: 0.0393\n",
      "Epoch 327/600\n",
      "Epoch 327: val_mae improved to 0.0389\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1532 - mae: 0.0268\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1532 - mae: 0.0268 - val_loss: 0.1542 - val_mae: 0.0389\n",
      "Epoch 328/600\n",
      "Epoch 328: val_mae improved to 0.0385\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1517 - mae: 0.0268\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1517 - mae: 0.0268 - val_loss: 0.1527 - val_mae: 0.0385\n",
      "Epoch 329/600\n",
      "Epoch 329: val_mae improved to 0.0381\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1503 - mae: 0.0266\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1503 - mae: 0.0266 - val_loss: 0.1512 - val_mae: 0.0381\n",
      "Epoch 330/600\n",
      "Epoch 330: val_mae improved to 0.0377\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1489 - mae: 0.0265\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.1489 - mae: 0.0265 - val_loss: 0.1497 - val_mae: 0.0377\n",
      "Epoch 331/600\n",
      "Epoch 331: val_mae improved to 0.0374\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1475 - mae: 0.0265\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1475 - mae: 0.0265 - val_loss: 0.1482 - val_mae: 0.0374\n",
      "Epoch 332/600\n",
      "Epoch 332: val_mae improved to 0.0370\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1461 - mae: 0.0264\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1461 - mae: 0.0264 - val_loss: 0.1467 - val_mae: 0.0370\n",
      "Epoch 333/600\n",
      "Epoch 333: val_mae improved to 0.0367\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1447 - mae: 0.0263\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1447 - mae: 0.0263 - val_loss: 0.1453 - val_mae: 0.0367\n",
      "Epoch 334/600\n",
      "Epoch 334: val_mae improved to 0.0363\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1434 - mae: 0.0262\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1434 - mae: 0.0262 - val_loss: 0.1439 - val_mae: 0.0363\n",
      "Epoch 335/600\n",
      "Epoch 335: val_mae improved to 0.0361\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1421 - mae: 0.0261\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1421 - mae: 0.0261 - val_loss: 0.1425 - val_mae: 0.0361\n",
      "Epoch 336/600\n",
      "Epoch 336: val_mae improved to 0.0357\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1407 - mae: 0.0261\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1407 - mae: 0.0261 - val_loss: 0.1411 - val_mae: 0.0357\n",
      "Epoch 337/600\n",
      "Epoch 337: val_mae improved to 0.0354\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1394 - mae: 0.0260\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1394 - mae: 0.0260 - val_loss: 0.1397 - val_mae: 0.0354\n",
      "Epoch 338/600\n",
      "Epoch 338: val_mae improved to 0.0351\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1381 - mae: 0.0258\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.1381 - mae: 0.0258 - val_loss: 0.1384 - val_mae: 0.0351\n",
      "Epoch 339/600\n",
      "Epoch 339: val_mae improved to 0.0349\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1369 - mae: 0.0258\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.1369 - mae: 0.0258 - val_loss: 0.1371 - val_mae: 0.0349\n",
      "Epoch 340/600\n",
      "Epoch 340: val_mae improved to 0.0346\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1356 - mae: 0.0257\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1356 - mae: 0.0257 - val_loss: 0.1357 - val_mae: 0.0346\n",
      "Epoch 341/600\n",
      "Epoch 341: val_mae improved to 0.0342\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1344 - mae: 0.0256\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1344 - mae: 0.0256 - val_loss: 0.1344 - val_mae: 0.0342\n",
      "Epoch 342/600\n",
      "Epoch 342: val_mae improved to 0.0340\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1331 - mae: 0.0256\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1331 - mae: 0.0256 - val_loss: 0.1332 - val_mae: 0.0340\n",
      "Epoch 343/600\n",
      "Epoch 343: val_mae improved to 0.0338\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1319 - mae: 0.0254\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1319 - mae: 0.0254 - val_loss: 0.1319 - val_mae: 0.0338\n",
      "Epoch 344/600\n",
      "Epoch 344: val_mae improved to 0.0335\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1307 - mae: 0.0254\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1307 - mae: 0.0254 - val_loss: 0.1307 - val_mae: 0.0335\n",
      "Epoch 345/600\n",
      "Epoch 345: val_mae improved to 0.0333\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1295 - mae: 0.0253\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1295 - mae: 0.0253 - val_loss: 0.1294 - val_mae: 0.0333\n",
      "Epoch 346/600\n",
      "Epoch 346: val_mae improved to 0.0330\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1284 - mae: 0.0252\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1284 - mae: 0.0252 - val_loss: 0.1282 - val_mae: 0.0330\n",
      "Epoch 347/600\n",
      "Epoch 347: val_mae improved to 0.0327\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1272 - mae: 0.0252\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1272 - mae: 0.0252 - val_loss: 0.1270 - val_mae: 0.0327\n",
      "Epoch 348/600\n",
      "Epoch 348: val_mae improved to 0.0325\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1260 - mae: 0.0251\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1260 - mae: 0.0251 - val_loss: 0.1258 - val_mae: 0.0325\n",
      "Epoch 349/600\n",
      "Epoch 349: val_mae improved to 0.0323\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1249 - mae: 0.0250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1249 - mae: 0.0250 - val_loss: 0.1246 - val_mae: 0.0323\n",
      "Epoch 350/600\n",
      "Epoch 350: val_mae improved to 0.0321\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1238 - mae: 0.0249\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1238 - mae: 0.0249 - val_loss: 0.1235 - val_mae: 0.0321\n",
      "Epoch 351/600\n",
      "Epoch 351: val_mae improved to 0.0319\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1227 - mae: 0.0248\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1227 - mae: 0.0248 - val_loss: 0.1223 - val_mae: 0.0319\n",
      "Epoch 352/600\n",
      "Epoch 352: val_mae improved to 0.0316\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1216 - mae: 0.0248\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1216 - mae: 0.0248 - val_loss: 0.1212 - val_mae: 0.0316\n",
      "Epoch 353/600\n",
      "Epoch 353: val_mae improved to 0.0314\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1205 - mae: 0.0247\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1205 - mae: 0.0247 - val_loss: 0.1201 - val_mae: 0.0314\n",
      "Epoch 354/600\n",
      "Epoch 354: val_mae improved to 0.0312\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1194 - mae: 0.0247\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1194 - mae: 0.0247 - val_loss: 0.1190 - val_mae: 0.0312\n",
      "Epoch 355/600\n",
      "Epoch 355: val_mae improved to 0.0310\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1184 - mae: 0.0246\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1184 - mae: 0.0246 - val_loss: 0.1179 - val_mae: 0.0310\n",
      "Epoch 356/600\n",
      "Epoch 356: val_mae improved to 0.0307\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1173 - mae: 0.0246\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.1173 - mae: 0.0246 - val_loss: 0.1168 - val_mae: 0.0307\n",
      "Epoch 357/600\n",
      "Epoch 357: val_mae improved to 0.0305\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1163 - mae: 0.0246\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1163 - mae: 0.0246 - val_loss: 0.1158 - val_mae: 0.0305\n",
      "Epoch 358/600\n",
      "Epoch 358: val_mae improved to 0.0304\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1152 - mae: 0.0245\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.1152 - mae: 0.0245 - val_loss: 0.1147 - val_mae: 0.0304\n",
      "Epoch 359/600\n",
      "Epoch 359: val_mae improved to 0.0301\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1142 - mae: 0.0245\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1142 - mae: 0.0245 - val_loss: 0.1137 - val_mae: 0.0301\n",
      "Epoch 360/600\n",
      "Epoch 360: val_mae improved to 0.0299\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1132 - mae: 0.0244\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1132 - mae: 0.0244 - val_loss: 0.1127 - val_mae: 0.0299\n",
      "Epoch 361/600\n",
      "Epoch 361: val_mae improved to 0.0297\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1122 - mae: 0.0244\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1122 - mae: 0.0244 - val_loss: 0.1116 - val_mae: 0.0297\n",
      "Epoch 362/600\n",
      "Epoch 362: val_mae improved to 0.0296\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1112 - mae: 0.0244\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1112 - mae: 0.0244 - val_loss: 0.1106 - val_mae: 0.0296\n",
      "Epoch 363/600\n",
      "Epoch 363: val_mae improved to 0.0294\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1103 - mae: 0.0244\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1103 - mae: 0.0244 - val_loss: 0.1097 - val_mae: 0.0294\n",
      "Epoch 364/600\n",
      "Epoch 364: val_mae improved to 0.0292\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1093 - mae: 0.0244\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1093 - mae: 0.0244 - val_loss: 0.1087 - val_mae: 0.0292\n",
      "Epoch 365/600\n",
      "Epoch 365: val_mae improved to 0.0290\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1084 - mae: 0.0243\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.1084 - mae: 0.0243 - val_loss: 0.1077 - val_mae: 0.0290\n",
      "Epoch 366/600\n",
      "Epoch 366: val_mae improved to 0.0289\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1074 - mae: 0.0243\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1074 - mae: 0.0243 - val_loss: 0.1068 - val_mae: 0.0289\n",
      "Epoch 367/600\n",
      "Epoch 367: val_mae improved to 0.0287\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1065 - mae: 0.0243\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.1065 - mae: 0.0243 - val_loss: 0.1058 - val_mae: 0.0287\n",
      "Epoch 368/600\n",
      "Epoch 368: val_mae improved to 0.0285\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1056 - mae: 0.0243\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1056 - mae: 0.0243 - val_loss: 0.1049 - val_mae: 0.0285\n",
      "Epoch 369/600\n",
      "Epoch 369: val_mae improved to 0.0283\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1047 - mae: 0.0242\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1047 - mae: 0.0242 - val_loss: 0.1040 - val_mae: 0.0283\n",
      "Epoch 370/600\n",
      "Epoch 370: val_mae improved to 0.0282\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1038 - mae: 0.0242\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1038 - mae: 0.0242 - val_loss: 0.1031 - val_mae: 0.0282\n",
      "Epoch 371/600\n",
      "Epoch 371: val_mae improved to 0.0281\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1029 - mae: 0.0242\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1029 - mae: 0.0242 - val_loss: 0.1022 - val_mae: 0.0281\n",
      "Epoch 372/600\n",
      "Epoch 372: val_mae improved to 0.0279\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1020 - mae: 0.0241\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1020 - mae: 0.0241 - val_loss: 0.1013 - val_mae: 0.0279\n",
      "Epoch 373/600\n",
      "Epoch 373: val_mae improved to 0.0278\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1011 - mae: 0.0241\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.1011 - mae: 0.0241 - val_loss: 0.1004 - val_mae: 0.0278\n",
      "Epoch 374/600\n",
      "Epoch 374: val_mae improved to 0.0277\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1003 - mae: 0.0241\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1003 - mae: 0.0241 - val_loss: 0.0996 - val_mae: 0.0277\n",
      "Epoch 375/600\n",
      "Epoch 375: val_mae improved to 0.0275\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0994 - mae: 0.0241\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0994 - mae: 0.0241 - val_loss: 0.0987 - val_mae: 0.0275\n",
      "Epoch 376/600\n",
      "Epoch 376: val_mae improved to 0.0274\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0986 - mae: 0.0241\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0986 - mae: 0.0241 - val_loss: 0.0979 - val_mae: 0.0274\n",
      "Epoch 377/600\n",
      "Epoch 377: val_mae improved to 0.0272\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0977 - mae: 0.0241\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0977 - mae: 0.0241 - val_loss: 0.0970 - val_mae: 0.0272\n",
      "Epoch 378/600\n",
      "Epoch 378: val_mae improved to 0.0271\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0969 - mae: 0.0240\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0969 - mae: 0.0240 - val_loss: 0.0962 - val_mae: 0.0271\n",
      "Epoch 379/600\n",
      "Epoch 379: val_mae improved to 0.0269\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0961 - mae: 0.0240\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0961 - mae: 0.0240 - val_loss: 0.0954 - val_mae: 0.0269\n",
      "Epoch 380/600\n",
      "Epoch 380: val_mae improved to 0.0268\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0953 - mae: 0.0240\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0953 - mae: 0.0240 - val_loss: 0.0946 - val_mae: 0.0268\n",
      "Epoch 381/600\n",
      "Epoch 381: val_mae improved to 0.0267\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0945 - mae: 0.0240\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0945 - mae: 0.0240 - val_loss: 0.0938 - val_mae: 0.0267\n",
      "Epoch 382/600\n",
      "Epoch 382: val_mae improved to 0.0266\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0937 - mae: 0.0239\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0937 - mae: 0.0239 - val_loss: 0.0930 - val_mae: 0.0266\n",
      "Epoch 383/600\n",
      "Epoch 383: val_mae improved to 0.0265\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0929 - mae: 0.0239\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0929 - mae: 0.0239 - val_loss: 0.0922 - val_mae: 0.0265\n",
      "Epoch 384/600\n",
      "Epoch 384: val_mae improved to 0.0263\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0921 - mae: 0.0239\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0921 - mae: 0.0239 - val_loss: 0.0915 - val_mae: 0.0263\n",
      "Epoch 385/600\n",
      "Epoch 385: val_mae improved to 0.0262\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0914 - mae: 0.0239\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0914 - mae: 0.0239 - val_loss: 0.0907 - val_mae: 0.0262\n",
      "Epoch 386/600\n",
      "Epoch 386: val_mae improved to 0.0261\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0906 - mae: 0.0238\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0906 - mae: 0.0238 - val_loss: 0.0899 - val_mae: 0.0261\n",
      "Epoch 387/600\n",
      "Epoch 387: val_mae improved to 0.0259\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0899 - mae: 0.0238\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0899 - mae: 0.0238 - val_loss: 0.0892 - val_mae: 0.0259\n",
      "Epoch 388/600\n",
      "Epoch 388: val_mae improved to 0.0258\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0891 - mae: 0.0238\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0891 - mae: 0.0238 - val_loss: 0.0885 - val_mae: 0.0258\n",
      "Epoch 389/600\n",
      "Epoch 389: val_mae improved to 0.0257\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0884 - mae: 0.0238\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0884 - mae: 0.0238 - val_loss: 0.0877 - val_mae: 0.0257\n",
      "Epoch 390/600\n",
      "Epoch 390: val_mae improved to 0.0256\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0877 - mae: 0.0238\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0877 - mae: 0.0238 - val_loss: 0.0870 - val_mae: 0.0256\n",
      "Epoch 391/600\n",
      "Epoch 391: val_mae improved to 0.0255\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0870 - mae: 0.0237\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0870 - mae: 0.0237 - val_loss: 0.0863 - val_mae: 0.0255\n",
      "Epoch 392/600\n",
      "Epoch 392: val_mae improved to 0.0254\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0862 - mae: 0.0237\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0862 - mae: 0.0237 - val_loss: 0.0856 - val_mae: 0.0254\n",
      "Epoch 393/600\n",
      "Epoch 393: val_mae improved to 0.0253\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0855 - mae: 0.0237\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.0855 - mae: 0.0237 - val_loss: 0.0849 - val_mae: 0.0253\n",
      "Epoch 394/600\n",
      "Epoch 394: val_mae improved to 0.0251\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0848 - mae: 0.0237\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0848 - mae: 0.0237 - val_loss: 0.0842 - val_mae: 0.0251\n",
      "Epoch 395/600\n",
      "Epoch 395: val_mae improved to 0.0250\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0842 - mae: 0.0237\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0842 - mae: 0.0237 - val_loss: 0.0835 - val_mae: 0.0250\n",
      "Epoch 396/600\n",
      "Epoch 396: val_mae improved to 0.0249\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0835 - mae: 0.0237\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0835 - mae: 0.0237 - val_loss: 0.0829 - val_mae: 0.0249\n",
      "Epoch 397/600\n",
      "Epoch 397: val_mae improved to 0.0248\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0828 - mae: 0.0237\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0828 - mae: 0.0237 - val_loss: 0.0822 - val_mae: 0.0248\n",
      "Epoch 398/600\n",
      "Epoch 398: val_mae improved to 0.0247\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0821 - mae: 0.0236\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0821 - mae: 0.0236 - val_loss: 0.0815 - val_mae: 0.0247\n",
      "Epoch 399/600\n",
      "Epoch 399: val_mae improved to 0.0246\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0815 - mae: 0.0236\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0815 - mae: 0.0236 - val_loss: 0.0809 - val_mae: 0.0246\n",
      "Epoch 400/600\n",
      "Epoch 400: val_mae improved to 0.0245\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0808 - mae: 0.0236\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0808 - mae: 0.0236 - val_loss: 0.0802 - val_mae: 0.0245\n",
      "Epoch 401/600\n",
      "Epoch 401: val_mae improved to 0.0244\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0802 - mae: 0.0236\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0802 - mae: 0.0236 - val_loss: 0.0797 - val_mae: 0.0244\n",
      "Epoch 402/600\n",
      "Epoch 402: val_mae improved to 0.0243\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0796 - mae: 0.0236\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.0796 - mae: 0.0236 - val_loss: 0.0791 - val_mae: 0.0243\n",
      "Epoch 403/600\n",
      "Epoch 403: val_mae did not improve. Wait count: 1/5\u001b[1m0s\u001b[0m 22ms/step - loss: 0.0790 - mae: 0.0235\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0790 - mae: 0.0235 - val_loss: 0.0785 - val_mae: 0.0244\n",
      "Epoch 404/600\n",
      "Epoch 404: val_mae did not improve. Wait count: 2/5\u001b[1m0s\u001b[0m 22ms/step - loss: 0.0784 - mae: 0.0235\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0784 - mae: 0.0235 - val_loss: 0.0780 - val_mae: 0.0246\n",
      "Epoch 405/600\n",
      "Epoch 405: val_mae did not improve. Wait count: 3/5\u001b[1m0s\u001b[0m 22ms/step - loss: 0.0779 - mae: 0.0235\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0779 - mae: 0.0235 - val_loss: 0.0774 - val_mae: 0.0247\n",
      "Epoch 406/600\n",
      "Epoch 406: val_mae did not improve. Wait count: 4/5\u001b[1m0s\u001b[0m 23ms/step - loss: 0.0773 - mae: 0.0235\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0773 - mae: 0.0235 - val_loss: 0.0769 - val_mae: 0.0248\n",
      "Epoch 407/600\n",
      "Epoch 407: val_mae did not improve. Wait count: 5/5\u001b[1m0s\u001b[0m 23ms/step - loss: 0.0768 - mae: 0.0234\n",
      "Restoring model weights from epoch 402\n",
      "EarlyStopping triggered at epoch 407\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.0768 - mae: 0.0234 - val_loss: 0.0763 - val_mae: 0.0249\n",
      "Training stopped at epoch 407 due to early stopping.\n"
     ]
    }
   ],
   "source": [
    "class EarlyStoppingWithWarmup(Callback):\n",
    "    def __init__(self, monitor='val_mae', mode='min', patience=5, warmup_epochs=150, restore_best_weights=True, verbose=1):\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.patience = patience\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "        self.stopped_epoch = 0\n",
    "        \n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.inf\n",
    "        else:\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            print(f\"Warning: EarlyStopping requires {self.monitor} available!\")\n",
    "            return\n",
    "        \n",
    "        # Warmup: Skip monitoring during the warmup period\n",
    "        if epoch < self.warmup_epochs:\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1}: Warmup phase ({epoch+1}/{self.warmup_epochs}) - {self.monitor}: {current:.4f}\")\n",
    "            return\n",
    "\n",
    "        if self.monitor_op(current, self.best):\n",
    "            self.best = current\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.wait = 0\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1}: {self.monitor} improved to {current:.4f}\")\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1}: {self.monitor} did not improve. Wait count: {self.wait}/{self.patience}\")\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                if self.restore_best_weights:\n",
    "                    if self.best_weights is not None:\n",
    "                        self.model.set_weights(self.best_weights)\n",
    "                        print(f\"Restoring model weights from epoch {self.stopped_epoch - self.patience + 1}\")\n",
    "                print(f\"EarlyStopping triggered at epoch {self.stopped_epoch + 1}\")\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0 and self.verbose:\n",
    "            print(f\"Training stopped at epoch {self.stopped_epoch + 1} due to early stopping.\")\n",
    "\n",
    "\n",
    "# Setting learning rate decay\n",
    "initial_lr = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    decay_steps=400,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Create an EarlyStopping callback\n",
    "early_stop = EarlyStoppingWithWarmup(\n",
    "    monitor='val_mae',          # Metric to monitor\n",
    "    patience=5,                 # Wait 5 epochs after min before stopping\n",
    "    mode='min',                 # Stop when val_mae stops decreasing\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.07), input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.06)),\n",
    "    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.06)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=600, verbose=1, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2479b61c-e217-468c-a626-1c7e41770121",
   "metadata": {},
   "source": [
    "The MAE for training set is 0.0236 eV after 402 epochs of training. Evaluate the model with cross-validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f5bdfa3-67b4-4ff6-bb04-1f3157dd11ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0791 - mae: 0.0243\n",
      "Validation MAE from model.evaluate: 0.0243\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_mae = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation MAE from model.evaluate: {val_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c8d6d0b-bd9c-4c57-98cb-6222af681b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml = pd.DataFrame({\n",
    "    'epoch': np.arange(1, len(history.history['loss']) + 1),\n",
    "    'training loss': history.history['loss'],\n",
    "    'training mae': history.history['mae'],\n",
    "    'validation loss': history.history['val_loss'],\n",
    "    'validation mae': history.history['val_mae']\n",
    "})\n",
    "\n",
    "df_ml.to_csv('training_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1313ad3c-ad85-4333-8630-4760a3d4e4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_target</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.791583</td>\n",
       "      <td>1.763040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.722750</td>\n",
       "      <td>1.743170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.707833</td>\n",
       "      <td>1.724792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.774417</td>\n",
       "      <td>1.759123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.790833</td>\n",
       "      <td>1.724371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.753917</td>\n",
       "      <td>1.724576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.759083</td>\n",
       "      <td>1.727178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.724667</td>\n",
       "      <td>1.726471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.755583</td>\n",
       "      <td>1.725015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.720583</td>\n",
       "      <td>1.716670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.681583</td>\n",
       "      <td>1.698117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.687500</td>\n",
       "      <td>1.699153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.718583</td>\n",
       "      <td>1.699464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.699333</td>\n",
       "      <td>1.666664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.680250</td>\n",
       "      <td>1.659302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.670000</td>\n",
       "      <td>1.656120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.638250</td>\n",
       "      <td>1.645471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.709167</td>\n",
       "      <td>1.652688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.672500</td>\n",
       "      <td>1.643779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.586167</td>\n",
       "      <td>1.640593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.622583</td>\n",
       "      <td>1.622757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.619417</td>\n",
       "      <td>1.636220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.607417</td>\n",
       "      <td>1.629049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    y_target    y_pred\n",
       "0   1.791583  1.763040\n",
       "1   1.722750  1.743170\n",
       "2   1.707833  1.724792\n",
       "3   1.774417  1.759123\n",
       "4   1.790833  1.724371\n",
       "5   1.753917  1.724576\n",
       "6   1.759083  1.727178\n",
       "7   1.724667  1.726471\n",
       "8   1.755583  1.725015\n",
       "9   1.720583  1.716670\n",
       "10  1.681583  1.698117\n",
       "11  1.687500  1.699153\n",
       "12  1.718583  1.699464\n",
       "13  1.699333  1.666664\n",
       "14  1.680250  1.659302\n",
       "15  1.670000  1.656120\n",
       "16  1.638250  1.645471\n",
       "17  1.709167  1.652688\n",
       "18  1.672500  1.643779\n",
       "19  1.586167  1.640593\n",
       "20  1.622583  1.622757\n",
       "21  1.619417  1.636220\n",
       "22  1.607417  1.629049"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_scaled)\n",
    "\n",
    "df_mlresult = pd.DataFrame({\n",
    "    'y_target': np.ravel(y),  # Flatten y to 1D too, if needed\n",
    "    'y_pred': np.ravel(y_pred)\n",
    "})\n",
    "df_mlresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "faa9acd3-cfb4-4011-ac60-687a044ff3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "mu      0.142 +/- 0.148\n",
      "t       0.048 +/- 0.049\n",
      "RA      0.044 +/- 0.100\n",
      "Nd      0.002 +/- 0.119\n",
      "XB      -0.019 +/- 0.031\n",
      "QA      -0.020 +/- 0.051\n",
      "XA      -0.074 +/- 0.107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "r = permutation_importance(model, X_val, y_val,\n",
    "                           scoring='r2',\n",
    "                           n_repeats=30,\n",
    "                           random_state=0)\n",
    "\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    print(f\"{features[i]:<8}\"\n",
    "            f\"{r.importances_mean[i]:.3f}\"\n",
    "            f\" +/- {r.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be2af91e-9075-4827-9c0c-3de041cbf3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 30)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.importances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "660e9af5-0fdf-4f04-a91a-ccce8b31deed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>mu</th>\n",
       "      <th>RA</th>\n",
       "      <th>XA</th>\n",
       "      <th>XB</th>\n",
       "      <th>QA</th>\n",
       "      <th>Nd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.107764</td>\n",
       "      <td>0.011662</td>\n",
       "      <td>0.152442</td>\n",
       "      <td>-0.117109</td>\n",
       "      <td>0.047277</td>\n",
       "      <td>-0.055348</td>\n",
       "      <td>-0.081956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100802</td>\n",
       "      <td>0.392042</td>\n",
       "      <td>0.074133</td>\n",
       "      <td>-0.207469</td>\n",
       "      <td>0.011280</td>\n",
       "      <td>-0.052570</td>\n",
       "      <td>0.153160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.042981</td>\n",
       "      <td>0.022382</td>\n",
       "      <td>-0.109571</td>\n",
       "      <td>-0.053612</td>\n",
       "      <td>-0.066202</td>\n",
       "      <td>-0.158470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.044259</td>\n",
       "      <td>0.333255</td>\n",
       "      <td>-0.092739</td>\n",
       "      <td>-0.195480</td>\n",
       "      <td>-0.041295</td>\n",
       "      <td>-0.094024</td>\n",
       "      <td>-0.021060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.046117</td>\n",
       "      <td>-0.062226</td>\n",
       "      <td>0.094421</td>\n",
       "      <td>0.078930</td>\n",
       "      <td>-0.038429</td>\n",
       "      <td>0.065467</td>\n",
       "      <td>-0.166033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.100802</td>\n",
       "      <td>0.392042</td>\n",
       "      <td>0.074133</td>\n",
       "      <td>-0.207469</td>\n",
       "      <td>0.011280</td>\n",
       "      <td>-0.052570</td>\n",
       "      <td>0.153160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.037752</td>\n",
       "      <td>0.163215</td>\n",
       "      <td>-0.135870</td>\n",
       "      <td>0.031019</td>\n",
       "      <td>-0.051722</td>\n",
       "      <td>0.025151</td>\n",
       "      <td>-0.053818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.127643</td>\n",
       "      <td>0.243443</td>\n",
       "      <td>0.138632</td>\n",
       "      <td>-0.251461</td>\n",
       "      <td>-0.039035</td>\n",
       "      <td>-0.092929</td>\n",
       "      <td>-0.059419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.072737</td>\n",
       "      <td>-0.101542</td>\n",
       "      <td>0.124974</td>\n",
       "      <td>-0.091007</td>\n",
       "      <td>-0.038317</td>\n",
       "      <td>-0.035744</td>\n",
       "      <td>-0.278937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.036740</td>\n",
       "      <td>0.123732</td>\n",
       "      <td>0.023315</td>\n",
       "      <td>-0.017711</td>\n",
       "      <td>-0.000731</td>\n",
       "      <td>0.013088</td>\n",
       "      <td>0.153863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.127643</td>\n",
       "      <td>0.243443</td>\n",
       "      <td>0.138632</td>\n",
       "      <td>-0.251461</td>\n",
       "      <td>-0.039035</td>\n",
       "      <td>-0.092929</td>\n",
       "      <td>-0.059419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.036740</td>\n",
       "      <td>0.123732</td>\n",
       "      <td>0.023315</td>\n",
       "      <td>-0.017711</td>\n",
       "      <td>-0.000731</td>\n",
       "      <td>0.013088</td>\n",
       "      <td>0.153863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.018381</td>\n",
       "      <td>0.040695</td>\n",
       "      <td>-0.029965</td>\n",
       "      <td>-0.020655</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.017036</td>\n",
       "      <td>-0.027293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.089382</td>\n",
       "      <td>0.052357</td>\n",
       "      <td>0.122477</td>\n",
       "      <td>-0.137764</td>\n",
       "      <td>0.047225</td>\n",
       "      <td>-0.072384</td>\n",
       "      <td>-0.109249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.021421</td>\n",
       "      <td>0.331394</td>\n",
       "      <td>-0.094896</td>\n",
       "      <td>-0.203037</td>\n",
       "      <td>-0.039717</td>\n",
       "      <td>-0.101735</td>\n",
       "      <td>-0.015576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.155596</td>\n",
       "      <td>0.302629</td>\n",
       "      <td>0.251615</td>\n",
       "      <td>-0.132228</td>\n",
       "      <td>-0.040819</td>\n",
       "      <td>0.027649</td>\n",
       "      <td>0.088728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.100802</td>\n",
       "      <td>0.392042</td>\n",
       "      <td>0.074133</td>\n",
       "      <td>-0.207469</td>\n",
       "      <td>0.011280</td>\n",
       "      <td>-0.052570</td>\n",
       "      <td>0.153160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.003932</td>\n",
       "      <td>0.050689</td>\n",
       "      <td>0.023315</td>\n",
       "      <td>0.116409</td>\n",
       "      <td>-0.054285</td>\n",
       "      <td>0.068559</td>\n",
       "      <td>0.048038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.017468</td>\n",
       "      <td>0.354022</td>\n",
       "      <td>-0.120921</td>\n",
       "      <td>-0.148415</td>\n",
       "      <td>0.013837</td>\n",
       "      <td>-0.052570</td>\n",
       "      <td>0.130585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.085582</td>\n",
       "      <td>0.188971</td>\n",
       "      <td>0.194724</td>\n",
       "      <td>-0.028915</td>\n",
       "      <td>-0.039241</td>\n",
       "      <td>0.025114</td>\n",
       "      <td>0.131844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.024664</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>0.152818</td>\n",
       "      <td>-0.020840</td>\n",
       "      <td>-0.050573</td>\n",
       "      <td>0.017239</td>\n",
       "      <td>-0.112907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.036740</td>\n",
       "      <td>0.123732</td>\n",
       "      <td>0.023315</td>\n",
       "      <td>-0.017711</td>\n",
       "      <td>-0.000731</td>\n",
       "      <td>0.013088</td>\n",
       "      <td>0.153863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.028213</td>\n",
       "      <td>0.144545</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.084539</td>\n",
       "      <td>-0.043852</td>\n",
       "      <td>0.032825</td>\n",
       "      <td>0.096919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.028263</td>\n",
       "      <td>0.016713</td>\n",
       "      <td>0.049390</td>\n",
       "      <td>0.087612</td>\n",
       "      <td>-0.039035</td>\n",
       "      <td>0.033920</td>\n",
       "      <td>0.035984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.017208</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>-0.012125</td>\n",
       "      <td>-0.015487</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.018082</td>\n",
       "      <td>-0.135854</td>\n",
       "      <td>0.153870</td>\n",
       "      <td>0.037004</td>\n",
       "      <td>-0.047385</td>\n",
       "      <td>0.047241</td>\n",
       "      <td>-0.179955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.010486</td>\n",
       "      <td>0.181602</td>\n",
       "      <td>-0.094870</td>\n",
       "      <td>-0.173600</td>\n",
       "      <td>-0.036530</td>\n",
       "      <td>-0.092929</td>\n",
       "      <td>-0.091852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.005897</td>\n",
       "      <td>0.166576</td>\n",
       "      <td>-0.119763</td>\n",
       "      <td>0.017676</td>\n",
       "      <td>-0.048588</td>\n",
       "      <td>0.013495</td>\n",
       "      <td>-0.080089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.097434</td>\n",
       "      <td>0.127938</td>\n",
       "      <td>0.097443</td>\n",
       "      <td>-0.092973</td>\n",
       "      <td>0.034554</td>\n",
       "      <td>-0.063912</td>\n",
       "      <td>0.105845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           t        mu        RA        XA        XB        QA        Nd\n",
       "0   0.107764  0.011662  0.152442 -0.117109  0.047277 -0.055348 -0.081956\n",
       "1   0.100802  0.392042  0.074133 -0.207469  0.011280 -0.052570  0.153160\n",
       "2   0.002187  0.042981  0.022382 -0.109571 -0.053612 -0.066202 -0.158470\n",
       "3   0.044259  0.333255 -0.092739 -0.195480 -0.041295 -0.094024 -0.021060\n",
       "4   0.046117 -0.062226  0.094421  0.078930 -0.038429  0.065467 -0.166033\n",
       "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "6   0.100802  0.392042  0.074133 -0.207469  0.011280 -0.052570  0.153160\n",
       "7  -0.037752  0.163215 -0.135870  0.031019 -0.051722  0.025151 -0.053818\n",
       "8   0.127643  0.243443  0.138632 -0.251461 -0.039035 -0.092929 -0.059419\n",
       "9   0.072737 -0.101542  0.124974 -0.091007 -0.038317 -0.035744 -0.278937\n",
       "10  0.036740  0.123732  0.023315 -0.017711 -0.000731  0.013088  0.153863\n",
       "11  0.127643  0.243443  0.138632 -0.251461 -0.039035 -0.092929 -0.059419\n",
       "12  0.036740  0.123732  0.023315 -0.017711 -0.000731  0.013088  0.153863\n",
       "13 -0.018381  0.040695 -0.029965 -0.020655 -0.000051 -0.017036 -0.027293\n",
       "14  0.089382  0.052357  0.122477 -0.137764  0.047225 -0.072384 -0.109249\n",
       "15  0.021421  0.331394 -0.094896 -0.203037 -0.039717 -0.101735 -0.015576\n",
       "16  0.155596  0.302629  0.251615 -0.132228 -0.040819  0.027649  0.088728\n",
       "17  0.100802  0.392042  0.074133 -0.207469  0.011280 -0.052570  0.153160\n",
       "18  0.003932  0.050689  0.023315  0.116409 -0.054285  0.068559  0.048038\n",
       "19  0.017468  0.354022 -0.120921 -0.148415  0.013837 -0.052570  0.130585\n",
       "20  0.085582  0.188971  0.194724 -0.028915 -0.039241  0.025114  0.131844\n",
       "21  0.024664  0.013085  0.152818 -0.020840 -0.050573  0.017239 -0.112907\n",
       "22  0.036740  0.123732  0.023315 -0.017711 -0.000731  0.013088  0.153863\n",
       "23  0.028213  0.144545  0.013072  0.084539 -0.043852  0.032825  0.096919\n",
       "24  0.028263  0.016713  0.049390  0.087612 -0.039035  0.033920  0.035984\n",
       "25 -0.017208  0.001094 -0.012125 -0.015487 -0.000058  0.000000  0.005127\n",
       "26  0.018082 -0.135854  0.153870  0.037004 -0.047385  0.047241 -0.179955\n",
       "27  0.010486  0.181602 -0.094870 -0.173600 -0.036530 -0.092929 -0.091852\n",
       "28 -0.005897  0.166576 -0.119763  0.017676 -0.048588  0.013495 -0.080089\n",
       "29  0.097434  0.127938  0.097443 -0.092973  0.034554 -0.063912  0.105845"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imp = pd.DataFrame()\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    df_imp[feature] = r.importances[i]\n",
    "\n",
    "df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "863e1681-d7ef-458a-9745-36108e71c5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install graphviz (see instructions at https://graphviz.gitlab.io/download/) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='nn_structure.png', sshow_shapes=True, show_layer_names=True, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039d37e2-0b9a-4c1c-8bac-533e108b6010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
